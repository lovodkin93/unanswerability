{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/sloboda1/anaconda3/envs/adversarial_gpt/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unanswerable Questions - Old"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = r\"data/squad/dev-v2.0.json\"\n",
    "indir_reasons = r\"data/squad/reasons\"\n",
    "outdir = r\"generated_prompts\"\n",
    "IS_CONTROL_GROUP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortcut_to_reason = {\"E\":\"Entity Swap\",\n",
    "                        \"#\":\"Number Swap\",\n",
    "                        \"N\":\"Negation\",\n",
    "                        \"A\":\"Antonym\",\n",
    "                        \"X\":\"Mutual Exclusion\",\n",
    "                        \"I\":\"No Information\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/squad/dev-v2.0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1798098/3846046956.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreasons_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reason\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/squad/dev-v2.0.json'"
     ]
    }
   ],
   "source": [
    "with open(indir) as f:\n",
    "    data = json.load(f)\n",
    "    data = data[\"data\"]\n",
    "\n",
    "reasons_df = pd.DataFrame(columns=[\"qid\", \"reason\"])\n",
    "for reasons_filename in os.listdir(indir_reasons):\n",
    "    curr_reasons_df = pd.read_csv(os.path.join(indir_reasons, reasons_filename), sep='\\t')\n",
    "    reasons_df = pd.concat([reasons_df, curr_reasons_df], ignore_index=True)\n",
    "reasons_df = reasons_df.drop_duplicates()\n",
    "question_to_reason_dict = {row[\"qid\"]:shortcut_to_reason[row[\"reason\"]] for index, row in reasons_df.iterrows()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter only unanswerable questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = []\n",
    "for elem in data:\n",
    "    for paragraph in elem[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            sample = f\"Passage: {paragraph['context']} Question: {qa['question']}\"\n",
    "            if (IS_CONTROL_GROUP and len(qa[\"answers\"])>0) or (not IS_CONTROL_GROUP and len(qa[\"answers\"])==0):\n",
    "                filtered_data.append({\"id\":qa['id'], \"input\":sample})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Instructions (Zero-Shot) - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "    if not IS_CONTROL_GROUP:\n",
    "        prompt_elem[\"Unanswerablity-Reason\"] = question_to_reason_dict[elem[\"id\"]]\n",
    "\n",
    "\n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"Given the following passage and question, answer the question:\n",
    "        {elem[\"input\"]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.\n",
    "    If it cannot be answered based on the passage, answer with \"N/A\" only, and nothing else:\n",
    "        {elem[\"input\"]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"CoT-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.\n",
    "    First make sure if it can be answered by the passage.\n",
    "    If it cannot be answered based on the passage, answer with \"N/A\" only, and nothing else:\n",
    "        {elem[\"input\"]}\"\"\"  \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"Given the following passage and question, choose one of the following options:\n",
    "    (A) the question can be answered by the passage (B) the question cannot be answered by the passage.\n",
    "    Please answer with \"A\" or \"B\" only, and nothing else:\n",
    "        {elem[\"input\"]}\"\"\" \n",
    "\n",
    "    prompt_list_zero_shot.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Instructions (Zero-Shot) - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot_version2 = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "    if not IS_CONTROL_GROUP:\n",
    "        prompt_elem[\"Unanswerablity-Reason\"] = question_to_reason_dict[elem[\"id\"]]\n",
    "\n",
    "\n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"Given the following passage and question, answer the question:\n",
    "        {elem[\"input\"]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.\n",
    "    If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "        {elem[\"input\"]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"CoT-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.\n",
    "    First make sure if it can be answered by the passage.\n",
    "    If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "        {elem[\"input\"]}\"\"\"  \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"Given the following passage and question, decide if the question is answerable based on the passage. Reply only \"answerable\" or \"unanswerable\":\n",
    "        {elem[\"input\"]}\"\"\" \n",
    "\n",
    "    prompt_list_zero_shot_version2.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Instructions (Few-Shot) - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    pos_example_1 = {'input': 'Passage: Madonna released the Material Girl clothing line, which she designed with her daughter, Lourdes. The 1980s inspired clothing line, borrowed from Madonna\\'s punk-girl style when she rose to fame in the 1980s, was released under the Macy\\'s label. Madonna also opened a series of fitness centers around the world named Hard Candy Fitness. In November 2011, Madonna and MG Icon announced the release of a second fashion brand called Truth or Dare by Madonna to include footwear, underclothing, and accessories. She also directed her second feature film, W.E., a biographic about the affair between King Edward VIII and Wallis Simpson; it was co-written with Alek Keshishian. Critical and commercial response to the film was negative. Madonna contributed the ballad \"Masterpiece\" for the film\\'s soundtrack, which won her a Golden Globe Award for Best Original Song. Question: Material Girl clothing line is released under which brand?',\n",
    "                         'Answer': 'Macy\\'s.',\n",
    "                         'CoT':'\"Material Girl clothing line\" is first mentioned in the first sentence of the passage. The second sentence further discusses the clothing line, saying that it \"was released under the Macy\\'s label\".'}\n",
    "    \n",
    "    pos_example_2 = {'input':'Passage: In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system and moving away from the university\\'s traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame continued to grow over the years, adding more colleges, programs, and sports teams. By 1921, with the addition of the College of Commerce, Notre Dame had grown from a small college to a university with five colleges and a professional law school. The university continued to expand and add new residence halls and buildings with each subsequent president. Question: Over how many years did the change to national standards undertaken at Notre Dame in the early 20th century take place?',\n",
    "                         'Answer':'three years.',\n",
    "                         'CoT':'The \"national standards\" are mentioned in the first sentence of the passage, where it is stated that \"In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards\".'}\n",
    "\n",
    "    \n",
    "    neg_example_1 = {'input':'Passage: The descendants of Rollo\\'s Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, blending their maternal Frankish heritage with Old Norse traditions and customs to synthesize a unique \"Norman\" culture in the north of France. The Norman language was forged by the adoption of the indigenous langue d\\'o誰l branch of Romance by a Norse-speaking ruling class, and it developed into the regional language that survives today. Question: What was replaced with the Norse religion?',\n",
    "                        'Answer':'the given question cannot be answered in the context of the passage.',\n",
    "                        'CoT':'In the passage, it is mentioned that Norse religion was replaced with Catholicism, whereas the question asks about the entity which was replaced by Norse religion. No such information is provided in the passage.'}\n",
    "\n",
    "    \n",
    "    neg_example_2 = {'input':'Passage: Exceptions to the restrictions faced by Windows Store apps are given to web browsers. The user\\'s default browser can distribute a Metro-style web browser in same package as the desktop version, which has access to functionality unavailable to other apps, such as being able to permanently run in the background, use multiple background processes, and use Windows API code instead of WinRT (allowing for code to be re-used with the desktop version, while still taking advantage of features available to Windows Store apps, such as charms). Microsoft advertises this exception privilege \"New experience enabled\" (formerly \"Metro-style enabled\"). Question: What term did Microsoft give to its exception privilige for file browsing?',\n",
    "                         'Answer':'the given question cannot be answered in the context of the passage.',\n",
    "                         'CoT':\"The passage talks about exception privilege given with respect to the entity 'web browser', whereas the question asks about 'file browser'\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "    if not IS_CONTROL_GROUP:\n",
    "        prompt_elem[\"Unanswerablity-Reason\"] = question_to_reason_dict[elem[\"id\"]]\n",
    "        \n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: The answer to the given question is: {pos_example_1[\"Answer\"]}\n",
    "\n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: The answer to the given question is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Adversarial-CoT\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer to the given question is: {pos_example_1[\"Answer\"]}\n",
    "\n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer to the given question is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: The answer to the given question is {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: The answer to the given question is {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"Answer\"].capitalize()}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-CoT\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer to the given question is: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer to the given question is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"CoT\"]} Therefore, {neg_example_1[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: The question is answerable.\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: The question is answerable.\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: The question is not answerable.\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability-CoT\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, The question is answerable.\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, The question is answerable.\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"CoT\"]} Therefore, The question is not answerable.\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_list_few_shot.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Instructions (Few-Shot) - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    pos_example_1 = {'input': 'Passage: Madonna released the Material Girl clothing line, which she designed with her daughter, Lourdes. The 1980s inspired clothing line, borrowed from Madonna\\'s punk-girl style when she rose to fame in the 1980s, was released under the Macy\\'s label. Madonna also opened a series of fitness centers around the world named Hard Candy Fitness. In November 2011, Madonna and MG Icon announced the release of a second fashion brand called Truth or Dare by Madonna to include footwear, underclothing, and accessories. She also directed her second feature film, W.E., a biographic about the affair between King Edward VIII and Wallis Simpson; it was co-written with Alek Keshishian. Critical and commercial response to the film was negative. Madonna contributed the ballad \"Masterpiece\" for the film\\'s soundtrack, which won her a Golden Globe Award for Best Original Song. Question: Material Girl clothing line is released under which brand?',\n",
    "                         'Answer': 'Macy\\'s.',\n",
    "                         'CoT':'\"Material Girl clothing line\" is first mentioned in the first sentence of the passage. The second sentence further discusses the clothing line, saying that it \"was released under the Macy\\'s label\".'}\n",
    "    \n",
    "    pos_example_2 = {'input':'Passage: In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system and moving away from the university\\'s traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame continued to grow over the years, adding more colleges, programs, and sports teams. By 1921, with the addition of the College of Commerce, Notre Dame had grown from a small college to a university with five colleges and a professional law school. The university continued to expand and add new residence halls and buildings with each subsequent president. Question: Over how many years did the change to national standards undertaken at Notre Dame in the early 20th century take place?',\n",
    "                         'Answer':'three years.',\n",
    "                         'CoT':'The \"national standards\" are mentioned in the first sentence of the passage, where it is stated that \"In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards\".'}\n",
    "\n",
    "    \n",
    "    neg_example_1 = {'input':'Passage: The descendants of Rollo\\'s Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, blending their maternal Frankish heritage with Old Norse traditions and customs to synthesize a unique \"Norman\" culture in the north of France. The Norman language was forged by the adoption of the indigenous langue d\\'o誰l branch of Romance by a Norse-speaking ruling class, and it developed into the regional language that survives today. Question: What was replaced with the Norse religion?',\n",
    "                        'Answer':'unanswerable.',\n",
    "                        'CoT':'In the passage, it is mentioned that Norse religion was replaced with Catholicism, whereas the question asks about the entity which was replaced by Norse religion. No such information is provided in the passage.'}\n",
    "\n",
    "    \n",
    "    neg_example_2 = {'input':'Passage: Exceptions to the restrictions faced by Windows Store apps are given to web browsers. The user\\'s default browser can distribute a Metro-style web browser in same package as the desktop version, which has access to functionality unavailable to other apps, such as being able to permanently run in the background, use multiple background processes, and use Windows API code instead of WinRT (allowing for code to be re-used with the desktop version, while still taking advantage of features available to Windows Store apps, such as charms). Microsoft advertises this exception privilege \"New experience enabled\" (formerly \"Metro-style enabled\"). Question: What term did Microsoft give to its exception privilige for file browsing?',\n",
    "                         'Answer':'unanswerable.',\n",
    "                         'CoT':\"The passage talks about exception privilege given with respect to the entity 'web browser', whereas the question asks about 'file browser'\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "    if not IS_CONTROL_GROUP:\n",
    "        prompt_elem[\"Unanswerablity-Reason\"] = question_to_reason_dict[elem[\"id\"]]\n",
    "        \n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"Answer\"]}\n",
    "\n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Adversarial-CoT\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer is: {pos_example_1[\"Answer\"]}\n",
    "\n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"Answer\"].capitalize()}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-CoT\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer is: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"CoT\"]} Therefore, the answer is: {neg_example_1[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: answerable.\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: answerable.\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: unanswerable.\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability-CoT\"] = f\"\"\"\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, The question is answerable.\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, The question is answerable.\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"CoT\"]} Therefore, The question is unanswerable.\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_list_few_shot.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Instructions (Few-Shot with Instructions) - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_with_instructions = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "\n",
    "    pos_example_1 = {'input': 'Passage: Madonna released the Material Girl clothing line, which she designed with her daughter, Lourdes. The 1980s inspired clothing line, borrowed from Madonna\\'s punk-girl style when she rose to fame in the 1980s, was released under the Macy\\'s label. Madonna also opened a series of fitness centers around the world named Hard Candy Fitness. In November 2011, Madonna and MG Icon announced the release of a second fashion brand called Truth or Dare by Madonna to include footwear, underclothing, and accessories. She also directed her second feature film, W.E., a biographic about the affair between King Edward VIII and Wallis Simpson; it was co-written with Alek Keshishian. Critical and commercial response to the film was negative. Madonna contributed the ballad \"Masterpiece\" for the film\\'s soundtrack, which won her a Golden Globe Award for Best Original Song. Question: Material Girl clothing line is released under which brand?',\n",
    "                         'Answer': 'Macy\\'s.',\n",
    "                         'CoT':'\"Material Girl clothing line\" is first mentioned in the first sentence of the passage. The second sentence further discusses the clothing line, saying that it \"was released under the Macy\\'s label\".'}\n",
    "    \n",
    "    pos_example_2 = {'input':'Passage: In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system and moving away from the university\\'s traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame continued to grow over the years, adding more colleges, programs, and sports teams. By 1921, with the addition of the College of Commerce, Notre Dame had grown from a small college to a university with five colleges and a professional law school. The university continued to expand and add new residence halls and buildings with each subsequent president. Question: Over how many years did the change to national standards undertaken at Notre Dame in the early 20th century take place?',\n",
    "                         'Answer':'three years.',\n",
    "                         'CoT':'The \"national standards\" are mentioned in the first sentence of the passage, where it is stated that \"In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards\".'}\n",
    "\n",
    "    \n",
    "    neg_example_1 = {'input':'Passage: The descendants of Rollo\\'s Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, blending their maternal Frankish heritage with Old Norse traditions and customs to synthesize a unique \"Norman\" culture in the north of France. The Norman language was forged by the adoption of the indigenous langue d\\'o誰l branch of Romance by a Norse-speaking ruling class, and it developed into the regional language that survives today. Question: What was replaced with the Norse religion?',\n",
    "                        'Answer':'N/A',\n",
    "                        'CoT':'In the passage, it is mentioned that Norse religion was replaced with Catholicism, whereas the question asks about the entity which was replaced by Norse religion. No such information is provided in the passage.'}\n",
    "\n",
    "    \n",
    "    neg_example_2 = {'input':'Passage: Exceptions to the restrictions faced by Windows Store apps are given to web browsers. The user\\'s default browser can distribute a Metro-style web browser in same package as the desktop version, which has access to functionality unavailable to other apps, such as being able to permanently run in the background, use multiple background processes, and use Windows API code instead of WinRT (allowing for code to be re-used with the desktop version, while still taking advantage of features available to Windows Store apps, such as charms). Microsoft advertises this exception privilege \"New experience enabled\" (formerly \"Metro-style enabled\"). Question: What term did Microsoft give to its exception privilige for file browsing?',\n",
    "                         'Answer':'N/A',\n",
    "                         'CoT':\"The passage talks about exception privilege given with respect to the entity 'web browser', whereas the question asks about 'file browser'\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "    \n",
    "    if not IS_CONTROL_GROUP:\n",
    "        prompt_elem[\"Unanswerablity-Reason\"] = question_to_reason_dict[elem[\"id\"]]\n",
    "\n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"Given the following passage and question, answer the question:\n",
    "\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"Answer\"]}\n",
    "\n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Adversarial-CoT\"] = f\"\"\"Given the following passage and question, answer the question:\n",
    "    \n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer is: {pos_example_1[\"Answer\"]}\n",
    "\n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "If it cannot be answered based on the passage, answer with \"N/A\" only, and nothing else:\n",
    "        \n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-CoT\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "If it cannot be answered based on the passage, answer with \"N/A\" only, and nothing else:\n",
    "\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer is: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"CoT\"]} Therefore, the answer is: {neg_example_1[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-Ablation1\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "If it cannot be answered based on the passage, answer with \"N/A\" only, and nothing else:\n",
    "        \n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-Ablation1-CoT\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "If it cannot be answered based on the passage, answer with \"N/A\" only, and nothing else:\n",
    "\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer is: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-Ablation2\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "        \n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: The answer is {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: The answer is {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-Ablation2-CoT\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer is: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"CoT\"]} Therefore, the answer is: {neg_example_1[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "\n",
    "    \n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"Given the following passage and question, choose one of the following options:\n",
    "(A) the question can be answered by the passage (B) the question cannot be answered by the passage.\n",
    "Please answer with \"A\" or \"B\" only, and nothing else:\n",
    "\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: (A)\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: (A)\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: (B)\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability-CoT\"] = f\"\"\"Given the following passage and question, choose one of the following options:\n",
    "(A) the question can be answered by the passage (B) the question cannot be answered by the passage.\n",
    "Please answer with \"A\" or \"B\" only, and nothing else:\n",
    "\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer is (A).\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer is (A).\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"CoT\"]} Therefore, the answer is (B).\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_list_few_shot_with_instructions.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Instructions (Few-Shot with Instructions) - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_with_instructions_v2 = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "\n",
    "    pos_example_1 = {'input': 'Passage: Madonna released the Material Girl clothing line, which she designed with her daughter, Lourdes. The 1980s inspired clothing line, borrowed from Madonna\\'s punk-girl style when she rose to fame in the 1980s, was released under the Macy\\'s label. Madonna also opened a series of fitness centers around the world named Hard Candy Fitness. In November 2011, Madonna and MG Icon announced the release of a second fashion brand called Truth or Dare by Madonna to include footwear, underclothing, and accessories. She also directed her second feature film, W.E., a biographic about the affair between King Edward VIII and Wallis Simpson; it was co-written with Alek Keshishian. Critical and commercial response to the film was negative. Madonna contributed the ballad \"Masterpiece\" for the film\\'s soundtrack, which won her a Golden Globe Award for Best Original Song. Question: Material Girl clothing line is released under which brand?',\n",
    "                         'Answer': 'Macy\\'s.',\n",
    "                         'CoT':'\"Material Girl clothing line\" is first mentioned in the first sentence of the passage. The second sentence further discusses the clothing line, saying that it \"was released under the Macy\\'s label\".'}\n",
    "    \n",
    "    pos_example_2 = {'input':'Passage: In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system and moving away from the university\\'s traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame continued to grow over the years, adding more colleges, programs, and sports teams. By 1921, with the addition of the College of Commerce, Notre Dame had grown from a small college to a university with five colleges and a professional law school. The university continued to expand and add new residence halls and buildings with each subsequent president. Question: Over how many years did the change to national standards undertaken at Notre Dame in the early 20th century take place?',\n",
    "                         'Answer':'three years.',\n",
    "                         'CoT':'The \"national standards\" are mentioned in the first sentence of the passage, where it is stated that \"In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards\".'}\n",
    "\n",
    "    \n",
    "    neg_example_1 = {'input':'Passage: The descendants of Rollo\\'s Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, blending their maternal Frankish heritage with Old Norse traditions and customs to synthesize a unique \"Norman\" culture in the north of France. The Norman language was forged by the adoption of the indigenous langue d\\'o誰l branch of Romance by a Norse-speaking ruling class, and it developed into the regional language that survives today. Question: What was replaced with the Norse religion?',\n",
    "                        'Answer':'unanswerable.',\n",
    "                        'CoT':'In the passage, it is mentioned that Norse religion was replaced with Catholicism, whereas the question asks about the entity which was replaced by Norse religion. No such information is provided in the passage.'}\n",
    "\n",
    "    \n",
    "    neg_example_2 = {'input':'Passage: Exceptions to the restrictions faced by Windows Store apps are given to web browsers. The user\\'s default browser can distribute a Metro-style web browser in same package as the desktop version, which has access to functionality unavailable to other apps, such as being able to permanently run in the background, use multiple background processes, and use Windows API code instead of WinRT (allowing for code to be re-used with the desktop version, while still taking advantage of features available to Windows Store apps, such as charms). Microsoft advertises this exception privilege \"New experience enabled\" (formerly \"Metro-style enabled\"). Question: What term did Microsoft give to its exception privilige for file browsing?',\n",
    "                         'Answer':'unanswerable.',\n",
    "                         'CoT':\"The passage talks about exception privilege given with respect to the entity 'web browser', whereas the question asks about 'file browser'\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "    \n",
    "    if not IS_CONTROL_GROUP:\n",
    "        prompt_elem[\"Unanswerablity-Reason\"] = question_to_reason_dict[elem[\"id\"]]\n",
    "\n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"Given the following passage and question, answer the question:\n",
    "\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"Answer\"]}\n",
    "\n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Adversarial-CoT\"] = f\"\"\"Given the following passage and question, answer the question:\n",
    "    \n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer is: {pos_example_1[\"Answer\"]}\n",
    "\n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "        \n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-CoT\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer is: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"CoT\"]} Therefore, the answer is: {neg_example_1[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-Ablation1\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "        \n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-Ablation1-CoT\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer is: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-Ablation2\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "        \n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: The answer is {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: The answer is {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial-Ablation2-CoT\"] = f\"\"\"Given the following passage and question, answer the question.     \n",
    "\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, the answer is: {pos_example_1[\"Answer\"]}\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, the answer is: {pos_example_2[\"Answer\"]}\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"CoT\"]} Therefore, the answer is: {neg_example_1[\"Answer\"]}\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "\n",
    "    \n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"Given the following passage and question, decide if the question is answerable based on the passage. Reply only \"answerable\" or \"unanswerable\":\n",
    "\n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: answerable.\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: answerable.\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: unanswerable.\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability-CoT\"] = f\"\"\"Given the following passage and question, decide if the question is answerable based on the passage. Reply only \"answerable\" or \"unanswerable\":\n",
    "    \n",
    "        Example 1:\n",
    "        {pos_example_1[\"input\"]}\n",
    "        Output: {pos_example_1[\"CoT\"]} Therefore, The question is answerable.\n",
    "        \n",
    "        Example 2:\n",
    "        {pos_example_2[\"input\"]}\n",
    "        Output: {pos_example_2[\"CoT\"]} Therefore, The question is answerable.\n",
    "\n",
    "        Example 3:\n",
    "        {neg_example_1[\"input\"]}\n",
    "        Output: {neg_example_1[\"CoT\"]} Therefore, The question is unanswerable.\n",
    "\n",
    "        Now your turn:\n",
    "        {elem[\"input\"]}\n",
    "        Output:\"\"\" \n",
    "\n",
    "    prompt_list_few_shot_with_instructions_v2.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outdir):\n",
    "   os.makedirs(outdir)\n",
    "\n",
    "if not os.path.exists(os.path.join(outdir, \"chatGPT\", \"zero_shot\")):\n",
    "   os.makedirs(os.path.join(outdir, \"chatGPT\", \"zero_shot\"))\n",
    "\n",
    "if not os.path.exists(os.path.join(outdir, \"chatGPT\", \"few_shot\")):\n",
    "   os.makedirs(os.path.join(outdir, \"chatGPT\", \"few_shot\"))\n",
    "\n",
    "if not os.path.exists(os.path.join(outdir, \"chatGPT\", \"few_shot_with_instructions\")):\n",
    "   os.makedirs(os.path.join(outdir, \"chatGPT\", \"few_shot_with_instructions\"))\n",
    "\n",
    "split_type = \"control_group\" if IS_CONTROL_GROUP else \"adversarial\"\n",
    "\n",
    "# # zero shot\n",
    "# with open(os.path.join(outdir, \"chatGPT\", \"zero_shot\", f\"squad_{split_type}.json\"), 'w') as f1:\n",
    "#     f1.write(json.dumps(prompt_list_zero_shot, indent=2))\n",
    "\n",
    "# few shot\n",
    "# with open(os.path.join(outdir, \"chatGPT\", \"few_shot\", f\"squad_{split_type}.json\"), 'w') as f1:\n",
    "#     f1.write(json.dumps(prompt_list_few_shot, indent=2))\n",
    "\n",
    "# # few shot (with instructions)\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"few_shot_with_instructions\", f\"squad_{split_type}.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_few_shot_with_instructions, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unanswerable Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get reasons for unanswerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_reasons = r\"../files_not_for_git/data/squad/reasons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortcut_to_reason = {\"E\":\"Entity Swap\",\n",
    "                        \"#\":\"Number Swap\",\n",
    "                        \"N\":\"Negation\",\n",
    "                        \"A\":\"Antonym\",\n",
    "                        \"X\":\"Mutual Exclusion\",\n",
    "                        \"I\":\"No Information\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_df = pd.DataFrame(columns=[\"qid\", \"reason\"])\n",
    "for reasons_filename in os.listdir(indir_reasons):\n",
    "    curr_reasons_df = pd.read_csv(os.path.join(indir_reasons, reasons_filename), sep='\\t')\n",
    "    reasons_df = pd.concat([reasons_df, curr_reasons_df], ignore_index=True)\n",
    "reasons_df = reasons_df.drop_duplicates()\n",
    "question_to_reason_dict = {row[\"qid\"]:shortcut_to_reason[row[\"reason\"]] for index, row in reasons_df.iterrows()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot with Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICL_examples_variant = 2 # any of 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_indir = r\"../files_not_for_git/data/squad/dev-v2.0.json\" #r\"../data/squad/train-v2.0.json\"\n",
    "squad_few_shot_with_instructions_outdir = \"../generated_prompts/all/few_shot_with_instructions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(squad_indir, 'r') as f1:\n",
    "    examples = json.loads(f1.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code to find ICL examples - need to be used in debug mode\n",
    "# for example in examples['data']:\n",
    "#     for paragraph in example['paragraphs']:\n",
    "#         curr_context = paragraph['context']\n",
    "#         for qa in paragraph['qas']:\n",
    "#             if qa['is_impossible']:\n",
    "#                 adversarial_qa = qa\n",
    "#             else:\n",
    "#                 control_group_qa = qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icl_examples_squad_v1():\n",
    "    pos_example_1 = {'input': 'Passage: Madonna released the Material Girl clothing line, which she designed with her daughter, Lourdes. The 1980s inspired clothing line, borrowed from Madonna\\'s punk-girl style when she rose to fame in the 1980s, was released under the Macy\\'s label. Madonna also opened a series of fitness centers around the world named Hard Candy Fitness. In November 2011, Madonna and MG Icon announced the release of a second fashion brand called Truth or Dare by Madonna to include footwear, underclothing, and accessories. She also directed her second feature film, W.E., a biographic about the affair between King Edward VIII and Wallis Simpson; it was co-written with Alek Keshishian. Critical and commercial response to the film was negative. Madonna contributed the ballad \"Masterpiece\" for the film\\'s soundtrack, which won her a Golden Globe Award for Best Original Song. Question: Material Girl clothing line is released under which brand?',\n",
    "                     'Answer': 'Macy\\'s.',\n",
    "                     'CoT':'\"Material Girl clothing line\" is first mentioned in the first sentence of the passage. The second sentence further discusses the clothing line, saying that it \"was released under the Macy\\'s label\".'}\n",
    "    \n",
    "    pos_example_2 = {'input':'Passage: In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system and moving away from the university\\'s traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame continued to grow over the years, adding more colleges, programs, and sports teams. By 1921, with the addition of the College of Commerce, Notre Dame had grown from a small college to a university with five colleges and a professional law school. The university continued to expand and add new residence halls and buildings with each subsequent president. Question: Over how many years did the change to national standards undertaken at Notre Dame in the early 20th century take place?',\n",
    "                     'Answer':'three years.',\n",
    "                     'CoT':'The \"national standards\" are mentioned in the first sentence of the passage, where it is stated that \"In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards\".'}\n",
    "\n",
    "    neg_example_1 = {'input':'Passage: The descendants of Rollo\\'s Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, blending their maternal Frankish heritage with Old Norse traditions and customs to synthesize a unique \"Norman\" culture in the north of France. The Norman language was forged by the adoption of the indigenous langue d\\'o誰l branch of Romance by a Norse-speaking ruling class, and it developed into the regional language that survives today. Question: What was replaced with the Norse religion?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'In the passage, it is mentioned that Norse religion was replaced with Catholicism, whereas the question asks about the entity which was replaced by Norse religion. No such information is provided in the passage.'}\n",
    "\n",
    "\n",
    "    pos_answerability_example_1 = {'input': 'Passage: Madonna released the Material Girl clothing line, which she designed with her daughter, Lourdes. The 1980s inspired clothing line, borrowed from Madonna\\'s punk-girl style when she rose to fame in the 1980s, was released under the Macy\\'s label. Madonna also opened a series of fitness centers around the world named Hard Candy Fitness. In November 2011, Madonna and MG Icon announced the release of a second fashion brand called Truth or Dare by Madonna to include footwear, underclothing, and accessories. She also directed her second feature film, W.E., a biographic about the affair between King Edward VIII and Wallis Simpson; it was co-written with Alek Keshishian. Critical and commercial response to the film was negative. Madonna contributed the ballad \"Masterpiece\" for the film\\'s soundtrack, which won her a Golden Globe Award for Best Original Song. Question: Material Girl clothing line is released under which brand?',\n",
    "                     'Answer': 'answerable.',\n",
    "                     'CoT':'\"Material Girl clothing line\" is first mentioned in the first sentence of the passage. The second sentence further discusses the clothing line, saying that it \"was released under the Macy\\'s label\".'}\n",
    "\n",
    "\n",
    "    neg_answerability_example_1 = {'input':'Passage: The descendants of Rollo\\'s Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, blending their maternal Frankish heritage with Old Norse traditions and customs to synthesize a unique \"Norman\" culture in the north of France. The Norman language was forged by the adoption of the indigenous langue d\\'o誰l branch of Romance by a Norse-speaking ruling class, and it developed into the regional language that survives today. Question: What was replaced with the Norse religion?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'In the passage, it is mentioned that Norse religion was replaced with Catholicism, whereas the question asks about the entity which was replaced by Norse religion. No such information is provided in the passage.'}\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icl_examples_squad_v2():\n",
    "    pos_example_1 = {'input': 'Passage: In November 2013 MGM and the McClory estate formally settled the issue with Danjaq, LLCsister company of Eon Productionswith MGM acquiring the full copyright film rights to the concept of Spectre and all of the characters associated with it. With the acquisition of the film rights and the organisation\\'s re-introduction to the series\\' continuity, the SPECTRE acronym was discarded and the organisation reimagined as \"Spectre\".  Question: Which two parties settled the issue in November 2003?',\n",
    "                     'Answer': 'MGM and the McClory estate.',\n",
    "                     'CoT':'The passage starts by saying that in November 2013 MGM and Mclory estate formally stated the issue with Danjaq.'}\n",
    "    \n",
    "    pos_example_2 = {'input':'Passage: Genome composition is used to describe the make up of contents of a haploid genome, which should include genome size, proportions of non-repetitive DNA and repetitive DNA in details. By comparing the genome compositions between genomes, scientists can better understand the evolutionary history of a given genome. What aspect of a genome can genome compositions help researchers in learning about?',\n",
    "                     'Answer':'evolutionary history.',\n",
    "                     'CoT':'The second paragraph sentence mentions that comparing genome composition can help scientists better understand the evolutionary history of a given genome. This evolutionary history is one aspect of a genome.'}\n",
    "\n",
    "    neg_example_1 = {'input':'Passage: The story focuses on series protagonist Link, who tries to prevent Hyrule from being engulfed by a corrupted parallel dimension known as the Twilight Realm. To do so, he takes the form of both a Hylian and a wolf, and is assisted by a mysterious creature named Midna. The game takes place hundreds of years after Ocarina of Time and Majora\\'s Mask, in an alternate timeline from The Wind Waker. Question: What land does Ocarina serve to protect?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'In the passage, the only character mentioned as trying to save some land is Link, where as the question asks what country does Ocarina serves to protect. No such information is provided in the passage.'}\n",
    "\n",
    "\n",
    "    pos_answerability_example_1 = {'input': 'Passage: In November 2013 MGM and the McClory estate formally settled the issue with Danjaq, LLCsister company of Eon Productionswith MGM acquiring the full copyright film rights to the concept of Spectre and all of the characters associated with it. With the acquisition of the film rights and the organisation\\'s re-introduction to the series\\' continuity, the SPECTRE acronym was discarded and the organisation reimagined as \"Spectre\".  Question: Which two parties settled the issue in November 2003?',\n",
    "                     'Answer': 'answerable.',\n",
    "                     'CoT':'The passage starts by saying that in November 2013 MGM and Mclory estate formally stated the issue with Danjaq.'}\n",
    "\n",
    "\n",
    "    neg_answerability_example_1 = {'input':'Passage: The story focuses on series protagonist Link, who tries to prevent Hyrule from being engulfed by a corrupted parallel dimension known as the Twilight Realm. To do so, he takes the form of both a Hylian and a wolf, and is assisted by a mysterious creature named Midna. The game takes place hundreds of years after Ocarina of Time and Majora\\'s Mask, in an alternate timeline from The Wind Waker. Question: What land does Ocarina serve to protect?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'In the passage, the only character mentioned as trying to save some land is Link, where as the question asks what country does Ocarina serves to protect. No such information is provided in the passage.'}\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icl_examples_squad_v3():\n",
    "    pos_example_1 = {'input': 'Passage: Thomas Newman returned as Spectre\\'s composer. Rather than composing the score once the film had moved into post-production, Newman worked during filming. The theatrical trailer released in July 2015 contained a rendition of John Barry\\'s On Her Majesty\\'s Secret Service theme. Mendes revealed that the final film would have more than one hundred minutes of music. The soundtrack album was released on 23 October 2015 in the UK and 6 November 2015 in the USA on the Decca Records label.  Question: Who wrote the music for Spectre?',\n",
    "                     'Answer': 'Thomas Newman.',\n",
    "                     'CoT':'The passage starts by saying that Thomas Newman returned as Spectra\\'s composer. That means that he was the one to write its music.'}\n",
    "    \n",
    "    pos_example_2 = {'input':'Passage: Between 64 and 104 major aftershocks, ranging in magnitude from 4.0 to 6.1, were recorded within 72 hours of the main quake. According to Chinese official counts, \"by 12:00 CST, November 6, 2008 there had been 42,719 total aftershocks, of which 246 ranged from 4.0 MS to 4.9 MS, 34 from 5.0 MS to 5.9 MS, and 8 from 6.0 Ms to 6.4 MS; the strongest aftershock measured 6.4 MS.\" The latest aftershock exceeding M6 occurred on August 5, 2008. Question: What do the Chinese say is the total number of shocks after the quake?',\n",
    "                     'Answer':'42,719',\n",
    "                     'CoT':'The first paragraph sentence discusses the number of aftershocks following the earthquake. The second paragraph sentence says that according to Chinese official counts, the total number was 42,719.'}\n",
    "\n",
    "    neg_example_1 = {'input':'Passage: Both the number of base pairs and the number of genes vary widely from one species to another, and there is only a rough correlation between the two (an observation known as the C-value paradox). At present, the highest known number of genes is around 60,000, for the protozoan causing trichomoniasis (see List of sequenced eukaryotic genomes), almost three times as many as in the human genome. Question: What is the highest known number of species?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'In the passage, it is mentioned that the highest known number of genes is around 60,000, whereas the question asks about the highest number of species. No such information is provided in the passage.'}\n",
    "\n",
    "\n",
    "    pos_answerability_example_1 = {'input': 'Passage: Thomas Newman returned as Spectre\\'s composer. Rather than composing the score once the film had moved into post-production, Newman worked during filming. The theatrical trailer released in July 2015 contained a rendition of John Barry\\'s On Her Majesty\\'s Secret Service theme. Mendes revealed that the final film would have more than one hundred minutes of music. The soundtrack album was released on 23 October 2015 in the UK and 6 November 2015 in the USA on the Decca Records label.  Question: Who wrote the music for Spectre?',\n",
    "                     'Answer': 'answerable.',\n",
    "                     'CoT':'The passage starts by saying that Thomas Newman returned as Spectra\\'s composer. That means that he was the one to write its music.'}\n",
    "\n",
    "\n",
    "    neg_answerability_example_1 = {'input': 'Passage: Both the number of base pairs and the number of genes vary widely from one species to another, and there is only a rough correlation between the two (an observation known as the C-value paradox). At present, the highest known number of genes is around 60,000, for the protozoan causing trichomoniasis (see List of sequenced eukaryotic genomes), almost three times as many as in the human genome. Question: What is the highest known number of species?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'In the passage, it is mentioned that the highest known number of genes is around 60,000, whereas the question asks about the highest number of species. No such information is provided in the passage.'}\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_to_string_squad(example, instruction, isIcl, isCoT):\n",
    "    example_str = f\"Instructions: {instruction}\\n {example['input']}\\n Answer:\"\n",
    "\n",
    "    if isIcl:\n",
    "        if isCoT:\n",
    "            example_str = f\"{example_str} {example['CoT']} So, the answer is: {example['Answer']}\"\n",
    "        else:\n",
    "            example_str = f\"{example_str} {example['Answer']}\"\n",
    "    return example_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_with_instructions_prompts_squad(curr_data, data_type):\n",
    "    if ICL_examples_variant == 1:\n",
    "        pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1 = get_icl_examples_squad_v1()\n",
    "    elif ICL_examples_variant == 2:\n",
    "        pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1 = get_icl_examples_squad_v2()\n",
    "    elif ICL_examples_variant == 3:\n",
    "        pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1 = get_icl_examples_squad_v3()\n",
    "    else:\n",
    "        raise Exception(f\"ICL_examples_variant can only be any of 1,2,3, but is currently {ICL_examples_variant}\")\n",
    "    instructions = {'Adversarial': 'Given the following passage and question, answer the question.',\n",
    "                    'Pseudo-Adversarial': 'Given the following passage and question, answer the question. If it cannot be answered based on the passage, reply \"unanswerable\".',\n",
    "                    'Answerability': 'Given the following passage and question, decide if the question is answerable based on the passage. Reply only \"answerable\" or \"unanswerable\".'}\n",
    "    prompt_list_few_shot_with_instructions = list()\n",
    "    for elem in curr_data:\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        if data_type == \"adversarial\":\n",
    "            prompt_elem[\"Unanswerability-Reason\"] = question_to_reason_dict[elem[\"id\"]]\n",
    "\n",
    "\n",
    "        # Adversarial\n",
    "        prompt_elem[\"Adversarial\"] = f\"\"\"\n",
    "                                    {example_to_string_squad(pos_example_1, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_squad(pos_example_2, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_squad(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        prompt_elem[\"Adversarial-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string_squad(pos_example_1, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_squad(pos_example_2, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_squad(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        # Pseudo-Adversarial\n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"\n",
    "                                    {example_to_string_squad(pos_example_1, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_squad(neg_example_1, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_squad(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        \n",
    "        prompt_elem[\"Pseudo-Adversarial-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string_squad(pos_example_1, instructions['Pseudo-Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_squad(neg_example_1, instructions['Pseudo-Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_squad(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        # Ablation1\n",
    "        prompt_elem[\"Ablation1\"] = f\"\"\"\n",
    "                                    {example_to_string_squad(pos_example_1, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_squad(pos_example_2, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_squad(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        \n",
    "        prompt_elem[\"Ablation1-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string_squad(pos_example_1, instructions['Pseudo-Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_squad(pos_example_2, instructions['Pseudo-Adversarial'], True, True)}\n",
    "                                    \n",
    "                                    {example_to_string_squad(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        # Ablation2\n",
    "        prompt_elem[\"Ablation2\"] = f\"\"\"\n",
    "                                    {example_to_string_squad(pos_example_1, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_squad(neg_example_1, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_squad(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        \n",
    "        prompt_elem[\"Ablation2-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string_squad(pos_example_1, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_squad(neg_example_1, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_squad(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        # Answerability\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"\n",
    "                                    {example_to_string_squad(pos_answerability_example_1, instructions['Answerability'], True, False)}\n",
    "\n",
    "                                    {example_to_string_squad(neg_answerability_example_1, instructions['Answerability'], True, False)}\n",
    "\n",
    "                                    {example_to_string_squad(elem, instructions['Answerability'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        prompt_elem[\"Answerability-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string_squad(pos_answerability_example_1, instructions['Answerability'], True, True)}\n",
    "\n",
    "                                    {example_to_string_squad(neg_answerability_example_1, instructions['Answerability'], True, True)}\n",
    "\n",
    "                                    {example_to_string_squad(elem, instructions['Answerability'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        for key,value in prompt_elem.items():\n",
    "            if key in [\"example_id\", \"annotation_id\"]:\n",
    "                continue\n",
    "            prompt_elem[key] = f\" {re.sub(' +', ' ', value).strip()}\"\n",
    "        prompt_list_few_shot_with_instructions.append(prompt_elem)\n",
    "    return prompt_list_few_shot_with_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_adversarial_or_control_group(data, data_types):\n",
    "    is_control_group = True if data_types==\"control_group\" else False\n",
    "    filtered_data = []\n",
    "    for elem in data:\n",
    "        for paragraph in elem[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                sample = f\"Passage: {paragraph['context']} Question: {qa['question']}\"\n",
    "                if (is_control_group and len(qa[\"answers\"])>0) or (not is_control_group and len(qa[\"answers\"])==0):\n",
    "                    filtered_data.append({\"id\":qa['id'], \"input\":sample})\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [\"adversarial\", \"control_group\"]\n",
    "for data_type in data_types:\n",
    "    with open(squad_indir, 'r') as f1:\n",
    "        curr_data = json.load(f1)\n",
    "        curr_data = curr_data[\"data\"]\n",
    "        curr_data = filter_adversarial_or_control_group(curr_data, data_type)\n",
    "    curr_output = get_few_shot_with_instructions_prompts_squad(curr_data, data_type)\n",
    "    curr_outdir = os.path.join(squad_few_shot_with_instructions_outdir, f\"squad_{data_type}_icl_examples_v{ICL_examples_variant}_all.json\")\n",
    "    with open(curr_outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(curr_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set (for training the classifier) - zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = r\"data/squad/train-v2.0.json\"\n",
    "outdir = r\"generated_prompts/train_set\"\n",
    "IS_CONTROL_GROUP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(indir) as f:\n",
    "    data = json.load(f)\n",
    "    data = data[\"data\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter only (un)answerable questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = []\n",
    "for elem in data:\n",
    "    for paragraph in elem[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            sample = f\"Passage: {paragraph['context']} Question: {qa['question']}\"\n",
    "            if (IS_CONTROL_GROUP and len(qa[\"answers\"])>0) or (not IS_CONTROL_GROUP and len(qa[\"answers\"])==0):\n",
    "                filtered_data.append({\"id\":qa['id'], \"input\":sample})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate prompts (zero shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot_version2 = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"Given the following passage and question, answer the question:\n",
    "        {elem[\"input\"]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.\n",
    "    If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "        {elem[\"input\"]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"CoT-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.\n",
    "    First make sure if it can be answered by the passage.\n",
    "    If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "        {elem[\"input\"]}\"\"\"  \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"Given the following passage and question, decide if the question is answerable based on the passage. Reply only \"answerable\" or \"unanswerable\":\n",
    "        {elem[\"input\"]}\"\"\" \n",
    "\n",
    "    prompt_list_zero_shot_version2.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outdir):\n",
    "   os.makedirs(outdir)\n",
    "\n",
    "if not os.path.exists(os.path.join(outdir, \"all\")):\n",
    "   os.makedirs(os.path.join(outdir, \"all\"))\n",
    "\n",
    "if not os.path.exists(os.path.join(outdir, \"all\", \"zero_shot\")):\n",
    "   os.makedirs(os.path.join(outdir, \"all\", \"zero_shot\"))\n",
    "\n",
    "\n",
    "\n",
    "split_type = \"control_group\" if IS_CONTROL_GROUP else \"adversarial\"\n",
    "\n",
    "# zero shot\n",
    "with open(os.path.join(outdir, \"all\", \"zero_shot\", f\"squad_trainset_{split_type}.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_zero_shot_version2, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NQ_indir = \"../data/NQ\"\n",
    "NQ_outdir = \"../generated_prompts/all/zero_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts_NQ(curr_data):\n",
    "    prompt_list_zero_shot = list()\n",
    "    for elem in curr_data:\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"example_id\"] = elem[\"example_id\"]\n",
    "        prompt_elem[\"annotation_id\"] = elem[\"annotation_id\"]\n",
    "\n",
    "        elem[\"Question\"] = elem[\"Question\"] if elem[\"Question\"].endswith(\"?\") else f'{elem[\"Question\"]} ?'\n",
    "\n",
    "        prompt_elem[\"Adversarial\"] = f\"\"\"Given the following passage and question, answer the question:\n",
    "            Passage: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "        \n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.\n",
    "        If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "            Passage: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "        \n",
    "        prompt_elem[\"CoT-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.\n",
    "        First make sure if it can be answered by the passage.\n",
    "        If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "            Passage: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"Given the following passage and question, decide if the question is answerable based on the passage. Reply only \"answerable\" or \"unanswerable\":\n",
    "            Passage: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "\n",
    "        prompt_list_zero_shot.append(prompt_elem)\n",
    "    return prompt_list_zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [\"adversarial\", \"control_group\"]\n",
    "for data_type in data_types:\n",
    "    with open(os.path.join(NQ_indir, f\"{data_type}_NQ.jsonl\"), 'r') as f1:\n",
    "        curr_data = json.loads(f1.read())\n",
    "    curr_output = get_prompts_NQ(curr_data)\n",
    "    curr_outdir = os.path.join(NQ_outdir, f\"NQ_{data_type}_all.json\")\n",
    "    with open(curr_outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(curr_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot with Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICL_examples_variant = 3 # any of 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "NQ_indir = \"../data/NQ\"\n",
    "NQ_few_shot_with_instructions_outdir = \"../generated_prompts/all/few_shot_with_instructions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icl_examples_NQ_v1():\n",
    "    pos_example_1 = {'Paragraphs': 'Hypoxia differs from hypoxemia and anoxemia in that hypoxia refers to a state in which oxygen supply is insufficient , whereas hypoxemia and anoxemia refer specifically to states that have low or zero arterial oxygen supply . Hypoxia in which there is complete deprivation of oxygen supply is referred to as anoxia .',\n",
    "                     'Question': 'a medical term which means a deficiency but not a total lack of oxygen ?',\n",
    "                     'Answer': 'hypoxia.',\n",
    "                     'CoT':'The passage mentions that Hypoxia is a state in which oxygen supply is insufficient. It further describes that Hypoxia in which there is complete deprivation of oxygen supply is referred to as anoxia, meaning that Hypoxia itself is not a total lack of oxygen.'}\n",
    "    \n",
    "    pos_example_2 = {'Paragraphs':'South Africa have played at six of the eight Rugby World Cup tournaments , having been unable to compete in the first two tournaments due to a sports boycott during the apartheid era . Following the end of apartheid , they hosted the 1995 Rugby World Cup and won the tournament , and were champions again at the 2007 tournament in France . With two tournament wins , they are one of the three best performing teams , along with Australia who have also won twice , and New Zealand with three wins , the only team to do better .',\n",
    "                     'Question': 'when did south africa first win the rugby world cup ?',\n",
    "                     'Answer':'1995.',\n",
    "                     'CoT':'The passage mentions that South Africa won the 1995 Rugby World Cup. The passage further says that they won again in 2007. Lastly, the passsage says that they had two wins, meaning that the win of 1995 was their first.'}\n",
    "\n",
    "    neg_example_1 = {'Paragraphs':'The Act of Settlement is an Act of the Parliament of England that was passed in 1701 to settle the succession to the English and Irish crowns on Protestants only . The next Protestant in line to the throne was the Electress Sophia of Hanover , a granddaughter of James VI of Scotland and I of England . After her the crowns would descend only to her non-Roman Catholic heirs .',\n",
    "                     'Question': 'The next Roman in line to the throne ?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'In the passage, it is mentioned that the next person in line to the throne is Protestant, and not Roman. It is also said that the crown would descend to her non-Roman heirs.'}\n",
    "\n",
    "    pos_answerability_example_1 = {'Paragraphs': 'Hypoxia differs from hypoxemia and anoxemia in that hypoxia refers to a state in which oxygen supply is insufficient , whereas hypoxemia and anoxemia refer specifically to states that have low or zero arterial oxygen supply . Hypoxia in which there is complete deprivation of oxygen supply is referred to as anoxia .',\n",
    "                     'Question': 'a medical term which means a deficiency but not a total lack of oxygen ?',\n",
    "                     'Answer': 'answerable.',\n",
    "                     'CoT':'The passage mentions that Hypoxia is a state in which oxygen supply is insufficient. It further describes that Hypoxia in which there is complete deprivation of oxygen supply is referred to as anoxia, meaning that Hypoxia itself is not a total lack of oxygen.'}\n",
    "\n",
    "    neg_answerability_example_1 = {'Paragraphs':'The Act of Settlement is an Act of the Parliament of England that was passed in 1701 to settle the succession to the English and Irish crowns on Protestants only . The next Protestant in line to the throne was the Electress Sophia of Hanover , a granddaughter of James VI of Scotland and I of England . After her the crowns would descend only to her non-Roman Catholic heirs .',\n",
    "                     'Question': 'The next Roman in line to the throne ?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'In the passage, it is mentioned that the next person in line to the throne is Protestant, and not Roman. It is also said that the crown would descend to her non-Roman heirs.'}\n",
    "\n",
    "\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icl_examples_NQ_v2():\n",
    "    pos_example_1 = {'Paragraphs': 'Louise Joy Brown ( born 25 July 1978 ) is an English woman known for being the first human to have been born after conception by in vitro fertilisation , or IVF .',\n",
    "                     'Question': 'when was the first in vitro baby born ?',\n",
    "                     'Answer': '25 July 1978.',\n",
    "                     'CoT':'The passage says that Louise Joy Brown was the first human to have been born by in vitro fertilisation. It also mentions that Louise Joy Brown was born on 25 July 1978.'}\n",
    "    \n",
    "    pos_example_2 = {'Paragraphs':'The 2018 College Football Playoff National Championship was a college football bowl game that determined the national champion in the NCAA Division I Football Bowl Subdivision for the 2017 season . The Alabama Crimson Tide defeated the Georgia Bulldogs 26 -- 23 in overtime . Alabama overcame a 13 -- 0 deficit at halftime . Tua Tagovailoa and Da\\'Ron Payne were respectively named the offensive and defensive players of the game .',\n",
    "                     'Question': 'who won the college football national championship tonight ?',\n",
    "                     'Answer':'The Alabama Crimson Tide.',\n",
    "                     'CoT':'The passage mentions starts by talking about the Football Playoff National Championship. It then says that the Alabama Crimson Tide defeated the Georgia Bulldogs.'}\n",
    "\n",
    "    neg_example_1 = {'Paragraphs':'The Ranch is an American comedy web television series starring Ashton Kutcher , Danny Masterson , Debra Winger , Elisha Cuthbert , and Sam Elliott that debuted in 2016 on Netflix . The show takes place on the fictional Iron River Ranch in the fictitious small town of Garrison , Colorado ; detailing the life of the Bennetts , a dysfunctional family consisting of two brothers , their rancher father , and his divorced wife and local bar owner . While the opening sequence shows scenes from Ouray , Colorado and surrounding Ouray County , The Ranch is filmed on a sound stage in front of a live audience in Burbank , California . Each season consists of 20 episodes broken up into two parts , each containing 10 episodes .',\n",
    "                     'Question': 'when does the next series of the ranch come out ?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'The passage portrays the \"the Ranch\", including when it debuted, but it doesn\\'t say when is its next series.'}\n",
    "\n",
    "    pos_answerability_example_1 = {'Paragraphs': 'Louise Joy Brown ( born 25 July 1978 ) is an English woman known for being the first human to have been born after conception by in vitro fertilisation , or IVF .',\n",
    "                     'Question': 'when was the first in vitro baby born ?',\n",
    "                     'Answer': 'answerable.',\n",
    "                     'CoT':'The passage says that Louise Joy Brown was the first human to have been born by in vitro fertilisation. It also mentions that Louise Joy Brown was born on 25 July 1978.'}\n",
    "\n",
    "    neg_answerability_example_1 = {'Paragraphs':'The Ranch is an American comedy web television series starring Ashton Kutcher , Danny Masterson , Debra Winger , Elisha Cuthbert , and Sam Elliott that debuted in 2016 on Netflix . The show takes place on the fictional Iron River Ranch in the fictitious small town of Garrison , Colorado ; detailing the life of the Bennetts , a dysfunctional family consisting of two brothers , their rancher father , and his divorced wife and local bar owner . While the opening sequence shows scenes from Ouray , Colorado and surrounding Ouray County , The Ranch is filmed on a sound stage in front of a live audience in Burbank , California . Each season consists of 20 episodes broken up into two parts , each containing 10 episodes .',\n",
    "                     'Question': 'when does the next series of the ranch come out ?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'The passage portrays the \"the Ranch\", including when it debuted, but it doesn\\'t say when is its next series.'}\n",
    "\n",
    "\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icl_examples_NQ_v3():\n",
    "    pos_example_1 = {'Paragraphs':'`` Fool ( If You Think It \\'s Over ) \\'\\' is the title of a popular song originally publicly released in 1978 by the British singer - songwriter Chris Rea . Rea also wrote the words and composed the music of the song , which appears on his 1978 debut album , Whatever Happened to Benny Santini ? . The single \\'s charting success in the USA earned him a Grammy nomination as Best New Artist in 1979 .',\n",
    "                     'Question': 'who sang fool if you think it over ?',\n",
    "                     'Answer':'Chris Rea.',\n",
    "                     'CoT':'The passage says that ``Fool ( If You Think It \\'s Over )\\'\\' is the title of a popular song. It also says that it was originally publicly released by Chris Rea.'}\n",
    "\n",
    "    pos_example_2 = {'Paragraphs': 'The Mississippi Freedom Democratic Party ( MFDP ) was an American political party created in 1964 as a branch of the populist Freedom Democratic organization in the state of Mississippi during the Civil Rights Movement . It was organized by African Americans and whites from Mississippi to challenge the legitimacy of the regular Mississippi Democratic Party , which allowed participation only by whites , when African Americans made up 40 percent of the state population .',\n",
    "                     'Question': 'why did the mississippi freedom democratic party emerge at the democratic party convention in 1964 ?',\n",
    "                     'Answer': 'to challenge the legitimacy of the regular Mississippi Democratic Party , which allowed participation only by whites , when African Americans made up 40 percent of the state population.',\n",
    "                     'CoT':'The passage says that the Mississippi Freedom Democratic Party was created in 1964. It then says that it was organized to challenge the legitimacy of the regular Mississippi Democratic Party , which allowed participation only by whites , when African Americans made up 40 percent of the state population .'}\n",
    "\n",
    "\n",
    "\n",
    "    neg_example_1 = {'Paragraphs':'Owing in part to the way in which the United Kingdom , and Northern Ireland , came into being , there is no legally defined term to describe what Northern Ireland \\' is \\' . There is also no uniform or guiding way to refer to Northern Ireland amongst the agencies of the UK government . For example , the websites of the Office of the Prime Minister of the United Kingdom and the UK Statistics Authority describe the United Kingdom as being made up of four countries , one of these being Northern Ireland . Other pages on the same websites refer to Northern Ireland specifically as a `` province \\'\\' as do publications of the UK Statistics Authority . The website of the Northern Ireland Statistics and Research Agency also refers to Northern Ireland as being a province as does the website of the Office of Public Sector Information and other agencies within Northern Ireland . Publications of HM Treasury and the Department of Finance and Personnel of the Northern Ireland Executive , on the other hand , describe Northern Ireland as being a `` region of the UK \\'\\' . The UK \\'s submission to the 2007 United Nations Conference on the Standardization of Geographical Names defines the UK as being made up of two countries ( England and Scotland ) , one principality ( Wales ) and one province ( Northern Ireland ) .',\n",
    "                     'Question': 'why is northern ireland not part of ireland ?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'The passage discusses Northern Ireland\\'s unclear legal status as part of the UK. However, it does not mention why is Northern Ireland a part of the UK and not Ireland.'}\n",
    "\n",
    "    pos_answerability_example_1 = {'Paragraphs':'`` Fool ( If You Think It \\'s Over ) \\'\\' is the title of a popular song originally publicly released in 1978 by the British singer - songwriter Chris Rea . Rea also wrote the words and composed the music of the song , which appears on his 1978 debut album , Whatever Happened to Benny Santini ? . The single \\'s charting success in the USA earned him a Grammy nomination as Best New Artist in 1979 .',\n",
    "                     'Question': 'who sang fool if you think it over ?',\n",
    "                     'Answer': 'answerable.',\n",
    "                     'CoT':'The passage says that ``Fool ( If You Think It \\'s Over )\\'\\' is the title of a popular song. It also says that it was originally publicly released by Chris Rea.'}\n",
    "\n",
    "    neg_answerability_example_1 = {'Paragraphs': 'Owing in part to the way in which the United Kingdom , and Northern Ireland , came into being , there is no legally defined term to describe what Northern Ireland \\' is \\' . There is also no uniform or guiding way to refer to Northern Ireland amongst the agencies of the UK government . For example , the websites of the Office of the Prime Minister of the United Kingdom and the UK Statistics Authority describe the United Kingdom as being made up of four countries , one of these being Northern Ireland . Other pages on the same websites refer to Northern Ireland specifically as a `` province \\'\\' as do publications of the UK Statistics Authority . The website of the Northern Ireland Statistics and Research Agency also refers to Northern Ireland as being a province as does the website of the Office of Public Sector Information and other agencies within Northern Ireland . Publications of HM Treasury and the Department of Finance and Personnel of the Northern Ireland Executive , on the other hand , describe Northern Ireland as being a `` region of the UK \\'\\' . The UK \\'s submission to the 2007 United Nations Conference on the Standardization of Geographical Names defines the UK as being made up of two countries ( England and Scotland ) , one principality ( Wales ) and one province ( Northern Ireland ) .',\n",
    "                     'Question': 'why is northern ireland not part of ireland ?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'The passage discusses Northern Ireland\\'s unclear legal status as part of the UK. However, it does not mention why is Northern Ireland a part of the UK and not Ireland.'}\n",
    "\n",
    "\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_to_string(example, instruction, isIcl, isCoT):\n",
    "    example_str = f\"Instructions: {instruction}\\n Passage: {example['Paragraphs']}\\n Question: {example['Question']}\\n Answer:\"\n",
    "\n",
    "    if isIcl:\n",
    "        if isCoT:\n",
    "            example_str = f\"{example_str} {example['CoT']} So, the answer is: {example['Answer']}\"\n",
    "        else:\n",
    "            example_str = f\"{example_str} {example['Answer']}\"\n",
    "    return example_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_with_instructions_prompts_NQ(curr_data):\n",
    "    if ICL_examples_variant == 1:\n",
    "        pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1 = get_icl_examples_NQ_v1()\n",
    "    elif ICL_examples_variant == 2:\n",
    "        pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1 = get_icl_examples_NQ_v2()\n",
    "    elif ICL_examples_variant == 3:\n",
    "        pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1 = get_icl_examples_NQ_v3()\n",
    "    else:\n",
    "        raise Exception(f\"ICL_examples_variant can only be any of 1,2,3, but is currently {ICL_examples_variant}\")\n",
    "    \n",
    "    instructions = {'Adversarial': 'Given the following passage and question, answer the question.',\n",
    "                    'Pseudo-Adversarial': 'Given the following passage and question, answer the question. If it cannot be answered based on the passage, reply \"unanswerable\".',\n",
    "                    'Answerability': 'Given the following passage and question, decide if the question is answerable based on the passage. Reply only \"answerable\" or \"unanswerable\".'}\n",
    "    prompt_list_few_shot_with_instructions = list()\n",
    "    for elem in curr_data:\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"example_id\"] = elem[\"example_id\"]\n",
    "        prompt_elem[\"annotation_id\"] = elem[\"annotation_id\"]\n",
    "\n",
    "        elem[\"Question\"] = elem[\"Question\"] if elem[\"Question\"].endswith(\"?\") else f\"{elem['Question']} ?\"\n",
    "        prompt_elem[\"Answer\"] = elem[\"answer\"]\n",
    "        # Adversarial\n",
    "        prompt_elem[\"Adversarial\"] = f\"\"\"\n",
    "                                    {example_to_string(pos_example_1, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string(pos_example_2, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        prompt_elem[\"Adversarial-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string(pos_example_1, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string(pos_example_2, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        # Pseudo-Adversarial\n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"\n",
    "                                    {example_to_string(pos_example_1, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string(neg_example_1, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        \n",
    "        prompt_elem[\"Pseudo-Adversarial-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string(pos_example_1, instructions['Pseudo-Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string(neg_example_1, instructions['Pseudo-Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        # Ablation1\n",
    "        prompt_elem[\"Ablation1\"] = f\"\"\"\n",
    "                                    {example_to_string(pos_example_1, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string(pos_example_2, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        \n",
    "        prompt_elem[\"Ablation1-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string(pos_example_1, instructions['Pseudo-Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string(pos_example_2, instructions['Pseudo-Adversarial'], True, True)}\n",
    "                                    \n",
    "                                    {example_to_string(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        # Ablation2\n",
    "        prompt_elem[\"Ablation2\"] = f\"\"\"\n",
    "                                    {example_to_string(pos_example_1, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string(neg_example_1, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        \n",
    "        prompt_elem[\"Ablation2-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string(pos_example_1, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string(neg_example_1, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        # Answerability\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"\n",
    "                                    {example_to_string(pos_answerability_example_1, instructions['Answerability'], True, False)}\n",
    "\n",
    "                                    {example_to_string(neg_answerability_example_1, instructions['Answerability'], True, False)}\n",
    "\n",
    "                                    {example_to_string(elem, instructions['Answerability'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        prompt_elem[\"Answerability-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string(pos_answerability_example_1, instructions['Answerability'], True, True)}\n",
    "\n",
    "                                    {example_to_string(neg_answerability_example_1, instructions['Answerability'], True, True)}\n",
    "\n",
    "                                    {example_to_string(elem, instructions['Answerability'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        for key,value in prompt_elem.items():\n",
    "            if key in [\"example_id\", \"annotation_id\"]:\n",
    "                continue\n",
    "            prompt_elem[key] = f\" {re.sub(' +', ' ', value).strip()}\"\n",
    "        prompt_list_few_shot_with_instructions.append(prompt_elem)\n",
    "    return prompt_list_few_shot_with_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [\"adversarial\", \"control_group\"]\n",
    "for data_type in data_types:\n",
    "    with open(os.path.join(NQ_indir, f\"{data_type}_NQ.jsonl\"), 'r') as f1:\n",
    "        curr_data = json.loads(f1.read())\n",
    "    curr_output = get_few_shot_with_instructions_prompts_NQ(curr_data)\n",
    "    curr_outdir = os.path.join(NQ_few_shot_with_instructions_outdir, f\"NQ_{data_type}_icl_examples_v{ICL_examples_variant}_all.json\")\n",
    "    with open(curr_outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(curr_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set (for training the classifier) - zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NQ_indir = \"data/NQ\"\n",
    "NQ_outdir = \"generated_prompts/train_set/all/zero_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts_NQ(curr_data):\n",
    "    prompt_list_zero_shot = list()\n",
    "    for elem in curr_data:\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"example_id\"] = elem[\"example_id\"]\n",
    "        prompt_elem[\"annotation_id\"] = elem[\"annotation_id\"]\n",
    "\n",
    "        elem[\"Question\"] = elem[\"Question\"] if elem[\"Question\"].endswith(\"?\") else f'{elem[\"Question\"]} ?'\n",
    "\n",
    "        prompt_elem[\"Adversarial\"] = f\"\"\"Given the following passage and question, answer the question:\n",
    "            Passage: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "        \n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.\n",
    "        If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "            Passage: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "        \n",
    "        prompt_elem[\"CoT-Adversarial\"] = f\"\"\"Given the following passage and question, answer the question.\n",
    "        First make sure if it can be answered by the passage.\n",
    "        If it cannot be answered based on the passage, reply \"unanswerable\":\n",
    "            Passage: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"Given the following passage and question, decide if the question is answerable based on the passage. Reply only \"answerable\" or \"unanswerable\":\n",
    "            Passage: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "\n",
    "        prompt_list_zero_shot.append(prompt_elem)\n",
    "    return prompt_list_zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [\"adversarial\", \"control_group\"]\n",
    "for data_type in data_types:\n",
    "    with open(os.path.join(NQ_indir, f\"{data_type}_NQ_train.jsonl\"), 'r') as f1:\n",
    "        curr_data = json.loads(f1.read())\n",
    "    curr_output = get_prompts_NQ(curr_data)\n",
    "    curr_outdir = os.path.join(NQ_outdir, f\"NQ_trainset_{data_type}_all.json\")\n",
    "    with open(curr_outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(curr_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Musique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musique_indir = \"data/musique\"\n",
    "musique_outdir = \"generated_prompts/all/zero_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts_musique(curr_data):\n",
    "    prompt_list_zero_shot = list()\n",
    "    for elem in curr_data:\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        prompt_elem[\"Adversarial\"] = f\"\"\"Given the following context and question, answer the question:\n",
    "            Context: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "        \n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Given the following context and question, answer the question.\n",
    "        If it cannot be answered based on the context, reply \"unanswerable\":\n",
    "            Context: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "        \n",
    "        prompt_elem[\"CoT-Adversarial\"] = f\"\"\"Given the following context and question, answer the question.\n",
    "        First make sure if it can be answered by the context.\n",
    "        If it cannot be answered based on the context, reply \"unanswerable\":\n",
    "            Context: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"Given the following context and question, decide if the question is answerable based on the context. Reply only \"answerable\" or \"unanswerable\":\n",
    "            Context: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "\n",
    "        prompt_elem[\"answer\"] = elem[\"answer\"]\n",
    "\n",
    "        if \"actual_answer\" in elem.keys():\n",
    "            prompt_elem[\"actual_answer\"] = elem[\"actual_answer\"]\n",
    "\n",
    "        prompt_list_zero_shot.append(prompt_elem)\n",
    "    return prompt_list_zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [\"adversarial\", \"control_group\"]\n",
    "for data_type in data_types:\n",
    "    with open(os.path.join(musique_indir, f\"{data_type}_musique.jsonl\"), 'r') as f1:\n",
    "        curr_data = json.loads(f1.read())\n",
    "    curr_output = get_prompts_musique(curr_data)\n",
    "    curr_outdir = os.path.join(musique_outdir, f\"musique_{data_type}_all.json\")\n",
    "    with open(curr_outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(curr_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot with Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICL_examples_variant = 3 # any of 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "musique_indir = \"../data/musique\"\n",
    "musique_few_shot_with_instructions_outdir = \"../generated_prompts/all/few_shot_with_instructions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icl_examples_musique_v1():\n",
    "    pos_example_1 = {'Paragraphs': 'Paragraph 1: South Africa have played at six of the eight Rugby World Cup tournaments, having been unable to compete in the first two tournaments due to a sports boycott during the apartheid era. Following the end of apartheid, they hosted the 1995 Rugby World Cup and won the tournament.\\n Paragraph 2: With two tournament wins, South Africa is one of the three best performing teams, along with Australia who have also won twice, and New Zealand with three wins, the only team to do better.',\n",
    "                     'Question': 'How many times did the winner of the 1995 Rugby World Cup win in total?',\n",
    "                     'Answer': 'two times.',\n",
    "                     'CoT':'Paragraph 1 mentions that the winner of the 1995 Rugby World Cup was South Africa. Paragraph 2 mentions that South Africa had two tournament wins.'}\n",
    "    \n",
    "    pos_example_2 = {'Paragraphs':'Paragraph 1: Barack Obama is an American politician who served as the 44th president of the United States from 2009 to 2017.\\n Pargaraph 2: Obama married Michelle on October 3, 1992, after being engaged for almost a year.\\n Paragraph 3: Barack Obama was born in Honolulu, Hawaii. After graduating from Columbia University in 1983, he worked as a community organizer in Chicago.',\n",
    "                     'Question': 'What is the name of the wife of the American president who was born in Hawaii?',\n",
    "                     'Answer':'Michelle',\n",
    "                     'CoT':'Paragraph 1 mentions that Barack Obama was an American President. Paragraph 3 mentions that Barack Obama was born in Hawaii, making him the American president born in Hawaii. Paragraph 2 mentions that he married Michelle.'}\n",
    "\n",
    "    neg_example_1 = {'Paragraphs':'Paragraph 1: Barack Obama is an American politician who served as the 44th president of the United States from 2009 to 2017.\\n Pargaraph 2: Obama married Michelle on October 3, 1992, after being engaged for almost a year.',\n",
    "                     'Question': 'What is the name of the wife of the American president who was born in New York?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'Paragraph 1 mentions that Barack Obama was an American President. Paragraph 2 mentions that he married Michelle, but it doesn\\'t say where he was born. The context doesn\\'t have information on an american president born in New York.'}\n",
    "    \n",
    "    pos_answerability_example_1 = {'Paragraphs': 'Paragraph 1: South Africa have played at six of the eight Rugby World Cup tournaments, having been unable to compete in the first two tournaments due to a sports boycott during the apartheid era. Following the end of apartheid, they hosted the 1995 Rugby World Cup and won the tournament.\\n Paragraph 2: With two tournament wins, South Africa is one of the three best performing teams, along with Australia who have also won twice, and New Zealand with three wins, the only team to do better.',\n",
    "                     'Question': 'How many times did the winner of the 1995 Rugby World Cup win in total?',\n",
    "                     'Answer': 'answerable.',\n",
    "                     'CoT':'Paragraph 1 mentions that the winner of the 1995 Rugby World Cup was South Africa. Paragraph 2 mentions that South Africa had two tournament wins.'}\n",
    "\n",
    "    \n",
    "    neg_answerability_example_1 = {'Paragraphs':'Paragraph 1: Barack Obama is an American politician who served as the 44th president of the United States from 2009 to 2017.\\n Pargaraph 2: Obama married Michelle on October 3, 1992, after being engaged for almost a year.',\n",
    "                     'Question': 'What is the name of the wife of the American president who was born in New York?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'Paragraph 1 mentions that Barack Obama was an American President. Paragraph 2 mentions that he married Michelle, but it doesn\\'t say where he was born. The context doesn\\'t have information on an american president born in New York.'}\n",
    " \n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icl_examples_musique_v2():\n",
    "    pos_example_1 = {'Paragraphs': 'Paragraph 1: Kaya toast is a well-known snack in Singapore. Kaya toast is prepared with kaya (coconut jam), a topping of sugar, coconut milk and eggs, pandan, and sometimes margarine or butter. Kaya is generally served on toast, and also sometimes on crackers. It is considered a breakfast staple, and remains popular in Singapore. The dish is sometimes dipped into soft-boiled egg with a little dark soy sauce and white pepper.\\n Paragraph 2: A justice of the peace in Singapore derives his powers from statute law. He is appointed by the President of the Republic of Singapore, under the provisions of section 11 (l) of the Subordinate Courts Act (Cap. 321). The President may revoke the appointment of any justice of the peace. A newly appointed justice of the peace is required by section 17 of the Subordinate Courts Act, to take the oath of office and allegiance as set out in the schedule to the Subordinate Courts Act, before exercising the functions of his office.',\n",
    "                     'Question': 'How do you become a justice of peace in the country where Kaya toast is popular?',\n",
    "                     'Answer': 'appointed by the President of the Republic of Singapore.',\n",
    "                     'CoT':'Paragraph 1 mentions that Kaya toast is a well-known snack in Singapore. Paragraph 2 says that a justice of peace in Singapore derives his powers from statute law. It also says that he is appointed by the President of the Republic of Singapore.'}\n",
    "    \n",
    "    pos_example_2 = {'Paragraphs':'Paragraph 1: Mount Henry is located in the Lewis Range, Glacier National Park in the U.S. state of Montana. Mount Henry is just south of Appistoki Peak in the Two Medicine region of the park.\\n Paragraph 2: KJRZ-LP (105.3 FM) was a radio station in Libby, Montana. It was owned and operated by the Libby Area Chamber of Commerce.\\n Paragraph 3: The Lewis Range is a mountain range located in the Rocky Mountains of northern Montana, United States and extreme southern Alberta, Canada. It was formed as a result of the Lewis Overthrust, a geologic thrust fault resulted in the overlying of younger Cretaceous rocks by older Proterozoic rocks. The range is located within Waterton Lakes National Park in Alberta, Canada and Glacier National Park and the Bob Marshall Wilderness Complex in Montana, United States. The highest peak is Mount Cleveland at .',\n",
    "                     'Question': 'In what mountain group is the range of which Mount Henry from the state where KJRZ-LP is located is part?',\n",
    "                     'Answer':'Rocky Mountains',\n",
    "                     'CoT':'Paragraph 2 says that KJRZ-LP was in Libby, Montana. Paragraph 1 mentions that Mount Henry is located in the Lewis Range. It also says that Lewis Range is in the state of Montana. Paragraph 3 mentions that the Lewis Range is located in the Rocky Mountains.'}\n",
    "\n",
    "    neg_example_1 = {'Paragraphs':'Paragraph 1: WODS (103.3 MHz) - known on-air as 103.3 AMP Radio - is a commercial FM radio station in Boston, Massachusetts. WODS airs a Top 40 (CHR) radio format, and is owned by Entercom. Its studios and offices are located on Leo M. Birmingham Parkwary in Brighton.\\n Paragraph 2: The Embassy of the United States to the Republic of Indonesia is located in Jakarta just south of the Monas at Jalan Medan Merdeka Selatan.\\n Paragraph 3: Westminster College is a private liberal arts college located in the Sugar House neighborhood of Salt Lake City, Utah, United States. The college comprises four schools: the School of Arts and Sciences, the Bill and Vieve Gore School of Business, the School of Education, and the School of Nursing and Health Sciences. It is the only accredited liberal arts college in the state of Utah.\\n Paragraph 4: The Shorter House is located at the end of Andrews Road in Thompson Ridge, a hamlet in the Town of Crawford in Orange County, New York, United States. It is a late 18th-century building later modified in the Greek Revival style.',\n",
    "                     'Question': 'What is the business category of Crawford House, located in the same city as WODS and the same state as Wellesley College in Mona Lisa Smile?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'Paragraph 1 says that WODS is location in Boston. However, no paragraph talks about which state Wellesley College.'}\n",
    "    \n",
    "    pos_answerability_example_1 = {'Paragraphs': 'Paragraph 1: Kaya toast is a well-known snack in Singapore. Kaya toast is prepared with kaya (coconut jam), a topping of sugar, coconut milk and eggs, pandan, and sometimes margarine or butter. Kaya is generally served on toast, and also sometimes on crackers. It is considered a breakfast staple, and remains popular in Singapore. The dish is sometimes dipped into soft-boiled egg with a little dark soy sauce and white pepper.\\n Paragraph 2: A justice of the peace in Singapore derives his powers from statute law. He is appointed by the President of the Republic of Singapore, under the provisions of section 11 (l) of the Subordinate Courts Act (Cap. 321). The President may revoke the appointment of any justice of the peace. A newly appointed justice of the peace is required by section 17 of the Subordinate Courts Act, to take the oath of office and allegiance as set out in the schedule to the Subordinate Courts Act, before exercising the functions of his office.',\n",
    "                     'Question': 'How do you become a justice of peace in the country where Kaya toast is popular?',\n",
    "                     'Answer': 'answerable.',\n",
    "                     'CoT':'Paragraph 1 mentions that Kaya toast is a well-known snack in Singapore. Paragraph 2 says that a justice of peace in Singapore derives his powers from statute law. It also says that he is appointed by the President of the Republic of Singapore.'}\n",
    "\n",
    "    \n",
    "    neg_answerability_example_1 = {'Paragraphs':'Paragraph 1: WODS (103.3 MHz) - known on-air as 103.3 AMP Radio - is a commercial FM radio station in Boston, Massachusetts. WODS airs a Top 40 (CHR) radio format, and is owned by Entercom. Its studios and offices are located on Leo M. Birmingham Parkwary in Brighton.\\n Paragraph 2: The Embassy of the United States to the Republic of Indonesia is located in Jakarta just south of the Monas at Jalan Medan Merdeka Selatan.\\n Paragraph 3: Westminster College is a private liberal arts college located in the Sugar House neighborhood of Salt Lake City, Utah, United States. The college comprises four schools: the School of Arts and Sciences, the Bill and Vieve Gore School of Business, the School of Education, and the School of Nursing and Health Sciences. It is the only accredited liberal arts college in the state of Utah.\\n Paragraph 4: The Shorter House is located at the end of Andrews Road in Thompson Ridge, a hamlet in the Town of Crawford in Orange County, New York, United States. It is a late 18th-century building later modified in the Greek Revival style.',\n",
    "                     'Question': 'What is the business category of Crawford House, located in the same city as WODS and the same state as Wellesley College in Mona Lisa Smile?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'Paragraph 1 says that WODS is location in Boston. However, no paragraph talks about which state Wellesley College.'}\n",
    "\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icl_examples_musique_v3():\n",
    "    pos_example_1 = {'Paragraphs': 'Paragraph 1: Meet Me in St. Louis is a musical film made by Metro - Goldwyn - Mayer and released in 1944. Divided into a series of seasonal vignettes, starting with Summer 1903, it relates the story of a year in the life of the Smith family in St. Louis, leading up to the opening of the Louisiana Purchase Exposition (more commonly referred to as the World\\'s Fair) in the spring of 1904. The picture stars Judy Garland, Margaret O\\'Brien, Mary Astor, Lucille Bremer, Tom Drake, Leon Ames, Marjorie Main, June Lockhart, and Joan Carroll.\\n Paragraph 2: Gracie is a 2007 American sports drama film directed by Davis Guggenheim. It stars Carly Schroeder as Gracie Bowen, Dermot Mulroney as Bryan Bowen, Elisabeth Shue as Lindsay Bowen, Jesse Lee Soffer as Johnny Bowen, and Andrew Shue as Coach Owen Clark.\\n Paragraph 3: He was born Philip Davis Guggenheim in St. Louis, Missouri, United States, the son of Marion Davis and film director and producer Charles Guggenheim. His father was Jewish, whereas his mother was Episcopalian. He graduated from the Potomac School (McLean, Virginia) (1979), from Sidwell Friends School (1982), and from Brown University (1986).',\n",
    "                     'Question': 'When does Meet Me in the birthplace of Gracie\\'s director take place?',\n",
    "                     'Answer': 'starting with Summer 1903.',\n",
    "                     'CoT':'Paragraph 2 mentions that Garcie was directed by Davis Guggenheim. Paragraph 3 says that Davis Guggenheim was born in St. Louis. Paragraph 1 says that Meet Me in St. Louis starts with Summer 1903.'}\n",
    "    \n",
    "    pos_example_2 = {'Paragraphs':'Paragraph 1: The city has a Mayor and is one of the 16 cities and towns in England and Wales to have a ceremonial sheriff who acts as a deputy for the Mayor. The current and 793rd Mayor of Southampton is Linda Norris. Catherine McEwing is the current and 578th sherriff. The town crier from 2004 until his death in 2014 was John Melody, who acted as master of ceremonies in the city and who possessed a cry of 104 decibels.\\n Paragraph 2: John May (born 26 September 1849 in Southampton, Hampshire; date of death unknown) was an English cricketer. May was a right-handed batsman who was a right-arm fast bowler.',\n",
    "                     'Question': 'Who is the current mayor of the birthplace of John May?',\n",
    "                     'Answer':'Linda Norris',\n",
    "                     'CoT':'Paragraph 2 says that John May was born in Southampton. Paragraph 1 mentions that the current Mayor of Southampton is Linda Norris.'}\n",
    "\n",
    "    neg_example_1 = {'Paragraphs':'Paragraph 1: Imran Khan has held the office of Prime Minister since 18 August 2018, following the outcome of nationwide general elections held on 25 July 2018.\\n Paragraph 2: Hampi, also referred to as the Group of Monuments at Hampi, is a UNESCO World Heritage Site located in east - central Karnataka, India. It became the centre of the Hindu Vijayanagara Empire capital in the 14th century. Chronicles left by Persian and European travellers, particularly the Portuguese, state Hampi was a prosperous, wealthy and grand city near the Tungabhadra River, with numerous temples, farms and trading markets. By 1500 CE, Hampi - Vijayanagara was the world\\'s second - largest medieval - era city after Beijing, and probably India\\'s richest at that time, attracting traders from Persia and Portugal. The Vijayanagara Empire was defeated by a coalition of Muslim sultanates; its capital was conquered, pillaged and destroyed by sultanate armies in 1565, after which Hampi remained in ruins.\\n Paragraph 3: As of June 2018, the Government of Karnataka consists of 27 ministers including Chief Minister and a Deputy Chief Minister.\\n Paragraph 4: Thekkady (Idukki district) is the location of the Periyar National Park, which is an important tourist attraction in the Kerala state of India.',\n",
    "                     'Question': 'As of 2018, who is the minister of the state where hampi tourist place is located?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'Paragraph 2 mentions that Hampi is located in east - central Karnataka, India. Paragraph 3 says that as of June 2018, the Government of Karnataka consists of 27 ministers including Chief Minister and a Deputy Chief Minister, but it doesn\\'t say who they are.'}\n",
    "    \n",
    "    pos_answerability_example_1 = {'Paragraphs': 'Paragraph 1: Meet Me in St. Louis is a musical film made by Metro - Goldwyn - Mayer and released in 1944. Divided into a series of seasonal vignettes, starting with Summer 1903, it relates the story of a year in the life of the Smith family in St. Louis, leading up to the opening of the Louisiana Purchase Exposition (more commonly referred to as the World\\'s Fair) in the spring of 1904. The picture stars Judy Garland, Margaret O\\'Brien, Mary Astor, Lucille Bremer, Tom Drake, Leon Ames, Marjorie Main, June Lockhart, and Joan Carroll.\\n Paragraph 2: Gracie is a 2007 American sports drama film directed by Davis Guggenheim. It stars Carly Schroeder as Gracie Bowen, Dermot Mulroney as Bryan Bowen, Elisabeth Shue as Lindsay Bowen, Jesse Lee Soffer as Johnny Bowen, and Andrew Shue as Coach Owen Clark.\\n Paragraph 3: He was born Philip Davis Guggenheim in St. Louis, Missouri, United States, the son of Marion Davis and film director and producer Charles Guggenheim. His father was Jewish, whereas his mother was Episcopalian. He graduated from the Potomac School (McLean, Virginia) (1979), from Sidwell Friends School (1982), and from Brown University (1986).',\n",
    "                     'Question': 'When does Meet Me in the birthplace of Gracie\\'s director take place?',\n",
    "                     'Answer': 'answerable.',\n",
    "                     'CoT':'Paragraph 2 mentions that Garcie was directed by Davis Guggenheim. Paragraph 3 says that Davis Guggenheim was born in St. Louis. Paragraph 1 says that Meet Me in St. Louis starts with Summer 1903.'}\n",
    "\n",
    "    \n",
    "    neg_answerability_example_1 = {'Paragraphs':'Paragraph 1: Imran Khan has held the office of Prime Minister since 18 August 2018, following the outcome of nationwide general elections held on 25 July 2018.\\n Paragraph 2: Hampi, also referred to as the Group of Monuments at Hampi, is a UNESCO World Heritage Site located in east - central Karnataka, India. It became the centre of the Hindu Vijayanagara Empire capital in the 14th century. Chronicles left by Persian and European travellers, particularly the Portuguese, state Hampi was a prosperous, wealthy and grand city near the Tungabhadra River, with numerous temples, farms and trading markets. By 1500 CE, Hampi - Vijayanagara was the world\\'s second - largest medieval - era city after Beijing, and probably India\\'s richest at that time, attracting traders from Persia and Portugal. The Vijayanagara Empire was defeated by a coalition of Muslim sultanates; its capital was conquered, pillaged and destroyed by sultanate armies in 1565, after which Hampi remained in ruins.\\n Paragraph 3: As of June 2018, the Government of Karnataka consists of 27 ministers including Chief Minister and a Deputy Chief Minister.\\n Paragraph 4: Thekkady (Idukki district) is the location of the Periyar National Park, which is an important tourist attraction in the Kerala state of India.',\n",
    "                     'Question': 'As of 2018, who is the minister of the state where hampi tourist place is located?',\n",
    "                     'Answer':'unanswerable.',\n",
    "                     'CoT':'Paragraph 2 mentions that Hampi is located in east - central Karnataka, India. Paragraph 3 says that as of June 2018, the Government of Karnataka consists of 27 ministers including Chief Minister and a Deputy Chief Minister, but it doesn\\'t say who they are.'}\n",
    "\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_to_string_musique(example, instruction, isIcl, isCoT):\n",
    "    example_str = f\"Instructions: {instruction}\\n Context: {example['Paragraphs']}\\n Question: {example['Question']}\\n Answer:\"\n",
    "\n",
    "    if isIcl:\n",
    "        if isCoT:\n",
    "            example_str = f\"{example_str} {example['CoT']} So, the answer is: {example['Answer']}\"\n",
    "        else:\n",
    "            example_str = f\"{example_str} {example['Answer']}\"\n",
    "    return example_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_with_instructions_prompts_musique(curr_data):\n",
    "    if ICL_examples_variant == 1:\n",
    "        pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1 = get_icl_examples_musique_v1()\n",
    "    elif ICL_examples_variant == 2:\n",
    "        pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1 = get_icl_examples_musique_v2()\n",
    "    elif ICL_examples_variant == 3:\n",
    "        pos_example_1, pos_example_2, neg_example_1, pos_answerability_example_1, neg_answerability_example_1 = get_icl_examples_musique_v3()\n",
    "    else:\n",
    "        raise Exception(f\"ICL_examples_variant can only be any of 1,2,3, but is currently {ICL_examples_variant}\")\n",
    "    \n",
    "    instructions = {'Adversarial': 'Given the following context and question, answer the question.',\n",
    "                    'Pseudo-Adversarial': 'Given the following context and question, answer the question. If it cannot be answered based on the context, reply \"unanswerable\".',\n",
    "                    'Answerability': 'Given the following context and question, decide if the question is answerable based on the context. Reply only \"answerable\" or \"unanswerable\".'}\n",
    "    prompt_list_few_shot_with_instructions = list()\n",
    "    for elem in curr_data:\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        prompt_elem[\"Answer\"] = elem[\"answer\"]\n",
    "        if \"actual_answer\" in elem.keys():\n",
    "            prompt_elem[\"actual_answer\"] = elem[\"actual_answer\"]\n",
    "        \n",
    "        # Adversarial\n",
    "        prompt_elem[\"Adversarial\"] = f\"\"\"\n",
    "                                    {example_to_string_musique(pos_example_1, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_musique(pos_example_2, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_musique(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        prompt_elem[\"Adversarial-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string_musique(pos_example_1, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_musique(pos_example_2, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_musique(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        # Pseudo-Adversarial\n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"\n",
    "                                    {example_to_string_musique(pos_example_1, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_musique(neg_example_1, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_musique(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        \n",
    "        prompt_elem[\"Pseudo-Adversarial-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string_musique(pos_example_1, instructions['Pseudo-Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_musique(neg_example_1, instructions['Pseudo-Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_musique(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        # Ablation1\n",
    "        prompt_elem[\"Ablation1\"] = f\"\"\"\n",
    "                                    {example_to_string_musique(pos_example_1, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_musique(pos_example_2, instructions['Pseudo-Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_musique(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        \n",
    "        prompt_elem[\"Ablation1-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string_musique(pos_example_1, instructions['Pseudo-Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_musique(pos_example_2, instructions['Pseudo-Adversarial'], True, True)}\n",
    "                                    \n",
    "                                    {example_to_string_musique(elem, instructions['Pseudo-Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        # Ablation2\n",
    "        prompt_elem[\"Ablation2\"] = f\"\"\"\n",
    "                                    {example_to_string_musique(pos_example_1, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_musique(neg_example_1, instructions['Adversarial'], True, False)}\n",
    "\n",
    "                                    {example_to_string_musique(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        \n",
    "        prompt_elem[\"Ablation2-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string_musique(pos_example_1, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_musique(neg_example_1, instructions['Adversarial'], True, True)}\n",
    "\n",
    "                                    {example_to_string_musique(elem, instructions['Adversarial'], False, False)}\"\"\"\n",
    "\n",
    "        # Answerability\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"\n",
    "                                    {example_to_string_musique(pos_answerability_example_1, instructions['Answerability'], True, False)}\n",
    "\n",
    "                                    {example_to_string_musique(neg_answerability_example_1, instructions['Answerability'], True, False)}\n",
    "\n",
    "                                    {example_to_string_musique(elem, instructions['Answerability'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        prompt_elem[\"Answerability-CoT\"] = f\"\"\"\n",
    "                                    {example_to_string_musique(pos_answerability_example_1, instructions['Answerability'], True, True)}\n",
    "\n",
    "                                    {example_to_string_musique(neg_answerability_example_1, instructions['Answerability'], True, True)}\n",
    "\n",
    "                                    {example_to_string_musique(elem, instructions['Answerability'], False, False)}\"\"\"\n",
    "\n",
    "\n",
    "        for key,value in prompt_elem.items():\n",
    "            if key in [\"example_id\", \"annotation_id\"]:\n",
    "                continue\n",
    "            prompt_elem[key] = f\" {re.sub(' +', ' ', value).strip()}\"\n",
    "        prompt_list_few_shot_with_instructions.append(prompt_elem)\n",
    "    return prompt_list_few_shot_with_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [\"adversarial\", \"control_group\"]\n",
    "for data_type in data_types:\n",
    "    with open(os.path.join(musique_indir, f\"{data_type}_musique.jsonl\"), 'r') as f1:\n",
    "        curr_data = json.loads(f1.read())\n",
    "    curr_output = get_few_shot_with_instructions_prompts_musique(curr_data)\n",
    "    curr_outdir = os.path.join(musique_few_shot_with_instructions_outdir, f\"musique_{data_type}_icl_examples_v{ICL_examples_variant}_all.json\")\n",
    "    with open(curr_outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(curr_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set (for training the classifier) - zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musique_indir = \"data/musique\"\n",
    "musique_outdir = \"generated_prompts/train_set/all/zero_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts_musique(curr_data):\n",
    "    prompt_list_zero_shot = list()\n",
    "    for elem in curr_data:\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        prompt_elem[\"Adversarial\"] = f\"\"\"Given the following context and question, answer the question:\n",
    "            Context: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "        \n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Given the following context and question, answer the question.\n",
    "        If it cannot be answered based on the context, reply \"unanswerable\":\n",
    "            Context: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "        \n",
    "        prompt_elem[\"CoT-Adversarial\"] = f\"\"\"Given the following context and question, answer the question.\n",
    "        First make sure if it can be answered by the context.\n",
    "        If it cannot be answered based on the context, reply \"unanswerable\":\n",
    "            Context: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"Given the following context and question, decide if the question is answerable based on the context. Reply only \"answerable\" or \"unanswerable\":\n",
    "            Context: {elem[\"Paragraphs\"]}\n",
    "            Question:  {elem[\"Question\"]}\"\"\" \n",
    "\n",
    "        prompt_elem[\"answer\"] = elem[\"answer\"]\n",
    "\n",
    "        if \"actual_answer\" in elem.keys():\n",
    "            prompt_elem[\"actual_answer\"] = elem[\"actual_answer\"]\n",
    "\n",
    "        prompt_list_zero_shot.append(prompt_elem)\n",
    "    return prompt_list_zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [\"adversarial\", \"control_group\"]\n",
    "for data_type in data_types:\n",
    "    with open(os.path.join(musique_indir, f\"{data_type}_musique_train.jsonl\"), 'r') as f1:\n",
    "        curr_data = json.loads(f1.read())\n",
    "    curr_output = get_prompts_musique(curr_data)\n",
    "    curr_outdir = os.path.join(musique_outdir, f\"musique_trainset_{data_type}_all.json\")\n",
    "    with open(curr_outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(curr_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = r\"data/task025_cosmosqa_incorrect_answer_generation.json\"\n",
    "outdir = r\"generated_prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(indir) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[\"Instances\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter only samples with more than two answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = [elem for elem in data[\"Instances\"] if len(elem[\"output\"])>2]\n",
    "len(filtered_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompts (Zero-Shot) - Adversarial - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "    context_end = elem[\"input\"].find(\"\\nCorrect Answer:\")\n",
    "    if context_end == -1:\n",
    "        raise Exception(\"Didn't find the beginning of the \\\"Correct Answer\\\" in the input\")\n",
    "\n",
    "    context = elem[\"input\"][:context_end]\n",
    "\n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-2-options\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\"\"\" \n",
    "\n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    If none of the options fit, answer \"N/A\".\n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"N/A\" if none of the options fit:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\"  \n",
    "    \n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question? \n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"(D)\":\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) None of the above.\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"{context}\n",
    "    Given this context, does any of the following options answer the question? \n",
    "    Answer with a single word - \"yes\" or \"no\":\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\" \n",
    "\n",
    "    prompt_list_zero_shot.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompts (Zero-Shot) - Control Group - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot_control_group = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "    context_end = elem[\"input\"].find(\"\\nCorrect Answer:\")\n",
    "    if context_end == -1:\n",
    "        raise Exception(\"Didn't find the beginning of the \\\"Correct Answer\\\" in the input\")\n",
    "\n",
    "    context = elem[\"input\"][:context_end]\n",
    "    correct_answer = elem[\"input\"][elem[\"input\"].index(\"\\nCorrect Answer:\"):].replace(\"\\nCorrect Answer:\", \"\").strip()\n",
    "\n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-2-options\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {correct_answer}\"\"\" \n",
    "\n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    If none of the options fit, answer \"N/A\".\n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"N/A\" if none of the options fit:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer}\"\"\"  \n",
    "    \n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question? \n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"(D)\":\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer}\n",
    "    (D) None of the above.\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"{context}\n",
    "    Given this context, does any of the following options answer the question? \n",
    "    Answer with a single word - \"yes\" or \"no\":\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer}\"\"\" \n",
    "\n",
    "    prompt_list_zero_shot_control_group.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompts (Zero-Shot) - Adversarial - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot_v2 = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "    context_end = elem[\"input\"].find(\"\\nCorrect Answer:\")\n",
    "    if context_end == -1:\n",
    "        raise Exception(\"Didn't find the beginning of the \\\"Correct Answer\\\" in the input\")\n",
    "\n",
    "    context = elem[\"input\"][:context_end]\n",
    "\n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-2-options\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\"\"\" \n",
    "\n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    If none of the options fit, reply \"unanswerable\".\n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"unanswerable\" if none of the options fit:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\"  \n",
    "    \n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question? \n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"(D)\":\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) None of the above.\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"Determine whether the following context, question and three options are answerable. Reply only \"answerable\" or \"unanswerable\":\n",
    "    {context}\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "\"\"\" \n",
    "\n",
    "    prompt_list_zero_shot_v2.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompts (Zero-Shot) - Control Group - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot_control_group_v2 = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "    context_end = elem[\"input\"].find(\"\\nCorrect Answer:\")\n",
    "    if context_end == -1:\n",
    "        raise Exception(\"Didn't find the beginning of the \\\"Correct Answer\\\" in the input\")\n",
    "\n",
    "    context = elem[\"input\"][:context_end]\n",
    "    correct_answer = elem[\"input\"][elem[\"input\"].index(\"\\nCorrect Answer:\"):].replace(\"\\nCorrect Answer:\", \"\").strip()\n",
    "\n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-2-options\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {correct_answer}\"\"\" \n",
    "\n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question?\n",
    "    If none of the options fit, reply \"unanswerable\".\n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"unanswerable\" if none of the options fit:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer}\"\"\"  \n",
    "    \n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"{context}\n",
    "    Given this context, which of the following options answers the question? \n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"(D)\":\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer}\n",
    "    (D) None of the above.\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"Determine whether the following context, question and three options are answerable. Reply only \"answerable\" or \"unanswerable\":\n",
    "    {context}\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer}\"\"\" \n",
    "\n",
    "    prompt_list_zero_shot_control_group_v2.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompts (Few-Shot) - Adversarial + Control_Group - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(example, is_CoT):\n",
    "    generated_example = f\"\"\"{example[\"input\"]} \n",
    "Options:\n",
    " (A) {example[\"option A\"]}\n",
    " (B) {example[\"option B\"]}\n",
    " (C) {example[\"option C\"]}\"\"\"\n",
    "\n",
    "    if 'option D' in example.keys():\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    " (D) {example[\"option D\"]}\"\"\"\n",
    "        \n",
    "    if is_CoT:\n",
    "        if not example[\"Answer\"] in [\"(A)\", \"(B)\", \"(C)\", \"(D)\"]:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, {example[\"Answer\"]}.\"\"\"\n",
    "        else:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, the answer is {example[\"Answer\"]}.\"\"\"\n",
    "    \n",
    "    else:\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"Answer\"]}\"\"\"\n",
    "    return generated_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_dicts_cosmos():\n",
    "    pos_example_1 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I found the money was charged but I have not got shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'You are told, in person over the phone, that your shoes were on their way. They have your money, which means your paid for the shoes. you haven\\'t received the shoes yet.'}\n",
    "    \n",
    "    pos_example_2 = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "\n",
    "    neg_example_1 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': 'none of the options answers the question',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "    neg_example_2 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(D)',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "\n",
    "    neg_example_3 = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "\n",
    "\n",
    "    pos_example_1_answerability = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'Answer': 'there is a correct option (option B)',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "    neg_example_1_answerability = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': 'there is no correct option',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_list_cosmos_few_shot(filtered_data, is_control_group):\n",
    "    prompt_list_few_shot = list()\n",
    "    for elem in filtered_data:\n",
    "\n",
    "        pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_example_dicts_cosmos()\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        context_end = elem[\"input\"].find(\"\\nCorrect Answer:\")\n",
    "        if context_end == -1:\n",
    "            raise Exception(\"Didn't find the beginning of the \\\"Correct Answer\\\" in the input\")\n",
    "\n",
    "        context = elem[\"input\"][:context_end]\n",
    "        correct_answer = elem[\"input\"][elem[\"input\"].index(\"\\nCorrect Answer:\"):].replace(\"\\nCorrect Answer:\", \"\").strip()\n",
    "\n",
    "\n",
    "    ############## Adversarial-3-options ############\n",
    "        prompt_elem[\"Adversarial-3-options\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ########### Adversarial-3-options-CoT ##########\n",
    "        prompt_elem[\"Adversarial-3-options-CoT\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\\n \n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ############## Adversarial-2-options ############\n",
    "        prompt_elem[\"Adversarial-2-options\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {correct_answer if is_control_group else elem[\"output\"][1]}\n",
    "    Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ########### Adversarial-2-options-CoT ##########\n",
    "        prompt_elem[\"Adversarial-2-options-CoT\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\\n \n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {correct_answer if is_control_group else elem[\"output\"][1]}\n",
    "    Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "\n",
    "    ############### Pseudo-Adversarial #############\n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\"  \n",
    "    ################################################\n",
    "\n",
    "    ############# Pseudo-Adversarial-CoT ###########\n",
    "        prompt_elem[\"Pseudo-Adversarial-CoT\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\"  \n",
    "    ################################################\n",
    "\n",
    "    ############## Adversarial-NA-fourth-option ############\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(neg_example_2, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_3, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "    Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option-CoT ###########\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(neg_example_2, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_3, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "    Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "    ###################### Answerability ####################\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1_answerability, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1_answerability, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "    ###################### Answerability-CoT ####################\n",
    "        prompt_elem[\"Answerability-CoT\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1_answerability, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1_answerability, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "        prompt_list_few_shot.append(prompt_elem)\n",
    "    return prompt_list_few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot = get_prompt_list_cosmos_few_shot(filtered_data, False)\n",
    "prompt_list_few_shot_control_group = get_prompt_list_cosmos_few_shot(filtered_data, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompts (Few-Shot) - Adversarial + Control_Group - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(example, is_CoT):\n",
    "    generated_example = f\"\"\"{example[\"input\"]} \n",
    "Options:\n",
    " (A) {example[\"option A\"]}\n",
    " (B) {example[\"option B\"]}\n",
    " (C) {example[\"option C\"]}\"\"\"\n",
    "\n",
    "    if 'option D' in example.keys():\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    " (D) {example[\"option D\"]}\"\"\"\n",
    "        \n",
    "    if is_CoT:\n",
    "        if not example[\"Answer\"] in [\"(A)\", \"(B)\", \"(C)\", \"(D)\"]:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, {example[\"Answer\"]}.\"\"\"\n",
    "        else:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, the answer is {example[\"Answer\"]}.\"\"\"\n",
    "    \n",
    "    else:\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"Answer\"]}\"\"\"\n",
    "    return generated_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_dicts_cosmos_v2():\n",
    "    pos_example_1 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I found the money was charged but I have not got shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'You are told, in person over the phone, that your shoes were on their way. They have your money, which means your paid for the shoes. you haven\\'t received the shoes yet.'}\n",
    "    \n",
    "    pos_example_2 = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "\n",
    "    neg_example_1 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': 'unanswerable',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "    neg_example_2 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(D)',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "\n",
    "    neg_example_3 = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "\n",
    "\n",
    "    pos_example_1_answerability = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'Answer': 'answerable',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "    neg_example_1_answerability = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': 'unanswerable',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_list_cosmos_few_shot_v2(filtered_data, is_control_group):\n",
    "    prompt_list_few_shot = list()\n",
    "    for elem in filtered_data:\n",
    "\n",
    "        pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_example_dicts_cosmos_v2()\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        context_end = elem[\"input\"].find(\"\\nCorrect Answer:\")\n",
    "        if context_end == -1:\n",
    "            raise Exception(\"Didn't find the beginning of the \\\"Correct Answer\\\" in the input\")\n",
    "\n",
    "        context = elem[\"input\"][:context_end]\n",
    "        correct_answer = elem[\"input\"][elem[\"input\"].index(\"\\nCorrect Answer:\"):].replace(\"\\nCorrect Answer:\", \"\").strip()\n",
    "\n",
    "\n",
    "    ############## Adversarial-3-options ############\n",
    "        prompt_elem[\"Adversarial-3-options\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ########### Adversarial-3-options-CoT ##########\n",
    "        prompt_elem[\"Adversarial-3-options-CoT\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\\n \n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ############## Adversarial-2-options ############\n",
    "        prompt_elem[\"Adversarial-2-options\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {correct_answer if is_control_group else elem[\"output\"][1]}\n",
    "    Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ########### Adversarial-2-options-CoT ##########\n",
    "        prompt_elem[\"Adversarial-2-options-CoT\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\\n \n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {correct_answer if is_control_group else elem[\"output\"][1]}\n",
    "    Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "\n",
    "    ############### Pseudo-Adversarial #############\n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\"  \n",
    "    ################################################\n",
    "\n",
    "    ############# Pseudo-Adversarial-CoT ###########\n",
    "        prompt_elem[\"Pseudo-Adversarial-CoT\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\"  \n",
    "    ################################################\n",
    "\n",
    "    ############## Adversarial-NA-fourth-option ############\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(neg_example_2, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_3, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "    Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option-CoT ###########\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(neg_example_2, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_3, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "    Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "    ###################### Answerability ####################\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1_answerability, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1_answerability, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "    ###################### Answerability-CoT ####################\n",
    "        prompt_elem[\"Answerability-CoT\"] = f\"\"\"\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1_answerability, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1_answerability, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "    Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "        prompt_list_few_shot.append(prompt_elem)\n",
    "    return prompt_list_few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_v2 = get_prompt_list_cosmos_few_shot_v2(filtered_data, False)\n",
    "prompt_list_few_shot_control_group_v2 = get_prompt_list_cosmos_few_shot_v2(filtered_data, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompts (Few-Shot with Instructions) - Adversarial + Control_Group - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(example, is_CoT):\n",
    "    generated_example = f\"\"\"{example[\"input\"]} \n",
    "Options:\n",
    " (A) {example[\"option A\"]}\n",
    " (B) {example[\"option B\"]}\n",
    " (C) {example[\"option C\"]}\"\"\"\n",
    "\n",
    "    if 'option D' in example.keys():\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    " (D) {example[\"option D\"]}\"\"\"\n",
    "        \n",
    "    if is_CoT:\n",
    "        if not example[\"Answer\"] in [\"(A)\", \"(B)\", \"(C)\", \"(D)\"]:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, {example[\"Answer\"]}.\"\"\"\n",
    "        else:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, the answer is {example[\"Answer\"]}.\"\"\"\n",
    "    \n",
    "    else:\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"Answer\"]}\"\"\"\n",
    "    return generated_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_with_instructions_example_dicts_cosmos():\n",
    "    pos_example_1 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I found the money was charged but I have not got shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'You are told, in person over the phone, that your shoes were on their way. They have your money, which means your paid for the shoes. you haven\\'t received the shoes yet.'}\n",
    "    \n",
    "    pos_example_2 = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "\n",
    "    neg_example_1 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': 'N/A',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "    neg_example_2 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(D)',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "\n",
    "    neg_example_3 = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "\n",
    "\n",
    "    pos_example_1_answerability = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'Answer': 'yes',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "    neg_example_1_answerability = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': 'no',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_list_cosmos_few_shot_with_instructions(filtered_data, is_control_group):\n",
    "    prompt_list_few_shot = list()\n",
    "    for elem in filtered_data:\n",
    "\n",
    "        pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_few_shot_with_instructions_example_dicts_cosmos()\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        context_end = elem[\"input\"].find(\"\\nCorrect Answer:\")\n",
    "        if context_end == -1:\n",
    "            raise Exception(\"Didn't find the beginning of the \\\"Correct Answer\\\" in the input\")\n",
    "\n",
    "        context = elem[\"input\"][:context_end]\n",
    "        correct_answer = elem[\"input\"][elem[\"input\"].index(\"\\nCorrect Answer:\"):].replace(\"\\nCorrect Answer:\", \"\").strip()\n",
    "\n",
    "\n",
    "    ############## Adversarial-3-options ############\n",
    "        prompt_elem[\"Adversarial-3-options\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ########### Adversarial-3-options-CoT ##########\n",
    "        prompt_elem[\"Adversarial-3-options-CoT\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\\n \n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ############## Adversarial-2-options ############\n",
    "        prompt_elem[\"Adversarial-2-options\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {correct_answer if is_control_group else elem[\"output\"][1]}\n",
    "Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ########### Adversarial-2-options-CoT ##########\n",
    "        prompt_elem[\"Adversarial-2-options-CoT\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\\n \n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {correct_answer if is_control_group else elem[\"output\"][1]}\n",
    "Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "\n",
    "    ############### Pseudo-Adversarial #############\n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "If none of the options fit, answer \\\"N/A\\\".\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\"  \n",
    "    ################################################\n",
    "\n",
    "    ############# Pseudo-Adversarial-CoT ###########\n",
    "        prompt_elem[\"Pseudo-Adversarial-CoT\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "If none of the options fit, answer \\\"N/A\\\".\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\"  \n",
    "    ################################################\n",
    "\n",
    "    ############## Adversarial-NA-fourth-option ############\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"Given the following context, question, and several options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(neg_example_2, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_3, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option-CoT ###########\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = f\"\"\"Given the following context, question, and several options, which of the options answers the question?\n",
    "    \n",
    "    Example 1:\n",
    "    {get_example(neg_example_2, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_3, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    ############## Adversarial-NA-fourth-option-Ablation1 ############\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-Ablation1\"] = f\"\"\"Given the following context, question, and several options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option-CoT-Ablation1 ###########\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-Ablation1-CoT\"] = f\"\"\"Given the following context, question, and several options, which of the options answers the question?\n",
    "    \n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###################### Answerability ####################\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"Given the following context, question, and three options, does any of the options answer the question? \n",
    "Answer with a single word - \\\"yes\\\" or \\\"no\\\":\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1_answerability, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1_answerability, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "    ###################### Answerability-CoT ####################\n",
    "        prompt_elem[\"Answerability-CoT\"] = f\"\"\"Given the following context, question, and three options, does any of the options answer the question? \n",
    "Answer with a single word - \\\"yes\\\" or \\\"no\\\":\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1_answerability, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1_answerability, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "        prompt_list_few_shot.append(prompt_elem)\n",
    "    \n",
    "    prompt_list_few_shot = [{key:re.sub(' +', ' ', value) for key,value in sample.items()} for sample in prompt_list_few_shot] # replace consecutive spaces with a single space\n",
    "    return prompt_list_few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_with_instructions = get_prompt_list_cosmos_few_shot_with_instructions(filtered_data, False)\n",
    "prompt_list_few_shot_with_instructions_control_group = get_prompt_list_cosmos_few_shot_with_instructions(filtered_data, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompts (Few-Shot with Instructions) - Adversarial + Control_Group - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(example, is_CoT):\n",
    "    generated_example = f\"\"\"{example[\"input\"]} \n",
    "Options:\n",
    " (A) {example[\"option A\"]}\n",
    " (B) {example[\"option B\"]}\n",
    " (C) {example[\"option C\"]}\"\"\"\n",
    "\n",
    "    if 'option D' in example.keys():\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    " (D) {example[\"option D\"]}\"\"\"\n",
    "        \n",
    "    if is_CoT:\n",
    "        if not example[\"Answer\"] in [\"(A)\", \"(B)\", \"(C)\", \"(D)\"]:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, {example[\"Answer\"]}.\"\"\"\n",
    "        else:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, the answer is {example[\"Answer\"]}.\"\"\"\n",
    "    \n",
    "    else:\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"Answer\"]}\"\"\"\n",
    "    return generated_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_with_instructions_example_dicts_cosmos_v2():\n",
    "    pos_example_1 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I found the money was charged but I have not got shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'You are told, in person over the phone, that your shoes were on their way. They have your money, which means your paid for the shoes. you haven\\'t received the shoes yet.'}\n",
    "    \n",
    "    pos_example_2 = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "\n",
    "    neg_example_1 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': 'unanswerable',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "    neg_example_2 = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(D)',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "\n",
    "    neg_example_3 = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "\n",
    "\n",
    "    pos_example_1_answerability = {'input': 'Context: you see , at my age relationship is kind of important and i thought i got the one after all these years . I noticed that once again i was wrong . i was good simply because i was good , i was caring , helping , supportive , bla bla blaaa . \\nQuestion: What may happen to me?',\n",
    "                     'option A': 'I got one important relationship.',\n",
    "                     'option B': 'I broke up with my friend.',\n",
    "                     'option C': 'I got a friend who is good, caring, helping and supportive.',\n",
    "                     'Answer': 'answerable',\n",
    "                     'CoT':'You say that relationship is important, and that you thought you finally got one. Then you say that you were wrong, which means you don\\'t have a relationship afterall.'}\n",
    "\n",
    "    neg_example_1_answerability = {'input': \"Context: I was told, in person over the phone, that my shoes were on their way. They have my money. I have no shoes. \\nQuestion: What may happen before I called them?\",\n",
    "                     'option A': 'I will pay them money after I receive the shoes.',\n",
    "                     'option B': 'I found the shoes were still on the way after several days.',\n",
    "                     'option C': 'I felt happy though I paid money and have not got the shoes.',\n",
    "                     'Answer': 'unanswerable',\n",
    "                     'CoT':'They have your money, so you already paid, so answer (A) is incorrect. You were told that the shoes are on their way during the call, not before it, so answer (B) is incorrect. You paid money and haven\\'t got your shoes yet, so it is implausible you were happy, so answer (C) is incorrect.'}\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_list_cosmos_few_shot_with_instructions_v2(filtered_data, is_control_group):\n",
    "    prompt_list_few_shot = list()\n",
    "    for elem in filtered_data:\n",
    "\n",
    "        pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_few_shot_with_instructions_example_dicts_cosmos_v2()\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        context_end = elem[\"input\"].find(\"\\nCorrect Answer:\")\n",
    "        if context_end == -1:\n",
    "            raise Exception(\"Didn't find the beginning of the \\\"Correct Answer\\\" in the input\")\n",
    "\n",
    "        context = elem[\"input\"][:context_end]\n",
    "        correct_answer = elem[\"input\"][elem[\"input\"].index(\"\\nCorrect Answer:\"):].replace(\"\\nCorrect Answer:\", \"\").strip()\n",
    "\n",
    "\n",
    "    ############## Adversarial-3-options ############\n",
    "        prompt_elem[\"Adversarial-3-options\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ########### Adversarial-3-options-CoT ##########\n",
    "        prompt_elem[\"Adversarial-3-options-CoT\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\\n \n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ############## Adversarial-2-options ############\n",
    "        prompt_elem[\"Adversarial-2-options\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {correct_answer if is_control_group else elem[\"output\"][1]}\n",
    "Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "    ########### Adversarial-2-options-CoT ##########\n",
    "        prompt_elem[\"Adversarial-2-options-CoT\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\\n \n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {correct_answer if is_control_group else elem[\"output\"][1]}\n",
    "Output:\"\"\" \n",
    "    ################################################\n",
    "\n",
    "\n",
    "    ############### Pseudo-Adversarial #############\n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "If none of the options fit, reply \"unanswerable\".\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\"  \n",
    "    ################################################\n",
    "\n",
    "    ############# Pseudo-Adversarial-CoT ###########\n",
    "        prompt_elem[\"Pseudo-Adversarial-CoT\"] = f\"\"\"Given the following context, question, and three options, which of the options answers the question?\n",
    "If none of the options fit, reply \"unanswerable\".\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\"  \n",
    "    ################################################\n",
    "\n",
    "    ############## Adversarial-NA-fourth-option ############\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"Given the following context, question, and several options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(neg_example_2, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_3, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option-CoT ###########\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = f\"\"\"Given the following context, question, and several options, which of the options answers the question?\n",
    "    \n",
    "    Example 1:\n",
    "    {get_example(neg_example_2, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_3, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    ############## Adversarial-NA-fourth-option-Ablation1 ############\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-Ablation1\"] = f\"\"\"Given the following context, question, and several options, which of the options answers the question?\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option-CoT-Ablation1 ###########\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-Ablation1-CoT\"] = f\"\"\"Given the following context, question, and several options, which of the options answers the question?\n",
    "    \n",
    "    Example 1:\n",
    "    {get_example(pos_example_1, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(pos_example_2, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "    (D) None of the above.\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###################### Answerability ####################\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"Determine whether the following context, question and three options are answerable. Reply only \"answerable\" or \"unanswerable\":\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1_answerability, False)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1_answerability, False)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "    ###################### Answerability-CoT ####################\n",
    "        prompt_elem[\"Answerability-CoT\"] = f\"\"\"Determine whether the following context, question and three options are answerable. Reply only \"answerable\" or \"unanswerable\":\n",
    "\n",
    "    Example 1:\n",
    "    {get_example(pos_example_1_answerability, True)}\n",
    "\n",
    "    Example 2: \n",
    "    {get_example(neg_example_1_answerability, True)}\n",
    "\n",
    "    Now your turn:\n",
    "    {context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {correct_answer if is_control_group else elem[\"output\"][2]}\n",
    "Output:\"\"\" \n",
    "    #########################################################\n",
    "\n",
    "        prompt_list_few_shot.append(prompt_elem)\n",
    "    \n",
    "    prompt_list_few_shot = [{key:re.sub(' +', ' ', value) for key,value in sample.items()} for sample in prompt_list_few_shot] # replace consecutive spaces with a single space\n",
    "    return prompt_list_few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_with_instructions_v2 = get_prompt_list_cosmos_few_shot_with_instructions_v2(filtered_data, False)\n",
    "prompt_list_few_shot_with_instructions_control_group_v2 = get_prompt_list_cosmos_few_shot_with_instructions_v2(filtered_data, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outdir):\n",
    "   os.makedirs(outdir)\n",
    "\n",
    "if not os.path.exists(os.path.join(outdir, \"chatGPT\", \"zero_shot\")):\n",
    "   os.makedirs(os.path.join(outdir, \"chatGPT\", \"zero_shot\"))\n",
    "\n",
    "if not os.path.exists(os.path.join(outdir, \"chatGPT\", \"few_shot\")):\n",
    "   os.makedirs(os.path.join(outdir, \"chatGPT\", \"few_shot\"))\n",
    "\n",
    "if not os.path.exists(os.path.join(outdir, \"chatGPT\", \"few_shot_with_instructions\")):\n",
    "   os.makedirs(os.path.join(outdir, \"chatGPT\", \"few_shot_with_instructions\"))\n",
    "\n",
    "\n",
    "# zero shot\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"zero_shot\", \"cosmosqa_incorrect_answers_adversarial.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_zero_shot, indent=2))\n",
    "\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"zero_shot\", \"cosmosqa_incorrect_answers_control_group.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_zero_shot_control_group, indent=2))\n",
    "\n",
    "# few shot\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"few_shot\", \"cosmosqa_incorrect_answers_adversarial.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_few_shot, indent=2))\n",
    "\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"few_shot\", \"cosmosqa_incorrect_answers_control_group.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_few_shot_control_group, indent=2))\n",
    "\n",
    "# few shot with instructions\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"few_shot_with_instructions\", \"cosmosqa_incorrect_answers_adversarial.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_few_shot_with_instructions, indent=2))\n",
    "\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"few_shot_with_instructions\", \"cosmosqa_incorrect_answers_control_group.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_few_shot_with_instructions_control_group, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tk-Instruction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Zero-Shot) - Adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "    context_end = elem[\"input\"].find(\"\\nCorrect Answer:\")\n",
    "    if context_end == -1:\n",
    "        raise Exception(\"Didn't find the beginning of the \\\"Correct Answer\\\" in the input\")\n",
    "\n",
    "    context = elem[\"input\"][:context_end]\n",
    "\n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"Definition:\n",
    "        Given the following context, question, and three options, which of the options answers the question?\n",
    "        {context}\n",
    "        Options:\n",
    "        (A) {elem[\"output\"][0]}\n",
    "        (B) {elem[\"output\"][1]}\n",
    "        (C) {elem[\"output\"][2]}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-2-options\"] = f\"\"\"Definition:\n",
    "        Given the following context, question, and three options, which of the options answers the question?\n",
    "        {context}\n",
    "        Options:\n",
    "        (A) {elem[\"output\"][0]}\n",
    "        (B) {elem[\"output\"][1]}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Definition:\n",
    "        Given the following context, question, and three options, which of the options answers the question?\n",
    "        If none of the options fit, answer \\\"N/A\\\".\n",
    "        {context}\n",
    "        Options:\n",
    "        (A) {elem[\"output\"][0]}\n",
    "        (B) {elem[\"output\"][1]}\n",
    "        (C) {elem[\"output\"][2]}\"\"\"  \n",
    "    \n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"Definition:\n",
    "        Given the following context, question, and three options, which of the options answers the question?\n",
    "        {context}\n",
    "        Options:\n",
    "        (A) {elem[\"output\"][0]}\n",
    "        (B) {elem[\"output\"][1]}\n",
    "        (C) {elem[\"output\"][2]}\n",
    "        (D) None of the above.\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"Definition:\n",
    "        Given the following context, question, and three options, does any of the options answer the question? \n",
    "        Answer with a single word - \\\"yes\\\" or \\\"no\\\":\n",
    "        {context}\n",
    "        Options:\n",
    "        (A) {elem[\"output\"][0]}\n",
    "        (B) {elem[\"output\"][1]}\n",
    "        (C) {elem[\"output\"][2]}\"\"\" \n",
    "\n",
    "    prompt_list_zero_shot.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Zero-Shot) - Control Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot_control_group = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "    context_end = elem[\"input\"].find(\"\\nCorrect Answer:\")\n",
    "    if context_end == -1:\n",
    "        raise Exception(\"Didn't find the beginning of the \\\"Correct Answer\\\" in the input\")\n",
    "\n",
    "    context = elem[\"input\"][:context_end]\n",
    "    correct_answer = elem[\"input\"][elem[\"input\"].index(\"\\nCorrect Answer:\"):].replace(\"\\nCorrect Answer:\", \"\").strip()\n",
    "\n",
    "    prompt_elem[\"Adversarial\"] = f\"\"\"Definition:\n",
    "        Given the following context, question, and three options, which of the options answers the question?\n",
    "        {context}\n",
    "        Options:\n",
    "        (A) {elem[\"output\"][0]}\n",
    "        (B) {elem[\"output\"][1]}\n",
    "        (C) {correct_answer}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-2-options\"] = f\"\"\"Definition:\n",
    "        Given the following context, question, and three options, which of the options answers the question?\n",
    "        {context}\n",
    "        Options:\n",
    "        (A) {elem[\"output\"][0]}\n",
    "        (B) {correct_answer}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Definition:\n",
    "        Given the following context, question, and three options, which of the options answers the question?\n",
    "        If none of the options fit, answer \\\"N/A\\\".\n",
    "        {context}\n",
    "        Options:\n",
    "        (A) {elem[\"output\"][0]}\n",
    "        (B) {elem[\"output\"][1]}\n",
    "        (C) {correct_answer}\"\"\"  \n",
    "    \n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"Definition:\n",
    "        Given the following context, question, and three options, which of the options answers the question?\n",
    "        {context}\n",
    "        Options:\n",
    "        (A) {elem[\"output\"][0]}\n",
    "        (B) {elem[\"output\"][1]}\n",
    "        (C) {correct_answer}\n",
    "        (D) None of the above.\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"Definition:\n",
    "        Given the following context, question, and three options, does any of the options answer the question? \n",
    "        Answer with a single word - \\\"yes\\\" or \\\"no\\\":\n",
    "        {context}\n",
    "        Options:\n",
    "        (A) {elem[\"output\"][0]}\n",
    "        (B) {elem[\"output\"][1]}\n",
    "        (C) {correct_answer}\"\"\" \n",
    "\n",
    "    prompt_list_zero_shot_control_group.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outdir):\n",
    "   os.makedirs(outdir)\n",
    "\n",
    "with open(os.path.join(outdir, \"tk-instruct\", \"zero_shot\", \"cosmosqa_incorrect_answers_adversarial.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_zero_shot, indent=2))\n",
    "\n",
    "with open(os.path.join(outdir, \"tk-instruct\", \"zero_shot\", \"cosmosqa_incorrect_answers_control_group.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_zero_shot_control_group, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = r\"data/task055_multirc_write_incorrect_answer.json\"\n",
    "indir_control_group = r\"data/mutlirc-v2/splitv2/train_456-fixedIds.json\"\n",
    "indir_control_group_dev = r\"data/mutlirc-v2/splitv2/dev_83-fixedIds.json\"\n",
    "outdir = r\"generated_prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(indir) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(indir_control_group) as f:\n",
    "    data_control_group = json.load(f)\n",
    "    data_control_group = data_control_group[\"data\"]\n",
    "\n",
    "with open(indir_control_group_dev) as f:\n",
    "    data_control_group_dev = json.load(f)\n",
    "    data_control_group_dev = data_control_group_dev[\"data\"]\n",
    "    data_control_group.extend(data_control_group_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_control_group)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter only samples with at least three answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial\n",
    "filtered_data = [elem for elem in data[\"Instances\"] if len(elem[\"output\"])>2]\n",
    "len(filtered_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Zero-Shot) - Adversarial - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "     \n",
    "    context_end = elem[\"input\"].find(\"Correct Answer:\")\n",
    "    context = elem[\"input\"][:context_end] if context_end != -1 else elem[\"input\"] # if there is the right answer in the input - remove it\n",
    "    context = re.sub(r\"Sent \\d+:\", \"\", context) # make all sentences into a single paragraph\n",
    "    context = re.sub(r\"sent \\d+:\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "    context = context.replace(\"Paragraph-\", \"Paragraph:\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "    context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "    context = context.replace(\"Question:\", \"\\nQuestion:\") # move the Question to a new line\n",
    "\n",
    "    context = context.replace(\"?.\", \"?\") # the paragraph and question appear together, followed by a \".\" --> so remove this redundant dot.\n",
    "\n",
    "\n",
    "    # prompt_elem[\"Adversarial-7-options\"] = f\"\"\"{context}\n",
    "    # Based on the paragraph, which of the following options answers the question?\n",
    "    # (A) {elem[\"output\"][0]}\n",
    "    # (B) {elem[\"output\"][1]}\n",
    "    # (C) {elem[\"output\"][2]}\n",
    "    # (D) {elem[\"output\"][3]}\n",
    "    # (E) {elem[\"output\"][4]}\n",
    "    # (F) {elem[\"output\"][5]}\n",
    "    # (G) {elem[\"output\"][6]}\"\"\" \n",
    "\n",
    "    # prompt_elem[\"Adversarial-6-options\"] = f\"\"\"{context}\n",
    "    # Based on the paragraph, which of the following options answers the question?\n",
    "    # (A) {elem[\"output\"][0]}\n",
    "    # (B) {elem[\"output\"][1]}\n",
    "    # (C) {elem[\"output\"][2]}\n",
    "    # (D) {elem[\"output\"][3]}\n",
    "    # (E) {elem[\"output\"][4]}\n",
    "    # (F) {elem[\"output\"][5]}\"\"\" \n",
    "\n",
    "    # prompt_elem[\"Adversarial-5-options\"] = f\"\"\"{context}\n",
    "    # Based on the paragraph, which of the following options answers the question?\n",
    "    # (A) {elem[\"output\"][0]}\n",
    "    # (B) {elem[\"output\"][1]}\n",
    "    # (C) {elem[\"output\"][2]}\n",
    "    # (D) {elem[\"output\"][3]}\n",
    "    # (E) {elem[\"output\"][4]}\"\"\"\n",
    "\n",
    "    # prompt_elem[\"Adversarial-4-options\"] = f\"\"\"{context}\n",
    "    # Based on the paragraph, which of the following options answers the question?\n",
    "    # (A) {elem[\"output\"][0]}\n",
    "    # (B) {elem[\"output\"][1]}\n",
    "    # (C) {elem[\"output\"][2]}\n",
    "    # (D) {elem[\"output\"][3]}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-3-options\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\" \n",
    "\n",
    "    # prompt_elem[\"Adversarial-2-options\"] = f\"\"\"{context}\n",
    "    # Based on the paragraph, which of the following options answers the question?\n",
    "    # (A) {elem[\"output\"][0]}\n",
    "    # (B) {elem[\"output\"][1]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, which of the following options answers the question?\n",
    "    If none of the options fit, answer \"N/A\".\n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"N/A\" if none of the options fit:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\"  \n",
    "\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, which of the following options answers the question?\n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\", \"(D)\":\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) None of the above.\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, does any of the following options answer the question?\n",
    "    Answer with a single word - \"yes\" or \"no\":\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\" \n",
    "\n",
    "    prompt_list_zero_shot.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Zero-Shot) - Adversarial - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot_v2 = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "     \n",
    "    context_end = elem[\"input\"].find(\"Correct Answer:\")\n",
    "    context = elem[\"input\"][:context_end] if context_end != -1 else elem[\"input\"] # if there is the right answer in the input - remove it\n",
    "    context = re.sub(r\"Sent \\d+:\", \"\", context) # make all sentences into a single paragraph\n",
    "    context = re.sub(r\"sent \\d+:\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "    context = context.replace(\"Paragraph-\", \"Paragraph:\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "    context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "    context = context.replace(\"Question:\", \"\\nQuestion:\") # move the Question to a new line\n",
    "\n",
    "    context = context.replace(\"?.\", \"?\") # the paragraph and question appear together, followed by a \".\" --> so remove this redundant dot.\n",
    "\n",
    "\n",
    "    prompt_elem[\"Adversarial-7-options\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) {elem[\"output\"][3]}\n",
    "    (E) {elem[\"output\"][4]}\n",
    "    (F) {elem[\"output\"][5]}\n",
    "    (G) {elem[\"output\"][6]}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-6-options\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) {elem[\"output\"][3]}\n",
    "    (E) {elem[\"output\"][4]}\n",
    "    (F) {elem[\"output\"][5]}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-5-options\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) {elem[\"output\"][3]}\n",
    "    (E) {elem[\"output\"][4]}\"\"\"\n",
    "\n",
    "    prompt_elem[\"Adversarial-4-options\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) {elem[\"output\"][3]}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-3-options\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-2-options\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, which of the following options answers the question?\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, which of the following options answers the question?\n",
    "    If none of the options fit, reply \"unanswerable\".\n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"unanswerable\" if none of the options fit:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\"  \n",
    "\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"{context}\n",
    "    Based on the paragraph, which of the following options answers the question?\n",
    "    Keep your answer short - only \"(A)\", \"(B)\", \"(C)\", \"(D)\":\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) None of the above.\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"Determine whether the following context, question and three options are answerable. Reply only \"answerable\" or \"unanswerable\":\n",
    "    {context}\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\" \n",
    "\n",
    "    prompt_list_zero_shot_v2.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Zero-Shot) - Control Group - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot_control_group = list()\n",
    "for elem in data_control_group:\n",
    "\n",
    "    # choose question with at least one correct answer and at least 3 incorrect answers\n",
    "    potential_questions = [i for i,q in enumerate(elem['paragraph']['questions']) if len([answer for answer in q[\"answers\"] if answer[\"isAnswer\"]])>0 and len([answer for answer in q[\"answers\"] if not answer[\"isAnswer\"]])>2]\n",
    "\n",
    "    # if empty, a.k.a there aren't questions with at least one correct answer and at least 3 incorrect answers\n",
    "    if not potential_questions:\n",
    "        continue\n",
    "\n",
    "    for question_i in potential_questions:\n",
    "        \n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        context = f\"Paragraph: {elem['paragraph']['text']}\"\n",
    "        context = re.sub(r\"<b>Sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph\n",
    "        context = re.sub(r\"<b>sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "        context = context.replace(\"<br>\", \"\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "        context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "\n",
    "        context = f\"{context}\\nQuestion: {elem['paragraph']['questions'][question_i]['question']}\"\n",
    "\n",
    "        correct_answer = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if answer[\"isAnswer\"]][0]\n",
    "        incorrect_answers = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if not answer[\"isAnswer\"]]\n",
    "\n",
    "\n",
    "        # prompt_elem[\"Adversarial-7-options\"] = f\"\"\"{context}\n",
    "        # Based on the paragraph, which of the following options answers the question?\n",
    "        # (A) {incorrect_answers[0]}\n",
    "        # (B) {incorrect_answers[1]}\n",
    "        # (C) {incorrect_answers[2]}\n",
    "        # (D) {incorrect_answers[3]}\n",
    "        # (E) {incorrect_answers[4]}\n",
    "        # (F) {incorrect_answers[5]}\n",
    "        # (G) {correct_answer}\"\"\" \n",
    "\n",
    "        # prompt_elem[\"Adversarial-6-options\"] = f\"\"\"{context}\n",
    "        # Based on the paragraph, which of the following options answers the question?\n",
    "        # (A) {incorrect_answers[0]}\n",
    "        # (B) {incorrect_answers[1]}\n",
    "        # (C) {incorrect_answers[2]}\n",
    "        # (D) {incorrect_answers[3]}\n",
    "        # (E) {incorrect_answers[4]}\n",
    "        # (F) {correct_answer}\"\"\" \n",
    "\n",
    "        # prompt_elem[\"Adversarial-5-options\"] = f\"\"\"{context}\n",
    "        # Based on the paragraph, which of the following options answers the question?\n",
    "        # (A) {incorrect_answers[0]}\n",
    "        # (B) {incorrect_answers[1]}\n",
    "        # (C) {incorrect_answers[2]}\n",
    "        # (D) {incorrect_answers[3]}\n",
    "        # (E) {correct_answer}\"\"\" \n",
    "\n",
    "        # prompt_elem[\"Adversarial-4-options\"] = f\"\"\"{context}\n",
    "        # Based on the paragraph, which of the following options answers the question?\n",
    "        # (A) {incorrect_answers[0]}\n",
    "        # (B) {incorrect_answers[1]}\n",
    "        # (C) {incorrect_answers[2]}\n",
    "        # (D) {correct_answer}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Adversarial-3-options\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, which of the following options answers the question?\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {correct_answer}\"\"\" \n",
    "\n",
    "        # prompt_elem[\"Adversarial-2-options\"] = f\"\"\"{context}\n",
    "        # Based on the paragraph, which of the following options answers the question?\n",
    "        # (A) {incorrect_answers[0]}\n",
    "        # (B) {correct_answer}\"\"\" \n",
    "\n",
    "        \n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, which of the following options answers the question?\n",
    "        If none of the options fit, answer \"N/A\".\n",
    "        Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"N/A\" if none of the options fit:\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {correct_answer}\"\"\"  \n",
    "        \n",
    "        prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, which of the following options answers the question?\n",
    "        Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"(D)\":\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {correct_answer}\n",
    "        (D) None of the above.\"\"\" \n",
    "\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, does any of the following options answer the question? \n",
    "        Answer with a single word - \"yes\" or \"no\":\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {correct_answer}\"\"\" \n",
    "\n",
    "        prompt_list_zero_shot_control_group.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Zero-Shot) - Control Group - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_zero_shot_control_group_v2 = list()\n",
    "for elem in data_control_group:\n",
    "\n",
    "    # choose question with at least one correct answer and at least 3 incorrect answers\n",
    "    potential_questions = [i for i,q in enumerate(elem['paragraph']['questions']) if len([answer for answer in q[\"answers\"] if answer[\"isAnswer\"]])>0 and len([answer for answer in q[\"answers\"] if not answer[\"isAnswer\"]])>2]\n",
    "\n",
    "    # if empty, a.k.a there aren't questions with at least one correct answer and at least 3 incorrect answers\n",
    "    if not potential_questions:\n",
    "        continue\n",
    "\n",
    "    for question_i in potential_questions:\n",
    "        \n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        context = f\"Paragraph: {elem['paragraph']['text']}\"\n",
    "        context = re.sub(r\"<b>Sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph\n",
    "        context = re.sub(r\"<b>sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "        context = context.replace(\"<br>\", \"\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "        context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "\n",
    "        context = f\"{context}\\nQuestion: {elem['paragraph']['questions'][question_i]['question']}\"\n",
    "\n",
    "        correct_answer = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if answer[\"isAnswer\"]][0]\n",
    "        incorrect_answers = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if not answer[\"isAnswer\"]]\n",
    "\n",
    "\n",
    "        prompt_elem[\"Adversarial-7-options\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, which of the following options answers the question?\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {incorrect_answers[2]}\n",
    "        (D) {incorrect_answers[3]}\n",
    "        (E) {incorrect_answers[4]}\n",
    "        (F) {incorrect_answers[5]}\n",
    "        (G) {correct_answer}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Adversarial-6-options\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, which of the following options answers the question?\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {incorrect_answers[2]}\n",
    "        (D) {incorrect_answers[3]}\n",
    "        (E) {incorrect_answers[4]}\n",
    "        (F) {correct_answer}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Adversarial-5-options\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, which of the following options answers the question?\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {incorrect_answers[2]}\n",
    "        (D) {incorrect_answers[3]}\n",
    "        (E) {correct_answer}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Adversarial-4-options\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, which of the following options answers the question?\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {incorrect_answers[2]}\n",
    "        (D) {correct_answer}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Adversarial-3-options\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, which of the following options answers the question?\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {correct_answer}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Adversarial-2-options\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, which of the following options answers the question?\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {correct_answer}\"\"\" \n",
    "\n",
    "        \n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, which of the following options answers the question?\n",
    "        If none of the options fit, answer \"unanswerable\".\n",
    "        Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"unanswerable\" if none of the options fit:\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {correct_answer}\"\"\"  \n",
    "        \n",
    "        prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"{context}\n",
    "        Based on the paragraph, which of the following options answers the question?\n",
    "        Keep your answer short - only \"(A)\", \"(B)\", \"(C)\" or \"(D)\":\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {correct_answer}\n",
    "        (D) None of the above.\"\"\" \n",
    "\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"Determine whether the following context, question and three options are answerable. Reply only \"answerable\" or \"unanswerable\":\n",
    "        {context}\n",
    "        (A) {incorrect_answers[0]}\n",
    "        (B) {incorrect_answers[1]}\n",
    "        (C) {correct_answer}\"\"\" \n",
    "\n",
    "        prompt_list_zero_shot_control_group_v2.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Few-Shot) - Adversarial - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(example, is_CoT):\n",
    "    generated_example = f\"\"\"{example[\"input\"]} \n",
    "Options:\n",
    " (A) {example[\"option A\"]}\n",
    " (B) {example[\"option B\"]}\n",
    " (C) {example[\"option C\"]}\"\"\"\n",
    "\n",
    "    if 'option D' in example.keys():\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    " (D) {example[\"option D\"]}\"\"\"\n",
    "        \n",
    "    if is_CoT:\n",
    "        if not example[\"Answer\"] in [\"(A)\", \"(B)\", \"(C)\", \"(D)\"]:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, {example[\"Answer\"]}.\"\"\"\n",
    "        else:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, the answer is {example[\"Answer\"]}.\"\"\"\n",
    "    \n",
    "    else:\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"Answer\"]}\"\"\"\n",
    "    return generated_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples_dict_multirc_few_shot():\n",
    "    pos_example_1 = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "    \n",
    "    pos_example_2 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '19.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19.'}\n",
    "\n",
    "\n",
    "    neg_example_1 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': 'none of the options answers the question',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    neg_example_2 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(D)',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    neg_example_3 = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "\n",
    "    pos_example_1_answerability = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'Answer': 'there is a correct option (option A)',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "    \n",
    "    \n",
    "    neg_example_1_answerability = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': 'there is no correct option',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_adversarial_prompt(example_1, example_2, is_CoT, context, options):\n",
    "    option_names = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\"]\n",
    "\n",
    "    full_prompt = f\"\"\"\n",
    "                    Example 1:\n",
    "                    {get_example(example_1, is_CoT)}\n",
    "\n",
    "                    Example 2: \n",
    "                    {get_example(example_2, is_CoT)}\n",
    "                    \n",
    "                    Now your turn:\n",
    "                    {context}\n",
    "                    Options:\"\"\"\n",
    "\n",
    "    for i,option in enumerate(options):\n",
    "        full_prompt = f\"\"\"{full_prompt}\n",
    "                        {option_names[i]} {option}\"\"\"\n",
    "    \n",
    "    full_prompt = f\"{full_prompt}\\n Output:\"\n",
    "    full_prompt = re.sub(' +', ' ', full_prompt) # replace consecutive spaces with a single space\n",
    "    full_prompt = full_prompt.replace(\"\\nQuestion:\", \"\\n Question:\").replace(\"\\nOptions:\", \"\\n Options:\").replace(\"\\nOutput:\", \"\\n Output:\")\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot = list()\n",
    "\n",
    "pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_examples_dict_multirc_few_shot()\n",
    "\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "     \n",
    "    context_end = elem[\"input\"].find(\"Correct Answer:\")\n",
    "    context = elem[\"input\"][:context_end] if context_end != -1 else elem[\"input\"] # if there is the right answer in the input - remove it\n",
    "    context = re.sub(r\"Sent \\d+:\", \"\", context) # make all sentences into a single paragraph\n",
    "    context = re.sub(r\"sent \\d+:\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "    context = context.replace(\"Paragraph-\", \"Paragraph:\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "    context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "    context = context.replace(\"Question:\", \"\\nQuestion:\") # move the Question to a new line\n",
    "\n",
    "    context = context.replace(\"?.\", \"?\") # the paragraph and question appear together, followed by a \".\" --> so remove this redundant dot.\n",
    "\n",
    "    \n",
    "\n",
    "    # ############## Adversarial-7-options ##############\n",
    "    # options = [option for option in elem[\"output\"][:7]]\n",
    "    # prompt_elem[\"Adversarial-7-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # # CoT \n",
    "    # options = [option for option in elem[\"output\"][:7]]\n",
    "    # prompt_elem[\"Adversarial-7-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    # ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    # ############## Adversarial-6-options ############\n",
    "    # options = [option for option in elem[\"output\"][:6]]\n",
    "    # prompt_elem[\"Adversarial-6-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # # CoT \n",
    "    # options = [option for option in elem[\"output\"][:6]]\n",
    "    # prompt_elem[\"Adversarial-6-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    # #################################################\n",
    "\n",
    "\n",
    "\n",
    "    # ############## Adversarial-5-options ##############\n",
    "    # options = [option for option in elem[\"output\"][:5]]\n",
    "    # prompt_elem[\"Adversarial-5-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # # CoT \n",
    "    # options = [option for option in elem[\"output\"][:5]]\n",
    "    # prompt_elem[\"Adversarial-5-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    # ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    # ############## Adversarial-4-options ##############\n",
    "    # options = [option for option in elem[\"output\"][:4]]\n",
    "    # prompt_elem[\"Adversarial-4-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # # CoT \n",
    "    # options = [option for option in elem[\"output\"][:4]]\n",
    "    # prompt_elem[\"Adversarial-4-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    # ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-3-options ##############\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Adversarial-3-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Adversarial-3-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    # ############## Adversarial-2-options ##############\n",
    "    # options = [option for option in elem[\"output\"][:2]]\n",
    "    # prompt_elem[\"Adversarial-2-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # # CoT \n",
    "    # options = [option for option in elem[\"output\"][:2]]\n",
    "    # prompt_elem[\"Adversarial-2-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    # ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ################# Pseudo-Adversarial ###############\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Pseudo-Adversarial-CoT\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option ##########\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "    ################### Answerability ##################\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Answerability\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Answerability-CoT\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    prompt_list_few_shot.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Few-Shot) - Control Group - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_control_group = list()\n",
    "\n",
    "pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_examples_dict_multirc_few_shot()\n",
    "\n",
    "for elem in data_control_group:\n",
    "\n",
    "    # choose question with at least one correct answer and at least 3 incorrect answers\n",
    "    potential_questions = [i for i,q in enumerate(elem['paragraph']['questions']) if len([answer for answer in q[\"answers\"] if answer[\"isAnswer\"]])>0 and len([answer for answer in q[\"answers\"] if not answer[\"isAnswer\"]])>2]\n",
    "\n",
    "    # if empty, a.k.a there aren't questions with at least one correct answer and at least 2 incorrect answers\n",
    "    if not potential_questions:\n",
    "        continue\n",
    "\n",
    "    for question_i in potential_questions:\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        context = f\"Paragraph: {elem['paragraph']['text']}\"\n",
    "        context = re.sub(r\"<b>Sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph\n",
    "        context = re.sub(r\"<b>sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "        context = context.replace(\"<br>\", \"\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "        context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "\n",
    "        context = f\"{context}\\nQuestion: {elem['paragraph']['questions'][question_i]['question']}\"\n",
    "\n",
    "        correct_answer = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if answer[\"isAnswer\"]][0]\n",
    "        incorrect_answers = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if not answer[\"isAnswer\"]]\n",
    "\n",
    "        adversarial_instructions = \"Given the following paragraph, question, and several options, which of the options answers the question?\"\n",
    "        pseudo_adversarial_instructions = f'{adversarial_instructions} \\nIf none of the options fit, answer \"N/A\".'\n",
    "        answerability_instructions = 'Based on the paragraph, does any of the following options answer the question? \\nAnswer with a single word - \"yes\" or \"no\".'\n",
    "\n",
    "\n",
    "\n",
    "        # ############## Adversarial-7-options ##############\n",
    "        # options = incorrect_answers[:6]\n",
    "        # options.append(correct_answer)    \n",
    "        # prompt_elem[\"Adversarial-7-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # # CoT \n",
    "        # options = incorrect_answers[:6]\n",
    "        # options.append(correct_answer)    \n",
    "        # prompt_elem[\"Adversarial-7-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        # ####################################################\n",
    "\n",
    "\n",
    "        # ############## Adversarial-6-options ##############\n",
    "        # options = incorrect_answers[:5]\n",
    "        # options.append(correct_answer)    \n",
    "        # prompt_elem[\"Adversarial-6-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # # CoT \n",
    "        # options = incorrect_answers[:5]\n",
    "        # options.append(correct_answer)    \n",
    "        # prompt_elem[\"Adversarial-6-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        # ####################################################\n",
    "\n",
    "        # ############## Adversarial-5-options ##############\n",
    "        # options = incorrect_answers[:4]\n",
    "        # options.append(correct_answer)    \n",
    "        # prompt_elem[\"Adversarial-5-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # # CoT \n",
    "        # options = incorrect_answers[:4]\n",
    "        # options.append(correct_answer)    \n",
    "        # prompt_elem[\"Adversarial-5-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        # ####################################################\n",
    "\n",
    "\n",
    "        # ############## Adversarial-4-options ##############\n",
    "        # options = incorrect_answers[:3]\n",
    "        # options.append(correct_answer)    \n",
    "        # prompt_elem[\"Adversarial-4-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # # CoT \n",
    "        # options = incorrect_answers[:3]\n",
    "        # options.append(correct_answer)    \n",
    "        # prompt_elem[\"Adversarial-4-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        # ####################################################\n",
    "\n",
    "\n",
    "        ############## Adversarial-3-options ##############\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-3-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-3-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        ####################################################\n",
    "\n",
    "        # ############## Adversarial-2-options ##############\n",
    "        # options = incorrect_answers[:1]\n",
    "        # options.append(correct_answer)    \n",
    "        # prompt_elem[\"Adversarial-2-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # # CoT \n",
    "        # options = incorrect_answers[:1]\n",
    "        # options.append(correct_answer)    \n",
    "        # prompt_elem[\"Adversarial-2-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        # ####################################################\n",
    "\n",
    "\n",
    "        ############## Pseudo-Adversarial ##############\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Pseudo-Adversarial-CoT\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, True, context, options)\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        ############ Adversarial-NA-fourth-option ############\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, True, context, options)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "        #################### Answerability ###################\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Answerability\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Answerability-CoT\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, True, context, options)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "        prompt_list_few_shot_control_group.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Few-Shot) - Adversarial - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(example, is_CoT):\n",
    "    generated_example = f\"\"\"{example[\"input\"]} \n",
    "Options:\n",
    " (A) {example[\"option A\"]}\n",
    " (B) {example[\"option B\"]}\n",
    " (C) {example[\"option C\"]}\"\"\"\n",
    "\n",
    "    if 'option D' in example.keys():\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    " (D) {example[\"option D\"]}\"\"\"\n",
    "        \n",
    "    if is_CoT:\n",
    "        if not example[\"Answer\"] in [\"(A)\", \"(B)\", \"(C)\", \"(D)\"]:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, {example[\"Answer\"]}.\"\"\"\n",
    "        else:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, the answer is {example[\"Answer\"]}.\"\"\"\n",
    "    \n",
    "    else:\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"Answer\"]}\"\"\"\n",
    "    return generated_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples_dict_multirc_few_shot_v2():\n",
    "    pos_example_1 = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "    \n",
    "    pos_example_2 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '19.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19.'}\n",
    "\n",
    "\n",
    "    neg_example_1 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': 'unanswerable',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    neg_example_2 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(D)',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    neg_example_3 = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "\n",
    "    pos_example_1_answerability = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'Answer': 'answerable',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "    \n",
    "    \n",
    "    neg_example_1_answerability = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': 'unanswerable',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_adversarial_prompt(example_1, example_2, is_CoT, context, options):\n",
    "    option_names = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\"]\n",
    "\n",
    "    full_prompt = f\"\"\"\n",
    "                    Example 1:\n",
    "                    {get_example(example_1, is_CoT)}\n",
    "\n",
    "                    Example 2: \n",
    "                    {get_example(example_2, is_CoT)}\n",
    "                    \n",
    "                    Now your turn:\n",
    "                    {context}\n",
    "                    Options:\"\"\"\n",
    "\n",
    "    for i,option in enumerate(options):\n",
    "        full_prompt = f\"\"\"{full_prompt}\n",
    "                        {option_names[i]} {option}\"\"\"\n",
    "    \n",
    "    full_prompt = f\"{full_prompt}\\n Output:\"\n",
    "    full_prompt = re.sub(' +', ' ', full_prompt) # replace consecutive spaces with a single space\n",
    "    full_prompt = full_prompt.replace(\"\\nQuestion:\", \"\\n Question:\").replace(\"\\nOptions:\", \"\\n Options:\").replace(\"\\nOutput:\", \"\\n Output:\")\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_v2 = list()\n",
    "\n",
    "pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_examples_dict_multirc_few_shot_v2()\n",
    "\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "     \n",
    "    context_end = elem[\"input\"].find(\"Correct Answer:\")\n",
    "    context = elem[\"input\"][:context_end] if context_end != -1 else elem[\"input\"] # if there is the right answer in the input - remove it\n",
    "    context = re.sub(r\"Sent \\d+:\", \"\", context) # make all sentences into a single paragraph\n",
    "    context = re.sub(r\"sent \\d+:\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "    context = context.replace(\"Paragraph-\", \"Paragraph:\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "    context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "    context = context.replace(\"Question:\", \"\\nQuestion:\") # move the Question to a new line\n",
    "\n",
    "    context = context.replace(\"?.\", \"?\") # the paragraph and question appear together, followed by a \".\" --> so remove this redundant dot.\n",
    "\n",
    "    \n",
    "\n",
    "    ############## Adversarial-7-options ##############\n",
    "    options = [option for option in elem[\"output\"][:7]]\n",
    "    prompt_elem[\"Adversarial-7-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:7]]\n",
    "    prompt_elem[\"Adversarial-7-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-6-options ############\n",
    "    options = [option for option in elem[\"output\"][:6]]\n",
    "    prompt_elem[\"Adversarial-6-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:6]]\n",
    "    prompt_elem[\"Adversarial-6-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    #################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-5-options ##############\n",
    "    options = [option for option in elem[\"output\"][:5]]\n",
    "    prompt_elem[\"Adversarial-5-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:5]]\n",
    "    prompt_elem[\"Adversarial-5-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-4-options ##############\n",
    "    options = [option for option in elem[\"output\"][:4]]\n",
    "    prompt_elem[\"Adversarial-4-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:4]]\n",
    "    prompt_elem[\"Adversarial-4-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-3-options ##############\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Adversarial-3-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Adversarial-3-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-2-options ##############\n",
    "    options = [option for option in elem[\"output\"][:2]]\n",
    "    prompt_elem[\"Adversarial-2-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:2]]\n",
    "    prompt_elem[\"Adversarial-2-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ################# Pseudo-Adversarial ###############\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Pseudo-Adversarial-CoT\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option ##########\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "    ################### Answerability ##################\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Answerability\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, False, context, options)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Answerability-CoT\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, True, context, options)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    prompt_list_few_shot_v2.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Few-Shot) - Control Group - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_control_group_v2 = list()\n",
    "\n",
    "pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_examples_dict_multirc_few_shot_v2()\n",
    "\n",
    "for elem in data_control_group:\n",
    "\n",
    "    # choose question with at least one correct answer and at least 3 incorrect answers\n",
    "    potential_questions = [i for i,q in enumerate(elem['paragraph']['questions']) if len([answer for answer in q[\"answers\"] if answer[\"isAnswer\"]])>0 and len([answer for answer in q[\"answers\"] if not answer[\"isAnswer\"]])>2]\n",
    "\n",
    "    # if empty, a.k.a there aren't questions with at least one correct answer and at least 3 incorrect answers\n",
    "    if not potential_questions:\n",
    "        continue\n",
    "\n",
    "    for question_i in potential_questions:\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        context = f\"Paragraph: {elem['paragraph']['text']}\"\n",
    "        context = re.sub(r\"<b>Sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph\n",
    "        context = re.sub(r\"<b>sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "        context = context.replace(\"<br>\", \"\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "        context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "\n",
    "        context = f\"{context}\\nQuestion: {elem['paragraph']['questions'][question_i]['question']}\"\n",
    "\n",
    "        correct_answer = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if answer[\"isAnswer\"]][0]\n",
    "        incorrect_answers = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if not answer[\"isAnswer\"]]\n",
    "\n",
    "        adversarial_instructions = \"Given the following paragraph, question, and several options, which of the options answers the question?\"\n",
    "        pseudo_adversarial_instructions = f'{adversarial_instructions} \\nIf none of the options fit, answer \"N/A\".'\n",
    "        answerability_instructions = 'Based on the paragraph, does any of the following options answer the question? \\nAnswer with a single word - \"yes\" or \"no\".'\n",
    "\n",
    "\n",
    "\n",
    "        ############## Adversarial-7-options ##############\n",
    "        options = incorrect_answers[:6]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-7-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:6]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-7-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        ############## Adversarial-6-options ##############\n",
    "        options = incorrect_answers[:5]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-6-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:5]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-6-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        ####################################################\n",
    "\n",
    "        ############## Adversarial-5-options ##############\n",
    "        options = incorrect_answers[:4]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-5-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:4]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-5-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        ############## Adversarial-4-options ##############\n",
    "        options = incorrect_answers[:3]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-4-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:3]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-4-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        ############## Adversarial-3-options ##############\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-3-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-3-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        ####################################################\n",
    "\n",
    "        ############## Adversarial-2-options ##############\n",
    "        options = incorrect_answers[:1]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-2-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:1]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Adversarial-2-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options)\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        ############## Pseudo-Adversarial ##############\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)    \n",
    "        prompt_elem[\"Pseudo-Adversarial-CoT\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, True, context, options)\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        ############ Adversarial-NA-fourth-option ############\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, True, context, options)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "        #################### Answerability ###################\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Answerability\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, False, context, options)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Answerability-CoT\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, True, context, options)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "        prompt_list_few_shot_control_group_v2.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Few-Shot with Instructions) - Adversarial - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(example, is_CoT):\n",
    "    generated_example = f\"\"\"{example[\"input\"]} \n",
    "Options:\n",
    " (A) {example[\"option A\"]}\n",
    " (B) {example[\"option B\"]}\n",
    " (C) {example[\"option C\"]}\"\"\"\n",
    "\n",
    "    if 'option D' in example.keys():\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    " (D) {example[\"option D\"]}\"\"\"\n",
    "        \n",
    "    if is_CoT:\n",
    "        if not example[\"Answer\"] in [\"(A)\", \"(B)\", \"(C)\", \"(D)\"]:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, {example[\"Answer\"]}.\"\"\"\n",
    "        else:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, the answer is {example[\"Answer\"]}.\"\"\"\n",
    "    \n",
    "    else:\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"Answer\"]}\"\"\"\n",
    "    return generated_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples_dict_multirc_few_shot_with_instructions():\n",
    "    pos_example_1 = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "    \n",
    "    pos_example_2 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '19.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19.'}\n",
    "\n",
    "\n",
    "    neg_example_1 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': 'N/A',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    neg_example_2 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(D)',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    neg_example_3 = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "\n",
    "    pos_example_1_answerability = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'Answer': 'yes',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "    \n",
    "    \n",
    "    neg_example_1_answerability = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': 'no',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_adversarial_prompt(example_1, example_2, is_CoT, context, options, instructions):\n",
    "    option_names = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\"]\n",
    "\n",
    "    full_prompt = f\"\"\"{instructions}\n",
    "    \n",
    "                    Example 1:\n",
    "                    {get_example(example_1, is_CoT)}\n",
    "\n",
    "                    Example 2: \n",
    "                    {get_example(example_2, is_CoT)}\n",
    "                    \n",
    "                    Now your turn:\n",
    "                    {context}\n",
    "                    Options:\"\"\"\n",
    "\n",
    "    for i,option in enumerate(options):\n",
    "        full_prompt = f\"\"\"{full_prompt}\n",
    "                        {option_names[i]} {option}\"\"\"\n",
    "    \n",
    "    full_prompt = f\"{full_prompt}\\n Output:\"\n",
    "    full_prompt = re.sub(' +', ' ', full_prompt) # replace consecutive spaces with a single space\n",
    "    full_prompt = full_prompt.replace(\"\\nQuestion:\", \"\\n Question:\").replace(\"\\nOptions:\", \"\\n Options:\").replace(\"\\nOutput:\", \"\\n Output:\")\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_with_instructions = list()\n",
    "\n",
    "pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_examples_dict_multirc_few_shot_with_instructions()\n",
    "\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "     \n",
    "    context_end = elem[\"input\"].find(\"Correct Answer:\")\n",
    "    context = elem[\"input\"][:context_end] if context_end != -1 else elem[\"input\"] # if there is the right answer in the input - remove it\n",
    "    context = re.sub(r\"Sent \\d+:\", \"\", context) # make all sentences into a single paragraph\n",
    "    context = re.sub(r\"sent \\d+:\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "    context = context.replace(\"Paragraph-\", \"Paragraph:\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "    context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "    context = context.replace(\"Question:\", \"\\nQuestion:\") # move the Question to a new line\n",
    "\n",
    "    context = context.replace(\"?.\", \"?\") # the paragraph and question appear together, followed by a \".\" --> so remove this redundant dot.\n",
    "\n",
    "    adversarial_instructions = \"Given the following paragraph, question, and several options, which of the options answers the question?\"\n",
    "    pseudo_adversarial_instructions = f'{adversarial_instructions} \\nIf none of the options fit, answer \"N/A\".'\n",
    "    answerability_instructions = 'Based on the paragraph, does any of the following options answer the question? \\nAnswer with a single word - \"yes\" or \"no\".'\n",
    "\n",
    "\n",
    "    # ############## Adversarial-7-options ##############\n",
    "    # options = [option for option in elem[\"output\"][:7]]\n",
    "    # prompt_elem[\"Adversarial-7-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # # CoT \n",
    "    # options = [option for option in elem[\"output\"][:7]]\n",
    "    # prompt_elem[\"Adversarial-7-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    # ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    # ############## Adversarial-6-options ############\n",
    "    # options = [option for option in elem[\"output\"][:6]]\n",
    "    # prompt_elem[\"Adversarial-6-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # # CoT \n",
    "    # options = [option for option in elem[\"output\"][:6]]\n",
    "    # prompt_elem[\"Adversarial-6-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    # #################################################\n",
    "\n",
    "\n",
    "\n",
    "    # ############## Adversarial-5-options ##############\n",
    "    # options = [option for option in elem[\"output\"][:5]]\n",
    "    # prompt_elem[\"Adversarial-5-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # # CoT \n",
    "    # options = [option for option in elem[\"output\"][:5]]\n",
    "    # prompt_elem[\"Adversarial-5-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    # ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    # ############## Adversarial-4-options ##############\n",
    "    # options = [option for option in elem[\"output\"][:4]]\n",
    "    # prompt_elem[\"Adversarial-4-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # # CoT \n",
    "    # options = [option for option in elem[\"output\"][:4]]\n",
    "    # prompt_elem[\"Adversarial-4-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    # ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-3-options ##############\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Adversarial-3-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Adversarial-3-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    # ############## Adversarial-2-options ##############\n",
    "    # options = [option for option in elem[\"output\"][:2]]\n",
    "    # prompt_elem[\"Adversarial-2-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # # CoT \n",
    "    # options = [option for option in elem[\"output\"][:2]]\n",
    "    # prompt_elem[\"Adversarial-2-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    # ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ################# Pseudo-Adversarial ###############\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, False, context, options, pseudo_adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Pseudo-Adversarial-CoT\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, True, context, options, pseudo_adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option ##########\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, False, context, options, adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, True, context, options, adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option-Ablation1 ##########\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option-Ablation1\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option-Ablation1-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ################### Answerability ##################\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Answerability\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, False, context, options, answerability_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Answerability-CoT\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, True, context, options, answerability_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    prompt_list_few_shot_with_instructions.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Few-Shot with Instructions) - Adversarial - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(example, is_CoT):\n",
    "    generated_example = f\"\"\"{example[\"input\"]} \n",
    "Options:\n",
    " (A) {example[\"option A\"]}\n",
    " (B) {example[\"option B\"]}\n",
    " (C) {example[\"option C\"]}\"\"\"\n",
    "\n",
    "    if 'option D' in example.keys():\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    " (D) {example[\"option D\"]}\"\"\"\n",
    "        \n",
    "    if is_CoT:\n",
    "        if not example[\"Answer\"] in [\"(A)\", \"(B)\", \"(C)\", \"(D)\"]:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, {example[\"Answer\"]}.\"\"\"\n",
    "        else:\n",
    "            generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"CoT\"]} Therefore, the answer is {example[\"Answer\"]}.\"\"\"\n",
    "    \n",
    "    else:\n",
    "        generated_example = f\"\"\"{generated_example}\n",
    "Output: {example[\"Answer\"]}\"\"\"\n",
    "    return generated_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples_dict_multirc_few_shot_with_instructions_v2():\n",
    "    pos_example_1 = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "    \n",
    "    pos_example_2 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '19.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': '(B)',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19.'}\n",
    "\n",
    "\n",
    "    neg_example_1 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': 'unanswerable',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    neg_example_2 = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(D)',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    neg_example_3 = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'option D': 'None of the above.',\n",
    "                     'Answer': '(A)',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "\n",
    "    pos_example_1_answerability = {'input': \"Paragraph: It was hot that day. The temperature on the wall of the backyard was showing something well over 100 F. Meanwhile Tom, at home, was trying finish the remainder of carrots from last night, and packing for his trip to Chicago tomorrow. As employees of the Art Museum, Tom and his older cousin often had to travel to Chicago. \\nQuestion: What was the temperature outside, when Tom was eating carrots?\",\n",
    "                     'option A': 'Well over 100 F.',\n",
    "                     'option B': 'Not very hot.',\n",
    "                     'option C': 'Far below 100 F.',\n",
    "                     'Answer': 'answerable',\n",
    "                     'CoT':'The second sentence says that \"The temperature on the wall of the backyard was showing something well over 100 F\". Then, it says that at the same time, Tom was \"trying to finish the remainder of carrots\".'}\n",
    "    \n",
    "    \n",
    "    neg_example_1_answerability = {'input': \"Paragraph: Obama was born on August 4, 1961, at Kapi併olani Maternity & Gynecological Hospital in Honolulu, Hawaii. He is the only President to have been born in Hawaii. He was born to a white mother and a black father. His mother, Ann Dunham (1942-1995), was born in Wichita, Kansas, of mostly English descent, with some German, Irish, Scottish, Swiss, and Welsh ancestry. \\nQuestion: How old was Obama's mother when he was born?\",\n",
    "                     'option A': '4.',\n",
    "                     'option B': '29.',\n",
    "                     'option C': '1961.',\n",
    "                     'Answer': 'unanswerable',\n",
    "                     'CoT':'Obama\\'s mother was born 1942. Obama was born in 1961. 1961 - 1942 = 19. So, the correct answer is 19.'}\n",
    "\n",
    "    return pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_adversarial_prompt(example_1, example_2, is_CoT, context, options, instructions):\n",
    "    option_names = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\"]\n",
    "\n",
    "    full_prompt = f\"\"\"{instructions}\n",
    "    \n",
    "                    Example 1:\n",
    "                    {get_example(example_1, is_CoT)}\n",
    "\n",
    "                    Example 2: \n",
    "                    {get_example(example_2, is_CoT)}\n",
    "                    \n",
    "                    Now your turn:\n",
    "                    {context}\n",
    "                    Options:\"\"\"\n",
    "\n",
    "    for i,option in enumerate(options):\n",
    "        full_prompt = f\"\"\"{full_prompt}\n",
    "                        {option_names[i]} {option}\"\"\"\n",
    "    \n",
    "    full_prompt = f\"{full_prompt}\\n Output:\"\n",
    "    full_prompt = re.sub(' +', ' ', full_prompt) # replace consecutive spaces with a single space\n",
    "    full_prompt = full_prompt.replace(\"\\nQuestion:\", \"\\n Question:\").replace(\"\\nOptions:\", \"\\n Options:\").replace(\"\\nOutput:\", \"\\n Output:\")\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_with_instructions_v2 = list()\n",
    "\n",
    "pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_examples_dict_multirc_few_shot_with_instructions_v2()\n",
    "\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "     \n",
    "    context_end = elem[\"input\"].find(\"Correct Answer:\")\n",
    "    context = elem[\"input\"][:context_end] if context_end != -1 else elem[\"input\"] # if there is the right answer in the input - remove it\n",
    "    context = re.sub(r\"Sent \\d+:\", \"\", context) # make all sentences into a single paragraph\n",
    "    context = re.sub(r\"sent \\d+:\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "    context = context.replace(\"Paragraph-\", \"Paragraph:\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "    context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "    context = context.replace(\"Question:\", \"\\nQuestion:\") # move the Question to a new line\n",
    "\n",
    "    context = context.replace(\"?.\", \"?\") # the paragraph and question appear together, followed by a \".\" --> so remove this redundant dot.\n",
    "\n",
    "    adversarial_instructions = \"Given the following paragraph, question, and several options, which of the options answers the question?\"\n",
    "    pseudo_adversarial_instructions = f'{adversarial_instructions} \\nIf none of the options fit, answer \"unanswerable\".'\n",
    "    answerability_instructions = 'Determine whether the following context, question and three options are answerable. Reply only \"answerable\" or \"unanswerable\":'\n",
    "\n",
    "\n",
    "    ############## Adversarial-7-options ##############\n",
    "    options = [option for option in elem[\"output\"][:7]]\n",
    "    prompt_elem[\"Adversarial-7-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:7]]\n",
    "    prompt_elem[\"Adversarial-7-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-6-options ############\n",
    "    options = [option for option in elem[\"output\"][:6]]\n",
    "    prompt_elem[\"Adversarial-6-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:6]]\n",
    "    prompt_elem[\"Adversarial-6-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    #################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-5-options ##############\n",
    "    options = [option for option in elem[\"output\"][:5]]\n",
    "    prompt_elem[\"Adversarial-5-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:5]]\n",
    "    prompt_elem[\"Adversarial-5-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-4-options ##############\n",
    "    options = [option for option in elem[\"output\"][:4]]\n",
    "    prompt_elem[\"Adversarial-4-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:4]]\n",
    "    prompt_elem[\"Adversarial-4-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-3-options ##############\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Adversarial-3-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Adversarial-3-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############## Adversarial-2-options ##############\n",
    "    options = [option for option in elem[\"output\"][:2]]\n",
    "    prompt_elem[\"Adversarial-2-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:2]]\n",
    "    prompt_elem[\"Adversarial-2-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ################# Pseudo-Adversarial ###############\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, False, context, options, pseudo_adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Pseudo-Adversarial-CoT\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, True, context, options, pseudo_adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option ##########\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, False, context, options, adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, True, context, options, adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ############ Adversarial-NA-fourth-option-Ablation1 ##########\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option-Ablation1\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    options.append(\"None of the above.\")\n",
    "    prompt_elem[\"Adversarial-NA-fourth-option-Ablation1-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ################### Answerability ##################\n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Answerability\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, False, context, options, answerability_instructions)\n",
    "    # CoT \n",
    "    options = [option for option in elem[\"output\"][:3]]\n",
    "    prompt_elem[\"Answerability-CoT\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, True, context, options, answerability_instructions)\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    prompt_list_few_shot_with_instructions_v2.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Few-Shot with Instructions) - Control Group - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_with_instructions_control_group = list()\n",
    "\n",
    "pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_examples_dict_multirc_few_shot_with_instructions()\n",
    "\n",
    "for elem in data_control_group:\n",
    "\n",
    "    # choose question with at least one correct answer and at least 3 incorrect answers\n",
    "    potential_questions = [i for i,q in enumerate(elem['paragraph']['questions']) if len([answer for answer in q[\"answers\"] if answer[\"isAnswer\"]])>0 and len([answer for answer in q[\"answers\"] if not answer[\"isAnswer\"]])>2]\n",
    "\n",
    "    # if empty, a.k.a there aren't questions with at least one correct answer and at least 3 incorrect answers\n",
    "    if not potential_questions:\n",
    "        continue\n",
    "\n",
    "    for question_i in potential_questions:\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        context = f\"Paragraph: {elem['paragraph']['text']}\"\n",
    "        context = re.sub(r\"<b>Sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph\n",
    "        context = re.sub(r\"<b>sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "        context = context.replace(\"<br>\", \"\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "        context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "\n",
    "        context = f\"{context}\\nQuestion: {elem['paragraph']['questions'][question_i]['question']}\"\n",
    "\n",
    "        correct_answer = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if answer[\"isAnswer\"]][0]\n",
    "        incorrect_answers = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if not answer[\"isAnswer\"]]\n",
    "\n",
    "        adversarial_instructions = \"Given the following paragraph, question, and several options, which of the options answers the question?\"\n",
    "        pseudo_adversarial_instructions = f'{adversarial_instructions} \\nIf none of the options fit, answer \"N/A\".'\n",
    "        answerability_instructions = 'Based on the paragraph, does any of the following options answer the question? \\nAnswer with a single word - \"yes\" or \"no\".'\n",
    "\n",
    "\n",
    "\n",
    "        # ############## Adversarial-7-options ##############\n",
    "        # options = incorrect_answers[:6]\n",
    "        # options.append(correct_answer)\n",
    "        # prompt_elem[\"Adversarial-7-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # # CoT \n",
    "        # options = incorrect_answers[:6]\n",
    "        # options.append(correct_answer)\n",
    "        # prompt_elem[\"Adversarial-7-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        # ####################################################\n",
    "\n",
    "\n",
    "        # ############## Adversarial-6-options ###############\n",
    "        # options = incorrect_answers[:5]\n",
    "        # options.append(correct_answer)\n",
    "        # prompt_elem[\"Adversarial-6-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # # CoT \n",
    "        # options = incorrect_answers[:5]\n",
    "        # options.append(correct_answer)\n",
    "        # prompt_elem[\"Adversarial-6-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        # ####################################################\n",
    "\n",
    "\n",
    "        # ############## Adversarial-5-options ##############\n",
    "        # options = incorrect_answers[:4]\n",
    "        # options.append(correct_answer)\n",
    "        # prompt_elem[\"Adversarial-5-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # # CoT \n",
    "        # options = incorrect_answers[:4]\n",
    "        # options.append(correct_answer)\n",
    "        # prompt_elem[\"Adversarial-5-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        # ####################################################\n",
    "\n",
    "\n",
    "        # ############## Adversarial-4-options ###############\n",
    "        # options = incorrect_answers[:3]\n",
    "        # options.append(correct_answer)\n",
    "        # prompt_elem[\"Adversarial-4-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # # CoT \n",
    "        # options = incorrect_answers[:3]\n",
    "        # options.append(correct_answer)\n",
    "        # prompt_elem[\"Adversarial-4-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        ############## Adversarial-3-options ################\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-3-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-3-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "        # ############## Adversarial-2-options #################\n",
    "        # options = incorrect_answers[:1]\n",
    "        # options.append(correct_answer)\n",
    "        # prompt_elem[\"Adversarial-2-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # # CoT \n",
    "        # options = incorrect_answers[:1]\n",
    "        # options.append(correct_answer)\n",
    "        # prompt_elem[\"Adversarial-2-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        # ######################################################\n",
    "\n",
    "\n",
    "        ################ Pseudo-Adversarial ##################\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, False, context, options, pseudo_adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Pseudo-Adversarial-CoT\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, True, context, options, pseudo_adversarial_instructions)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "        ############ Adversarial-NA-fourth-option ############\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, False, context, options, adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, True, context, options, adversarial_instructions)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "\n",
    "        ############ Adversarial-NA-fourth-option-Ablation1 ############\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-Ablation1\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-Ablation1-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #################### Answerability ###################\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Answerability\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, False, context, options, answerability_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Answerability-CoT\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, True, context, options, answerability_instructions)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "        prompt_list_few_shot_with_instructions_control_group.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Few-Shot with Instructions) - Control Group - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_few_shot_with_instructions_control_group_v2 = list()\n",
    "\n",
    "pos_example_1, pos_example_2, neg_example_1, neg_example_2, neg_example_3, pos_example_1_answerability, neg_example_1_answerability = get_examples_dict_multirc_few_shot_with_instructions_v2()\n",
    "\n",
    "for elem in data_control_group:\n",
    "\n",
    "    # choose question with at least one correct answer and at least 3 incorrect answers\n",
    "    potential_questions = [i for i,q in enumerate(elem['paragraph']['questions']) if len([answer for answer in q[\"answers\"] if answer[\"isAnswer\"]])>0 and len([answer for answer in q[\"answers\"] if not answer[\"isAnswer\"]])>2]\n",
    "\n",
    "    # if empty, a.k.a there aren't questions with at least one correct answer and at least 3 incorrect answers\n",
    "    if not potential_questions:\n",
    "        continue\n",
    "\n",
    "    for question_i in potential_questions:\n",
    "\n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        context = f\"Paragraph: {elem['paragraph']['text']}\"\n",
    "        context = re.sub(r\"<b>Sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph\n",
    "        context = re.sub(r\"<b>sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "        context = context.replace(\"<br>\", \"\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "        context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "\n",
    "        context = f\"{context}\\nQuestion: {elem['paragraph']['questions'][question_i]['question']}\"\n",
    "\n",
    "        correct_answer = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if answer[\"isAnswer\"]][0]\n",
    "        incorrect_answers = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if not answer[\"isAnswer\"]]\n",
    "\n",
    "        adversarial_instructions = \"Given the following paragraph, question, and several options, which of the options answers the question?\"\n",
    "        pseudo_adversarial_instructions = f'{adversarial_instructions} \\nIf none of the options fit, answer \"unanswerable\".'\n",
    "        answerability_instructions = 'Determine whether the following context, question and three options are answerable. Reply only \"answerable\" or \"unanswerable\":'\n",
    "\n",
    "\n",
    "\n",
    "        ############## Adversarial-7-options ##############\n",
    "        options = incorrect_answers[:6]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-7-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:6]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-7-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        ############## Adversarial-6-options ###############\n",
    "        options = incorrect_answers[:5]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-6-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:5]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-6-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        ############## Adversarial-5-options ##############\n",
    "        options = incorrect_answers[:4]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-5-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:4]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-5-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        ############## Adversarial-4-options ###############\n",
    "        options = incorrect_answers[:3]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-4-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:3]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-4-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        #####################################################\n",
    "\n",
    "\n",
    "        ############## Adversarial-3-options ################\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-3-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-3-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "        ############## Adversarial-2-options #################\n",
    "        options = incorrect_answers[:1]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-2-options\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:1]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Adversarial-2-options-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "        ################ Pseudo-Adversarial ##################\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, False, context, options, pseudo_adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Pseudo-Adversarial-CoT\"] = get_full_adversarial_prompt(pos_example_1, neg_example_1, True, context, options, pseudo_adversarial_instructions)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "        ############ Adversarial-NA-fourth-option ############\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, False, context, options, adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-CoT\"] = get_full_adversarial_prompt(neg_example_2, neg_example_3, True, context, options, adversarial_instructions)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "\n",
    "        ############ Adversarial-NA-fourth-option-Ablation1 ############\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-Ablation1\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, False, context, options, adversarial_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        options.append(\"None of the above.\")\n",
    "        prompt_elem[\"Adversarial-NA-fourth-option-Ablation1-CoT\"] = get_full_adversarial_prompt(pos_example_1, pos_example_2, True, context, options, adversarial_instructions)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #################### Answerability ###################\n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Answerability\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, False, context, options, answerability_instructions)\n",
    "        # CoT \n",
    "        options = incorrect_answers[:2]\n",
    "        options.append(correct_answer)\n",
    "        prompt_elem[\"Answerability-CoT\"] = get_full_adversarial_prompt(pos_example_1_answerability, neg_example_1_answerability, True, context, options, answerability_instructions)\n",
    "        ######################################################\n",
    "\n",
    "\n",
    "        prompt_list_few_shot_with_instructions_control_group_v2.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outdir):\n",
    "   os.makedirs(outdir)\n",
    "# zero shot\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"zero_shot\", \"multirc_incorrect_answers_adversarial.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_zero_shot, indent=2))\n",
    "\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"zero_shot\", \"multirc_incorrect_answers_control_group.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_zero_shot_control_group, indent=2))\n",
    "\n",
    "# few shot\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"few_shot\", \"multirc_incorrect_answers_adversarial.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_few_shot, indent=2))\n",
    "\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"few_shot\", \"multirc_incorrect_answers_control_group.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_few_shot_control_group, indent=2))\n",
    "\n",
    "# few shot with instructions\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"few_shot_with_instructions\", \"multirc_incorrect_answers_adversarial.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_few_shot_with_instructions, indent=2))\n",
    "\n",
    "with open(os.path.join(outdir, \"chatGPT\", \"few_shot_with_instructions\", \"multirc_incorrect_answers_control_group.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_few_shot_with_instructions_control_group, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tk-Instruction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Zero-Shot) - Adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = list()\n",
    "for elem in filtered_data:\n",
    "\n",
    "    prompt_elem = dict()\n",
    "\n",
    "    prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "    context_end = elem[\"input\"].find(\"Correct Answer:\")\n",
    "    context = elem[\"input\"][:context_end] if context_end != -1 else elem[\"input\"] # if there is the right answer in the input - remove it.\n",
    "    context = re.sub(r\"Sent \\d+:\", \"\", context) # make all sentences into a single paragraph\n",
    "    context = re.sub(r\"sent \\d+:\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "    context = context.replace(\"Paragraph-\", \"Paragraph:\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "    context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "    context = context.replace(\"Question:\", \"\\nQuestion:\") # move the Question to a new line\n",
    "    context = context.replace(\"?.\", \"?\") # the paragraph and question appear together, followed by a \".\" --> so remove this redundant dot.\n",
    "\n",
    "\n",
    "    prompt_elem[\"Adversarial-7-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) {elem[\"output\"][3]}\n",
    "    (E) {elem[\"output\"][4]}\n",
    "    (F) {elem[\"output\"][5]}\n",
    "    (G) {elem[\"output\"][6]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Adversarial-6-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) {elem[\"output\"][3]}\n",
    "    (E) {elem[\"output\"][4]}\n",
    "    (F) {elem[\"output\"][5]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Adversarial-5-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) {elem[\"output\"][3]}\n",
    "    (E) {elem[\"output\"][4]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Adversarial-4-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) {elem[\"output\"][3]}\"\"\" \n",
    "    \n",
    "    prompt_elem[\"Adversarial-3-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Adversarial-2-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\"\"\" \n",
    "\n",
    "    prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\n",
    "If none of the options fit, answer \\\"N/A\\\".\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\"  \n",
    "    \n",
    "    prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\n",
    "    (D) None of the above.\"\"\" \n",
    "\n",
    "    prompt_elem[\"Answerability\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, does any of the options answer the question? \n",
    "Answer with a single word - \\\"yes\\\" or \\\"no\\\":\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {elem[\"output\"][0]}\n",
    "    (B) {elem[\"output\"][1]}\n",
    "    (C) {elem[\"output\"][2]}\"\"\" \n",
    "\n",
    "    prompt_list.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Instructions (Zero-Shot) - Control Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list_control_group = list()\n",
    "for elem in data_control_group:\n",
    "    # choose question with at least one correct answer and at least 7 incorrect answers\n",
    "    potential_questions = [i for i,q in enumerate(elem['paragraph']['questions']) if len([answer for answer in q[\"answers\"] if answer[\"isAnswer\"]])>0 and len([answer for answer in q[\"answers\"] if not answer[\"isAnswer\"]])>7]\n",
    "\n",
    "    # if empty, a.k.a there aren't questions with at least one correct answer and at least 7 incorrect answers\n",
    "    if not potential_questions:\n",
    "        continue\n",
    "\n",
    "    for question_i in potential_questions:\n",
    "\n",
    "        \n",
    "        prompt_elem = dict()\n",
    "\n",
    "        prompt_elem[\"id\"] = elem[\"id\"]\n",
    "\n",
    "        context = f\"Paragraph: {elem['paragraph']['text']}\"\n",
    "        context = re.sub(r\"<b>Sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph\n",
    "        context = re.sub(r\"<b>sent \\d+: </b>\", \"\", context) # make all sentences into a single paragraph (if \"sent i:\" starts with a non-capital \"S\")\n",
    "        context = context.replace(\"<br>\", \"\").replace(\"\\n\", \" \") # replace \"Paragraph-\" with \"Paragraph:\" and remove all new lines\n",
    "        context = re.sub(' +', ' ', context) # replace consecutive spaces with a single space\n",
    "\n",
    "        context = f\"{context}\\nQuestion: {elem['paragraph']['questions'][question_i]['question']}\"\n",
    "        \n",
    "\n",
    "        correct_answer = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if answer[\"isAnswer\"]][0]\n",
    "        incorrect_answers = [answer[\"text\"] for answer in elem['paragraph']['questions'][question_i][\"answers\"] if not answer[\"isAnswer\"]]\n",
    "\n",
    "\n",
    "\n",
    "        prompt_elem[\"Adversarial-7-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {incorrect_answers[0]}\n",
    "    (B) {incorrect_answers[1]}\n",
    "    (C) {incorrect_answers[2]}\n",
    "    (D) {incorrect_answers[3]}\n",
    "    (E) {incorrect_answers[4]}\n",
    "    (F) {incorrect_answers[5]}\n",
    "    (G) {correct_answer}\"\"\" \n",
    "        \n",
    "        prompt_elem[\"Adversarial-6-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {incorrect_answers[0]}\n",
    "    (B) {incorrect_answers[1]}\n",
    "    (C) {incorrect_answers[2]}\n",
    "    (D) {incorrect_answers[3]}\n",
    "    (E) {incorrect_answers[4]}\n",
    "    (F) {correct_answer}\"\"\" \n",
    "        \n",
    "\n",
    "        prompt_elem[\"Adversarial-5-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {incorrect_answers[0]}\n",
    "    (B) {incorrect_answers[1]}\n",
    "    (C) {incorrect_answers[2]}\n",
    "    (D) {incorrect_answers[3]}\n",
    "    (E) {correct_answer}\"\"\" \n",
    "        \n",
    "        prompt_elem[\"Adversarial-4-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {incorrect_answers[0]}\n",
    "    (B) {incorrect_answers[1]}\n",
    "    (C) {incorrect_answers[2]}\n",
    "    (D) {correct_answer}\"\"\" \n",
    "        \n",
    "        prompt_elem[\"Adversarial-3-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {incorrect_answers[0]}\n",
    "    (B) {incorrect_answers[1]}\n",
    "    (C) {correct_answer}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Adversarial-2-options\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {incorrect_answers[0]}\n",
    "    (B) {correct_answer}\"\"\" \n",
    "\n",
    "        prompt_elem[\"Pseudo-Adversarial\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\n",
    "If none of the options fit, answer \\\"N/A\\\".\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {incorrect_answers[0]}\n",
    "    (B) {incorrect_answers[1]}\n",
    "    (C) {correct_answer}\"\"\"  \n",
    "        \n",
    "        prompt_elem[\"Adversarial-NA-fourth-option\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, which of the options answers the question?\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {incorrect_answers[0]}\n",
    "    (B) {incorrect_answers[1]}\n",
    "    (C) {correct_answer}\n",
    "    (D) None of the above.\"\"\" \n",
    "\n",
    "        prompt_elem[\"Answerability\"] = f\"\"\"Definition:\n",
    "Given the following paragraph, question, and several options, does any of the options answer the question? \n",
    "Answer with a single word - \\\"yes\\\" or \\\"no\\\":\\n\n",
    "{context}\n",
    "Options:\n",
    "    (A) {incorrect_answers[0]}\n",
    "    (B) {incorrect_answers[1]}\n",
    "    (C) {correct_answer}\"\"\" \n",
    "\n",
    "        prompt_list_control_group.append(prompt_elem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outdir):\n",
    "   os.makedirs(outdir)\n",
    "\n",
    "with open(os.path.join(outdir, \"tk-instruct\", \"zero_shot\", \"multirc_incorrect_answers_adversarial.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list, indent=2))\n",
    "\n",
    "with open(os.path.join(outdir, \"tk-instruct\", \"zero_shot\", \"multirc_incorrect_answers_control_group.json\"), 'w') as f1:\n",
    "    f1.write(json.dumps(prompt_list_control_group, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import filecmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir1 = \"/home/nlp/sloboda1/projects/unanswerable_adversarial/generated_text_debugging\"\n",
    "indir2 = \"/home/nlp/sloboda1/projects/unanswerable_adversarial/generated_text_debugging1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdir, dirs, files in os.walk(indir1):\n",
    "    for file in files:\n",
    "        file1 = os.path.join(subdir, file)\n",
    "        file2 = file1.replace(indir1, indir2)\n",
    "        comparison = filecmp.cmp(file1, file2)\n",
    "        if not comparison:\n",
    "            print(f\"not equal: {file1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ccca151241cdff9659bdd5dad71069d9aa32f7367869ba4cd0a3ff2a4cdc870"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
