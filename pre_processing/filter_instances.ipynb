{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 2000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ids_path = r\"generated_prompts/filtered/instance_ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_ids(indir, id_name=\"id\"):\n",
    "    with open(indir, 'r') as f1:\n",
    "        data = json.loads(f1.read())\n",
    "    rand_indice = random.sample(range(len(data)), NUM_EXAMPLES)\n",
    "    rand_indice.sort()\n",
    "    print(f\"len(rand_indice)={len(rand_indice)}; len(set(rand_indice))={len(set(rand_indice))}\")\n",
    "    filtered_ids = [data[ind][id_name] for ind in rand_indice]\n",
    "    return filtered_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_filtered_ids(ids, outdir):\n",
    "    if os.path.exists(outdir):\n",
    "        print(f\"{outdir} exists! skipping...\")\n",
    "        return\n",
    "    with open(outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_adversarial = r\"generated_prompts/chatGPT/zero_shot/squad_v2_adversarial.json\"\n",
    "indir_control_group = r\"generated_prompts/chatGPT/zero_shot/squad_v2_control_group.json\"\n",
    "\n",
    "filtered_adversarial_ids = get_filtered_ids(indir_adversarial)\n",
    "filtered_control_group_ids = get_filtered_ids(indir_control_group)\n",
    "\n",
    "save_filtered_ids(filtered_adversarial_ids, os.path.join(filtered_ids_path, f\"squad_adversarial_ids.json\"))\n",
    "save_filtered_ids(filtered_control_group_ids, os.path.join(filtered_ids_path, f\"squad_control_group_ids.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(rand_indice)=2000; len(set(rand_indice))=2000\n",
      "len(rand_indice)=2000; len(set(rand_indice))=2000\n"
     ]
    }
   ],
   "source": [
    "indir_adversarial = r\"generated_prompts/all/zero_shot/NQ_adversarial_all.json\"\n",
    "indir_control_group = r\"generated_prompts/all/zero_shot/NQ_control_group_all.json\"\n",
    "\n",
    "filtered_adversarial_ids = get_filtered_ids(indir_adversarial, id_name=\"example_id\")\n",
    "filtered_control_group_ids = get_filtered_ids(indir_control_group, id_name=\"example_id\")\n",
    "\n",
    "save_filtered_ids(filtered_adversarial_ids, os.path.join(filtered_ids_path, f\"NQ_adversarial_ids.json\"))\n",
    "save_filtered_ids(filtered_control_group_ids, os.path.join(filtered_ids_path, f\"NQ_control_group_ids.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Musique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### relevant only for the training data for the classifiers - change first the \"filtered_ids_path\" to \"generated_prompts/train_set/filtered/instance_ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(rand_indice)=2000; len(set(rand_indice))=2000\n",
      "len(rand_indice)=2000; len(set(rand_indice))=2000\n"
     ]
    }
   ],
   "source": [
    "indir_adversarial = r\"generated_prompts/train_set/all/zero_shot/musique_trainset_adversarial_all.json\"\n",
    "indir_control_group = r\"generated_prompts/train_set/all/zero_shot/musique_trainset_control_group_all.json\"\n",
    "\n",
    "filtered_adversarial_ids = get_filtered_ids(indir_adversarial)\n",
    "filtered_control_group_ids = get_filtered_ids(indir_control_group)\n",
    "\n",
    "save_filtered_ids(filtered_adversarial_ids, os.path.join(filtered_ids_path, f\"musique_trainset_adversarial_ids.json\"))\n",
    "save_filtered_ids(filtered_control_group_ids, os.path.join(filtered_ids_path, f\"musique_trainset_control_group_ids.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Indice - of Unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ids_path = r\"generated_prompts/filtered/instance_ids\"\n",
    "unfiltered_ids_path = r\"generated_prompts/unfiltered/instance_ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unfiltered_ids(indir, filtered_ids_path, id_name=\"id\"):\n",
    "    with open(indir, 'r') as f1:\n",
    "        data = json.loads(f1.read())\n",
    "    \n",
    "    with open(filtered_ids_path, 'r') as f1:\n",
    "        filtered_ids = json.loads(f1.read())\n",
    "\n",
    "    unfiltered_ids = [instance[id_name] for instance in data if not instance[id_name] in filtered_ids]\n",
    "    print(f\"len(unfiltered_ids)={len(unfiltered_ids)}\")\n",
    "    return unfiltered_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_unfiltered_ids(ids, outdir):\n",
    "    if os.path.exists(outdir):\n",
    "        print(f\"{outdir} exists! skipping...\")\n",
    "        return\n",
    "    with open(outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_adversarial = r\"generated_prompts/chatGPT/zero_shot/squad_v2_adversarial.json\"\n",
    "indir_control_group = r\"generated_prompts/chatGPT/zero_shot/squad_v2_control_group.json\"\n",
    "adversarial_filtered_ids_path = \"generated_prompts/filtered/instance_ids/squad_adversarial_ids.json\"\n",
    "control_group_filtered_ids_path = \"generated_prompts/filtered/instance_ids/squad_control_group_ids.json\"\n",
    "\n",
    "unfiltered_adversarial_ids = get_unfiltered_ids(indir_adversarial, adversarial_filtered_ids_path)\n",
    "unfiltered_control_group_ids = get_unfiltered_ids(indir_control_group, control_group_filtered_ids_path)\n",
    "\n",
    "save_unfiltered_ids(unfiltered_adversarial_ids, os.path.join(unfiltered_ids_path, f\"squad_adversarial_ids.json\"))\n",
    "save_unfiltered_ids(unfiltered_control_group_ids, os.path.join(unfiltered_ids_path, f\"squad_control_group_ids.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(unfiltered_ids)=1499\n",
      "len(unfiltered_ids)=0\n",
      "generated_prompts/train_set/filtered/instance_ids/NQ_trainset_adversarial_ids.json exists! skipping...\n",
      "generated_prompts/train_set/filtered/instance_ids/NQ_trainset_control_group_ids.json exists! skipping...\n"
     ]
    }
   ],
   "source": [
    "indir_adversarial = r\"generated_prompts/all/zero_shot/NQ_adversarial_all.json\"\n",
    "indir_control_group = r\"generated_prompts/all/zero_shot/NQ_control_group_all.json\"\n",
    "adversarial_filtered_ids_path = \"generated_prompts/filtered/instance_ids/NQ_adversarial_ids.json\"\n",
    "control_group_filtered_ids_path = \"generated_prompts/filtered/instance_ids/NQ_control_group_ids.json\"\n",
    "\n",
    "unfiltered_adversarial_ids = get_unfiltered_ids(indir_adversarial, adversarial_filtered_ids_path, id_name=\"example_id\")\n",
    "unfiltered_control_group_ids = get_unfiltered_ids(indir_control_group, control_group_filtered_ids_path, id_name=\"example_id\")\n",
    "\n",
    "save_unfiltered_ids(unfiltered_adversarial_ids, os.path.join(unfiltered_ids_path, f\"NQ_adversarial_ids.json\"))\n",
    "save_unfiltered_ids(unfiltered_control_group_ids, os.path.join(unfiltered_ids_path, f\"NQ_control_group_ids.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ids_path = r\"generated_prompts/filtered/instance_ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_instances(full_data_path, filtered_ids_path, id_name=\"id\"):\n",
    "    with open(full_data_path, 'r') as f1:\n",
    "        data = json.loads(f1.read())\n",
    "    with open(filtered_ids_path, 'r') as f1:\n",
    "        filtered_ids = json.loads(f1.read())\n",
    "    filtered_data = [elem for elem in data if elem[id_name] in filtered_ids][:NUM_EXAMPLES]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_filtered_instances(filtered_data, outdir):\n",
    "    if os.path.exists(outdir):\n",
    "        print(f\"{outdir} exists! skipping...\")\n",
    "        return\n",
    "    with open(outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(filtered_data, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TYPES=[\"zero_shot\", \"few_shot\", \"few_shot_with_instructions\"]\n",
    "\n",
    "for PROMPT_TYPE in PROMPT_TYPES:\n",
    "    data_adversarial_path = fr\"generated_prompts/chatGPT/{PROMPT_TYPE}/squad_v2_adversarial.json\"\n",
    "    data_control_group_path = fr\"generated_prompts/chatGPT/{PROMPT_TYPE}/squad_v2_control_group.json\"\n",
    "\n",
    "    filtered_ids_adversarial_path = r\"generated_prompts/filtered/instance_ids/squad_adversarial_ids.json\"\n",
    "    filtered_ids_control_group_path = r\"generated_prompts/filtered/instance_ids/squad_control_group_ids.json\"\n",
    "\n",
    "    outdir_path = rf\"generated_prompts/filtered/{PROMPT_TYPE}\"\n",
    "\n",
    "    filtered_adversarial_instances = filter_instances(data_adversarial_path, filtered_ids_adversarial_path)\n",
    "    filtered_control_group_instances = filter_instances(data_control_group_path, filtered_ids_control_group_path)\n",
    "\n",
    "    save_filtered_instances(filtered_adversarial_instances, os.path.join(outdir_path, f\"squad_adversarial_filtered.json\"))\n",
    "    save_filtered_instances(filtered_control_group_instances, os.path.join(outdir_path, f\"squad_control_group_filtered.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_prompts/filtered/zero_shot/NQ_trainset_adversarial_filtered.json exists! skipping...\n",
      "generated_prompts/filtered/zero_shot/NQ_trainset_control_group_filtered.json exists! skipping...\n"
     ]
    }
   ],
   "source": [
    "# PROMPT_TYPES=[\"zero_shot\", \"few_shot\", \"few_shot_with_instructions\"]\n",
    "PROMPT_TYPES=[\"zero_shot\"]\n",
    "\n",
    "\n",
    "for PROMPT_TYPE in PROMPT_TYPES:\n",
    "    data_adversarial_path = fr\"generated_prompts/all/{PROMPT_TYPE}/NQ_adversarial_all.json\"\n",
    "    data_control_group_path = fr\"generated_prompts/all/{PROMPT_TYPE}/NQ_control_group_all.json\"\n",
    "\n",
    "    filtered_ids_adversarial_path = r\"generated_prompts/filtered/instance_ids/NQ_adversarial_ids.json\"\n",
    "    filtered_ids_control_group_path = r\"generated_prompts/filtered/instance_ids/NQ_control_group_ids.json\"\n",
    "\n",
    "    outdir_path = rf\"generated_prompts/filtered/{PROMPT_TYPE}\"\n",
    "\n",
    "    filtered_adversarial_instances = filter_instances(data_adversarial_path, filtered_ids_adversarial_path, id_name=\"example_id\")\n",
    "    filtered_control_group_instances = filter_instances(data_control_group_path, filtered_ids_control_group_path, id_name=\"example_id\")\n",
    "\n",
    "    save_filtered_instances(filtered_adversarial_instances, os.path.join(outdir_path, f\"NQ_adversarial_filtered.json\"))\n",
    "    save_filtered_instances(filtered_control_group_instances, os.path.join(outdir_path, f\"NQ_control_group_filtered.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Musique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### relevant only for the training data for the classifiers - change first the \"filtered_ids_path\" to \"generated_prompts/train_set/filtered/instance_ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT_TYPES=[\"zero_shot\", \"few_shot\", \"few_shot_with_instructions\"]\n",
    "PROMPT_TYPES=[\"zero_shot\"]\n",
    "\n",
    "\n",
    "for PROMPT_TYPE in PROMPT_TYPES:\n",
    "    data_adversarial_path = fr\"generated_prompts/train_set/all/{PROMPT_TYPE}/musique_trainset_adversarial_all.json\"\n",
    "    data_control_group_path = fr\"generated_prompts/train_set/all/{PROMPT_TYPE}/musique_trainset_control_group_all.json\"\n",
    "\n",
    "    filtered_ids_adversarial_path = r\"generated_prompts/train_set/filtered/instance_ids/musique_trainset_adversarial_ids.json\"\n",
    "    filtered_ids_control_group_path = r\"generated_prompts/train_set/filtered/instance_ids/musique_trainset_control_group_ids.json\"\n",
    "\n",
    "    outdir_path = rf\"generated_prompts/train_set/filtered/{PROMPT_TYPE}\"\n",
    "\n",
    "    filtered_adversarial_instances = filter_instances(data_adversarial_path, filtered_ids_adversarial_path)\n",
    "    filtered_control_group_instances = filter_instances(data_control_group_path, filtered_ids_control_group_path)\n",
    "\n",
    "    save_filtered_instances(filtered_adversarial_instances, os.path.join(outdir_path, f\"musique_trainset_adversarial_filtered.json\"))\n",
    "    save_filtered_instances(filtered_control_group_instances, os.path.join(outdir_path, f\"musique_trainset_control_group_filtered.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_instances(full_data_path):\n",
    "    with open(full_data_path, 'r') as f1:\n",
    "        data = json.loads(f1.read())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_instances(data, outdir):\n",
    "    if os.path.exists(outdir):\n",
    "        print(f\"{outdir} exists! skipping...\")\n",
    "        return\n",
    "    with open(outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(subdirs):\n",
    "    full_subdir = \"\"\n",
    "    for subdir in subdirs:\n",
    "        full_subdir = os.path.join(full_subdir, subdir)\n",
    "\n",
    "        if not os.path.exists(full_subdir):\n",
    "            os.makedirs(full_subdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TYPES=[\"zero_shot\", \"few_shot\", \"few_shot_with_instructions\"]\n",
    "\n",
    "for PROMPT_TYPE in PROMPT_TYPES:\n",
    "    data_adversarial_path = fr\"generated_prompts/chatGPT/{PROMPT_TYPE}/squad_v2_adversarial.json\"\n",
    "    data_control_group_path = fr\"generated_prompts/chatGPT/{PROMPT_TYPE}/squad_v2_control_group.json\"\n",
    "\n",
    "    outdir_path = rf\"generated_prompts/all/{PROMPT_TYPE}\"\n",
    "    create_dir([\"generated_prompts\", \"all\", PROMPT_TYPE])\n",
    "\n",
    "    adversarial_instances = get_all_instances(data_adversarial_path)\n",
    "    control_group_instances = get_all_instances(data_control_group_path)\n",
    "\n",
    "    save_all_instances(adversarial_instances, os.path.join(outdir_path, f\"squad_adversarial_all.json\"))\n",
    "    save_all_instances(control_group_instances, os.path.join(outdir_path, f\"squad_control_group_all.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Unfiltered Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered_ids_path = r\"generated_prompts/unfiltered/instance_ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfiltered_instances(full_data_path, unfiltered_ids_path, id_name=\"id\"):\n",
    "    with open(full_data_path, 'r') as f1:\n",
    "        data = json.loads(f1.read())\n",
    "    with open(unfiltered_ids_path, 'r') as f1:\n",
    "        filtered_ids = json.loads(f1.read())\n",
    "    filtered_data = [elem for elem in data if elem[id_name] in filtered_ids]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_unfiltered_instances(unfiltered_data, outdir):\n",
    "    if os.path.exists(outdir):\n",
    "        print(f\"{outdir} exists! skipping...\")\n",
    "        return\n",
    "    with open(outdir, 'w') as f1:\n",
    "        f1.write(json.dumps(unfiltered_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(subdirs):\n",
    "    full_subdir = \"\"\n",
    "    for subdir in subdirs:\n",
    "        full_subdir = os.path.join(full_subdir, subdir)\n",
    "\n",
    "        if not os.path.exists(full_subdir):\n",
    "            os.makedirs(full_subdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TYPES=[\"zero_shot\", \"few_shot\", \"few_shot_with_instructions\"]\n",
    "\n",
    "for PROMPT_TYPE in PROMPT_TYPES:\n",
    "    data_adversarial_path = fr\"generated_prompts/chatGPT/{PROMPT_TYPE}/squad_v2_adversarial.json\"\n",
    "    data_control_group_path = fr\"generated_prompts/chatGPT/{PROMPT_TYPE}/squad_v2_control_group.json\"\n",
    "\n",
    "    unfiltered_ids_adversarial_path = r\"generated_prompts/unfiltered/instance_ids/squad_adversarial_ids.json\"\n",
    "    unfiltered_ids_control_group_path = r\"generated_prompts/unfiltered/instance_ids/squad_control_group_ids.json\"\n",
    "\n",
    "    outdir_path = rf\"generated_prompts/unfiltered/{PROMPT_TYPE}\"\n",
    "    create_dir([\"generated_prompts\", \"unfiltered\", PROMPT_TYPE])\n",
    "\n",
    "    unfiltered_adversarial_instances = unfiltered_instances(data_adversarial_path, unfiltered_ids_adversarial_path)\n",
    "    unfiltered_control_group_instances = unfiltered_instances(data_control_group_path, unfiltered_ids_control_group_path)\n",
    "\n",
    "    save_unfiltered_instances(unfiltered_adversarial_instances, os.path.join(outdir_path, f\"squad_adversarial_unfiltered.json\"))\n",
    "    save_unfiltered_instances(unfiltered_control_group_instances, os.path.join(outdir_path, f\"squad_control_group_unfiltered.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT_TYPES=[\"zero_shot\", \"few_shot\", \"few_shot_with_instructions\"]\n",
    "PROMPT_TYPES=[\"zero_shot\"]\n",
    "\n",
    "for PROMPT_TYPE in PROMPT_TYPES:\n",
    "    data_adversarial_path = fr\"generated_prompts/all/{PROMPT_TYPE}/NQ_adversarial_all.json\"\n",
    "    data_control_group_path = fr\"generated_prompts/all/{PROMPT_TYPE}/NQ_control_group_all.json\"\n",
    "\n",
    "    unfiltered_ids_adversarial_path = r\"generated_prompts/unfiltered/instance_ids/NQ_adversarial_ids.json\"\n",
    "    unfiltered_ids_control_group_path = r\"generated_prompts/unfiltered/instance_ids/NQ_control_group_ids.json\"\n",
    "\n",
    "    outdir_path = rf\"generated_prompts/unfiltered/{PROMPT_TYPE}\"\n",
    "    create_dir([\"generated_prompts\", \"unfiltered\", PROMPT_TYPE])\n",
    "\n",
    "    unfiltered_adversarial_instances = unfiltered_instances(data_adversarial_path, unfiltered_ids_adversarial_path, id_name=\"example_id\")\n",
    "    unfiltered_control_group_instances = unfiltered_instances(data_control_group_path, unfiltered_ids_control_group_path, id_name=\"example_id\")\n",
    "\n",
    "    save_unfiltered_instances(unfiltered_adversarial_instances, os.path.join(outdir_path, f\"NQ_adversarial_unfiltered.json\"))\n",
    "    save_unfiltered_instances(unfiltered_control_group_instances, os.path.join(outdir_path, f\"NQ_control_group_unfiltered.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dev-v2.0.json of Squad-v2.0 for the evaluate script"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter separately the control_group and the adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instance = 50 # if \"all\" then takes all \"filtered\". Otherwise - take first n_instance\n",
    "get_filtered = False # if False - will take the instances that weren't filtered\n",
    "indir_dev_v2_0 = r\"data/squad/dev-v2.0.json\"\n",
    "data_type = \"control_group\" #\"adversarial\"\n",
    "subdir_filtered_str = \"filtered\" if get_filtered else \"unfiltered\"\n",
    "\n",
    "indice_path = fr\"generated_prompts/{subdir_filtered_str}/instance_ids/squad_{data_type}_ids.json\"\n",
    "\n",
    "outdir_suffix = f\"_first_{n_instance}\" if n_instance != \"all\" else \"\"\n",
    "outdir_filtered_dev_v2_0 = rf\"data/squad/dev-v2.0_{data_type}_{subdir_filtered_str}_1000{outdir_suffix}.json\"\n",
    "\n",
    "with open(indice_path, 'r') as f1:\n",
    "    filtered_indice = json.loads(f1.read())\n",
    "    if n_instance != \"all\":\n",
    "        filtered_indice = filtered_indice[:n_instance]\n",
    "\n",
    "with open(indir_dev_v2_0, 'r') as f1:\n",
    "    full_dev_v2_0_data = json.loads(f1.read())\n",
    "\n",
    "filtered_dev_v2_0_data = copy.deepcopy(full_dev_v2_0_data)\n",
    "\n",
    "for i,passage in enumerate(filtered_dev_v2_0_data[\"data\"]):\n",
    "    for j,paragraph in enumerate(passage[\"paragraphs\"]):\n",
    "        filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"][j][\"qas\"] = [qa for qa in filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"][j][\"qas\"] if qa['id'] in filtered_indice]\n",
    "    filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"] = [p for p in filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"] if p[\"qas\"]] # remove paragraphs without qas\n",
    "\n",
    "filtered_dev_v2_0_data[\"data\"] = [p for p in filtered_dev_v2_0_data[\"data\"] if p[\"paragraphs\"]] # remove passages without paragaphs (i.e., whose all paragraphs are without qas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list()\n",
    "for i,passage in enumerate(filtered_dev_v2_0_data[\"data\"]):\n",
    "    for j,paragraph in enumerate(passage[\"paragraphs\"]):\n",
    "        ids.extend([elem['id'] for elem in paragraph[\"qas\"]])\n",
    "ids == filtered_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir_filtered_dev_v2_0, 'w') as f1:\n",
    "    f1.write(json.dumps(filtered_dev_v2_0_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter together the control_group and the adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instance = 50 # if \"all\" then takes all \"filtered\". Otherwise - take first n_instance\n",
    "get_filtered = False # if False - will take the instances that weren't filtered\n",
    "indir_dev_v2_0 = r\"data/squad/dev-v2.0.json\"\n",
    "subdir_filtered_str = \"filtered\" if get_filtered else \"unfiltered\"\n",
    "\n",
    "indice_path_adversarial = fr\"generated_prompts/{subdir_filtered_str}/instance_ids/squad_adversarial_ids.json\"\n",
    "indice_path_control_group = fr\"generated_prompts/{subdir_filtered_str}/instance_ids/squad_control_group_ids.json\"\n",
    "\n",
    "outdir_suffix = f\"_first_{n_instance}\" if n_instance != \"all\" else \"\"\n",
    "outdir_filtered_dev_v2_0 = rf\"data/squad/dev-v2.0_{subdir_filtered_str}_1000{outdir_suffix}.json\"\n",
    "\n",
    "\n",
    "\n",
    "with open(indice_path_adversarial, 'r') as f1:\n",
    "    filtered_indice = json.loads(f1.read())\n",
    "    if n_instance != \"all\":\n",
    "        filtered_indice = filtered_indice[:n_instance]\n",
    "\n",
    "with open(indice_path_control_group, 'r') as f1:\n",
    "    curr_filtered_indice = json.loads(f1.read())\n",
    "    if n_instance != \"all\":\n",
    "        curr_filtered_indice = curr_filtered_indice[:n_instance]\n",
    "    filtered_indice.extend(curr_filtered_indice)\n",
    "\n",
    "# with open(\"responses_embeddings/projections/28-04-2023_10:23:49/UL2/zero_shot/k_0/squad_evaluate_script_format/squad_Adversarial.json\", 'r') as f1:\n",
    "#     temp_data = json.loads(f1.read())\n",
    "#     filtered_indice = list(temp_data.keys())\n",
    "\n",
    "\n",
    "\n",
    "with open(indir_dev_v2_0, 'r') as f1:\n",
    "    full_dev_v2_0_data = json.loads(f1.read())\n",
    "\n",
    "filtered_dev_v2_0_data = copy.deepcopy(full_dev_v2_0_data)\n",
    "\n",
    "for i,passage in enumerate(filtered_dev_v2_0_data[\"data\"]):\n",
    "    for j,paragraph in enumerate(passage[\"paragraphs\"]):\n",
    "        filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"][j][\"qas\"] = [qa for qa in filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"][j][\"qas\"] if qa['id'] in filtered_indice]\n",
    "    filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"] = [p for p in filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"] if p[\"qas\"]] # remove paragraphs without qas\n",
    "\n",
    "filtered_dev_v2_0_data[\"data\"] = [p for p in filtered_dev_v2_0_data[\"data\"] if p[\"paragraphs\"]] # remove passages without paragaphs (i.e., whose all paragraphs are without qas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list()\n",
    "for i,passage in enumerate(filtered_dev_v2_0_data[\"data\"]):\n",
    "    for j,paragraph in enumerate(passage[\"paragraphs\"]):\n",
    "        ids.extend([elem['id'] for elem in paragraph[\"qas\"]])\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir_filtered_dev_v2_0, 'w') as f1:\n",
    "    f1.write(json.dumps(filtered_dev_v2_0_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter together the control_group and the adversarial of all the samples that remained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir_dev_v2_0 = r\"data/squad/dev-v2.0.json\"\n",
    "\n",
    "path_adversarial = fr\"generated_prompts/all/zero_shot/squad_adversarial_all.json\"\n",
    "path_control_group = fr\"generated_prompts/all/zero_shot/squad_control_group_all.json\"\n",
    "\n",
    "outdir_filtered_dev_v2_0 = rf\"data/squad/dev-v2.0_all.json\"\n",
    "\n",
    "\n",
    "\n",
    "with open(path_adversarial, 'r') as f1:\n",
    "    adversarial_data = json.loads(f1.read())\n",
    "    all_indice = [elem[\"id\"] for elem in adversarial_data]\n",
    "\n",
    "with open(path_control_group, 'r') as f1:\n",
    "    control_group_data = json.loads(f1.read())\n",
    "    curr_all_indice = [elem[\"id\"] for elem in control_group_data]\n",
    "    all_indice.extend(curr_all_indice)\n",
    "\n",
    "# with open(\"responses_embeddings/projections/28-04-2023_10:23:49/UL2/zero_shot/k_0/squad_evaluate_script_format/squad_Adversarial.json\", 'r') as f1:\n",
    "#     temp_data = json.loads(f1.read())\n",
    "#     filtered_indice = list(temp_data.keys())\n",
    "\n",
    "\n",
    "\n",
    "with open(indir_dev_v2_0, 'r') as f1:\n",
    "    full_dev_v2_0_data = json.loads(f1.read())\n",
    "\n",
    "filtered_dev_v2_0_data = copy.deepcopy(full_dev_v2_0_data)\n",
    "\n",
    "for i,passage in enumerate(filtered_dev_v2_0_data[\"data\"]):\n",
    "    for j,paragraph in enumerate(passage[\"paragraphs\"]):\n",
    "        filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"][j][\"qas\"] = [qa for qa in filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"][j][\"qas\"] if qa['id'] in all_indice]\n",
    "    filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"] = [p for p in filtered_dev_v2_0_data[\"data\"][i][\"paragraphs\"] if p[\"qas\"]] # remove paragraphs without qas\n",
    "\n",
    "filtered_dev_v2_0_data[\"data\"] = [p for p in filtered_dev_v2_0_data[\"data\"] if p[\"paragraphs\"]] # remove passages without paragaphs (i.e., whose all paragraphs are without qas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list()\n",
    "for i,passage in enumerate(filtered_dev_v2_0_data[\"data\"]):\n",
    "    for j,paragraph in enumerate(passage[\"paragraphs\"]):\n",
    "        ids.extend([elem['id'] for elem in paragraph[\"qas\"]])\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(adversarial_data) + len(control_group_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir_filtered_dev_v2_0, 'w') as f1:\n",
    "    f1.write(json.dumps(filtered_dev_v2_0_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/nlp/sloboda1/projects/unanswerable_adversarial/responses_embeddings/k-beams/08-05-2023_23:01:15/UL2/zero_shot/k_beams_1_num_return_seq_1/variant1/num_return_seq_1/squad_evaluate_script_format/squad_Adversarial.json\", \"r\") as f1:\n",
    "    a = json.loads(f1.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter instances of gold Squad for the evaluate script (for the devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"squad\"\n",
    "num_instances_from_end = 100\n",
    "gold_data_indir = \"../data/squad/train-v2.0.json\"\n",
    "devset_indir = f\"../generated_prompts/train_set/filtered/zero_shot/variant1\"\n",
    "outdir = \"../data/squad/devset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(devset_indir, f\"{dataset_name}_trainset_adversarial_filtered.json\"), 'r') as f1:\n",
    "    adversarial_data = json.loads(f1.read())[-num_instances_from_end:] \n",
    "\n",
    "\n",
    "with open(os.path.join(devset_indir, f\"{dataset_name}_trainset_control_group_filtered.json\"), 'r') as f1:\n",
    "    control_group_data = json.loads(f1.read())[-num_instances_from_end:] \n",
    "\n",
    "devset_ids = [elem['id'] for elem in adversarial_data]\n",
    "devset_ids = devset_ids + [elem['id'] for elem in control_group_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gold_data_indir, 'r') as f1:\n",
    "    gold_data = json.loads(f1.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,passage in enumerate(gold_data[\"data\"]):\n",
    "    for j,paragraph in enumerate(passage[\"paragraphs\"]):\n",
    "        gold_data[\"data\"][i][\"paragraphs\"][j][\"qas\"] = [qa for qa in gold_data[\"data\"][i][\"paragraphs\"][j][\"qas\"] if qa['id'] in devset_ids]\n",
    "    gold_data[\"data\"][i][\"paragraphs\"] = [p for p in gold_data[\"data\"][i][\"paragraphs\"] if p[\"qas\"]] # remove paragraphs without qas\n",
    "\n",
    "gold_data[\"data\"] = [p for p in gold_data[\"data\"] if p[\"paragraphs\"]] # remove passages without paragaphs (i.e., whose all paragraphs are without qas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = list()\n",
    "for i,passage in enumerate(gold_data[\"data\"]):\n",
    "    for j,paragraph in enumerate(passage[\"paragraphs\"]):\n",
    "        ids.extend([elem['id'] for elem in paragraph[\"qas\"]])\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir, 'w') as f1:\n",
    "    f1.write(json.dumps(gold_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create gold data for NQ and musique devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"musique\"\n",
    "devset_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../generated_prompts/train_set/filtered/zero_shot/variant1/{dataset}_trainset_adversarial_filtered.json\", 'r') as f1:\n",
    "    generated_prompts_adversarial = json.loads(f1.read())\n",
    "    generated_prompts_adversarial = generated_prompts_adversarial[-devset_length:]\n",
    "    if dataset == \"NQ\":\n",
    "        devset_adversarial_ids = [elem['example_id'] for elem in generated_prompts_adversarial]\n",
    "    else:\n",
    "        devset_adversarial_ids = [elem['id'] for elem in generated_prompts_adversarial]\n",
    "\n",
    "with open(f\"../generated_prompts/train_set/filtered/zero_shot/variant1/{dataset}_trainset_control_group_filtered.json\", 'r') as f1:\n",
    "    generated_prompts_control_group = json.loads(f1.read())\n",
    "    generated_prompts_control_group = generated_prompts_control_group[-devset_length:]\n",
    "    if dataset == \"NQ\":\n",
    "        devset_control_group_ids = [elem['example_id'] for elem in generated_prompts_control_group]\n",
    "    else:\n",
    "        devset_control_group_ids = [elem['id'] for elem in generated_prompts_control_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/{dataset}/adversarial_{dataset}_devset.jsonl\", 'r') as f1:\n",
    "    adversarial_data = json.loads(f1.read())\n",
    "\n",
    "with open(f\"../data/{dataset}/control_group_{dataset}_devset.jsonl\", 'r') as f1:\n",
    "    control_group_data = json.loads(f1.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {f\"{elem}-unanswerable\":\"\" for elem in devset_adversarial_ids}\n",
    "\n",
    "if dataset == \"NQ\":\n",
    "    with open(f\"../data/{dataset}/control_group_{dataset}_devset.jsonl\", 'r') as f1:\n",
    "        control_group_data = json.loads(f1.read())\n",
    "    output_data.update({elem['example_id']:[elem['answer']] for elem in control_group_data})\n",
    "\n",
    "elif dataset == \"musique\":\n",
    "    output_data.update({elem['id']:[elem['answer']] for elem in generated_prompts_control_group})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/{dataset}/{dataset}_devset_answers.jsonl\", 'w') as f1:\n",
    "    f1.write(json.dumps(output_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
